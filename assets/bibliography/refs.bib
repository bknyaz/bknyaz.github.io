@article{jacobs1991adaptive,
  title={Adaptive Mixtures of Local Experts },
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press},
  url={https://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf}
}

@inproceedings{shazeer2017outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  booktitle={ICLR},
  year={2017},
  url={https://arxiv.org/abs/1701.06538}
}

@article{fedus2022switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022},
  url={https://arxiv.org/abs/2101.03961}
}

@inproceedings{dauphin2017language,
  title={Language Modeling with Gated Convolutional Networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={ICML},
  pages={933--941},
  year={2017},
  organization={PMLR},
  url={https://arxiv.org/abs/1612.08083}
}

@article{shazeer2020glu,
  title={GLU Variants Improve Transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020},
  url={https://arxiv.org/abs/2002.05202}
}

@inproceedings{tay2023scaling,
  title={Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Chung, Hyung and Fedus, William and Rao, Jinfeng and Narang, Sharan and Tran, Vinh and Yogatama, Dani and Metzler, Donald},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={12342--12364},
  year={2023},
  url={https://aclanthology.org/2023.findings-emnlp.825/}
}

@article{muqeeth2024soft,
  title={Soft Merging of Experts with Adaptive Routing},
  author={Muqeeth, Mohammed and Liu, Haokun and Raffel, Colin},
  journal={Transactions on Machine Learning Research},
  year={2024},
  url={https://arxiv.org/abs/2306.03745}
}

@article{team2025kimi,
  title={Kimi K2: Open Agentic Intelligence},
  author={Team, Kimi and Bai, Yifan and Bao, Yiping and Chen, Guanduo and Chen, Jiahao and Chen, Ningxin and Chen, Ruijue and Chen, Yanru and Chen, Yuankun and Chen, Yutian and others},
  journal={arXiv preprint arXiv:2507.20534},
  year={2025},
  url={https://arxiv.org/abs/2507.20534}
}

@article{lasby2025reap,
  title={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},
  author={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},
  journal={arXiv preprint arXiv:2510.13999},
  year={2025},
  url={https://arxiv.org/abs/2510.13999}
}

@inproceedings{li2023merge,
  title={Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy},
  author={Li, Pingzhi and Zhang, Zhenyu and Yadav, Prateek and Sung, Yi-Lin and Cheng, Yu and Bansal, Mohit and Chen, Tianlong},
  booktitle={ICLR},
  year={2024},
  url={https://arxiv.org/abs/2310.01334},
}

@inproceedings{chen2025retrainingfree,
title={Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical Clustering},
author={I-Chun Chen and Hsu-Shen Liu and Wei-Fang Sun and Chen-Hao Chao and Yen-Chang Hsu and Chun-Yi Lee},
booktitle={ICML},
year={2025},
url={https://arxiv.org/abs/2410.08589},
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International conference on machine learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR},
  url={https://arxiv.org/abs/2203.05482}
}

@incollection{hecht1990algebraic,
  title={On the algebraic structure of feedforward network weight spaces},
  author={Hecht-Nielsen, Robert},
  booktitle={Advanced Neural Computers},
  pages={129--135},
  year={1990},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/chapter/edited-volume/abs/pii/B9780444884008500194}
}

@inproceedings{ainsworth2022git,
  title={Git Re-Basin: Merging Models modulo Permutation Symmetries},
  author={Ainsworth, Samuel K and Hayase, Jonathan and Srinivasa, Siddhartha},
  booktitle={ICLR},
  year={2023},
  url={https://arxiv.org/abs/2209.04836}
}

@inproceedings{stoica2024zipit,
  title={ZipIt! Merging Models from Different Tasks without Training},
  author={Stoica, George and Bolya, Daniel and Bjorner, Jakob Brandt and Ramesh, Pratik and Hearn, Taylor and Hoffman, Judy},
  booktitle={ICLR},
  year={2024},
  url={https://arxiv.org/abs/2305.03053}
}

@inproceedings{lim2024graph,
  title={Graph Metanetworks for Processing Diverse Neural Architectures},
  author={Lim, Derek and Maron, Haggai and Law, Marc T and Lorraine, Jonathan and Lucas, James},
  booktitle={ICLR},
  year={2024},
  url={https://arxiv.org/abs/2312.04501}
}

@inproceedings{kofinas2024graph,
  title={Graph Neural Networks for Learning Equivariant Representations of Neural Networks},
  author={Kofinas, Miltiadis and Knyazev, Boris and Zhang, Yan and Chen, Yunlu and Burghouts,
    Gertjan J. and Gavves, Efstratios and Snoek, Cees G. M. and Zhang, David W.},
  booktitle={ICLR},
  year={2024},
  url={https://arxiv.org/abs/2403.12143}
}

@inproceedings{knyazev2024accelerating,
  title={Accelerating Training with Neuron Interaction and Nowcasting Networks},
  author={Knyazev, Boris and Moudgil, Abhinav and Lajoie, Guillaume and Belilovsky, Eugene and Lacoste-Julien, Simon},
  booktitle={ICLR},
  year={2025},
  url={https://arxiv.org/abs/2409.04434}
}

@inproceedings{sukhbaatar2024branch,
  title={Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM},
  author={Sukhbaatar, Sainbayar and Golovneva, Olga and Sharma, Vasu and Xu, Hu and Lin, Xi Victoria and Roziere, Baptiste and Kahn, Jacob and Li, Shang-Wen and Yih, Wen-tau and Weston, Jason E and others},
  booktitle={First Conference on Language Modeling (COLM)},
  year={2024},
  url={https://arxiv.org/abs/2403.07816}
}

@inproceedings{zhou2025mergeme,
  title={MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs},
  author={Zhou, Yuhang and Karamanolakis, Giannis and Soto, Victor and Rumshisky, Anna and Kulkarni, Mayank and Huang, Furong and Ai, Wei and Lu, Jianhua},
  booktitle={Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={2315--2328},
  year={2025},
  url={https://arxiv.org/abs/2502.00997}
}

@inproceedings{ilharco2023editing,
  title={Editing Models with Task Arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={ICLR},
  year={2023},
  url={https://arxiv.org/abs/2212.04089}
}

@article{yadav2023ties,
  title={TIES-Merging: Resolving Interference When Merging Models},
  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={7093--7115},
  year={2023},
  url={https://arxiv.org/abs/2306.01708}
}

@inproceedings{stoica2025model,
  title={Model merging with SVD to tie the Knots},
  author={Stoica, George and Ramesh, Pratik and Ecsedi, Boglarka and Choshen, Leshem and Hoffman, Judy},
  year={2025},
  organization={ICLR},
  url={https://arxiv.org/abs/2410.19735}
}

@inproceedings{gargiulo2025task,
  title={Task singular vectors: Reducing task interference in model merging},
  author={Gargiulo, Antonio Andrea and Crisostomi, Donato and Bucarelli, Maria Sofia and Scardapane, Simone and Silvestri, Fabrizio and Rodola, Emanuele},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={18695--18705},
  year={2025},
  url={https://arxiv.org/abs/2412.00081}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International conference on machine learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR},
  url={https://arxiv.org/abs/1912.05671}
}

@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR},
  url={https://arxiv.org/abs/1802.09568}
}

@inproceedings{vyas2025soap,
  title={SOAP: Improving and Stabilizing Shampoo using Adam for Language Modeling},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham M},
  booktitle={ICLR},
  year={2025},
  url={https://arxiv.org/abs/2409.11321}
}

@article{jordan2024muon,
  title={Muon: An optimizer for hidden layers in neural networks, 2024},
  author={Jordan, Keller and Jin, Yuchen and Boza, Vlado and Jiacheng, You and Cesista, Franz and Newhouse, Laker and Bernstein, Jeremy},
  year={2024},
  url={https://kellerjordan.github.io/posts/muon},
}

@inproceedings{jang2023learning,
  title={Learning to boost training by periodic nowcasting near future weights},
  author={Jang, Jinhyeok and Yun, Woo-han and Kim, Won Hwa and Yoon, Youngwoo and Kim, Jaehong and Lee, Jaeyeon and Han, ByungOk},
  booktitle={International Conference on Machine Learning},
  pages={14730--14757},
  year={2023},
  organization={PMLR},
  url={https://proceedings.mlr.press/v202/jang23b.html}
}

@article{metz2022velo,
  title={Velo: Training versatile learned optimizers by scaling up},
  author={Metz, Luke and Harrison, James and Freeman, C Daniel and Merchant, Amil and Beyer, Lucas and Bradbury, James and Agrawal, Naman and Poole, Ben and Mordatch, Igor and Roberts, Adam and others},
  journal={arXiv preprint arXiv:2211.09760},
  year={2022},
  url={https://arxiv.org/abs/2211.09760}
}