@article{jacobs1991adaptive,
  title={Adaptive Mixtures of Local Experts },
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press},
  url={https://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf}
}

@inproceedings{shazeer2017outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  booktitle={ICLR},
  year={2017},
  url={https://arxiv.org/abs/1701.06538}
}

@article{fedus2022switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022},
  url={https://arxiv.org/abs/2101.03961}
}

@inproceedings{dauphin2017language,
  title={Language Modeling with Gated Convolutional Networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={ICML},
  pages={933--941},
  year={2017},
  organization={PMLR},
  url={https://arxiv.org/abs/1612.08083}
}

@article{shazeer2020glu,
  title={GLU Variants Improve Transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020},
  url={https://arxiv.org/abs/2002.05202}
}

@inproceedings{tay2023scaling,
  title={Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Chung, Hyung and Fedus, William and Rao, Jinfeng and Narang, Sharan and Tran, Vinh and Yogatama, Dani and Metzler, Donald},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={12342--12364},
  year={2023},
  url={https://aclanthology.org/2023.findings-emnlp.825/}
}

@article{muqeeth2024soft,
  title={Soft Merging of Experts with Adaptive Routing},
  author={Muqeeth, Mohammed and Liu, Haokun and Raffel, Colin},
  journal={Transactions on Machine Learning Research},
  year={2024},
  url={https://arxiv.org/abs/2306.03745}
}

@article{team2025kimi,
  title={Kimi K2: Open Agentic Intelligence},
  author={Team, Kimi and Bai, Yifan and Bao, Yiping and Chen, Guanduo and Chen, Jiahao and Chen, Ningxin and Chen, Ruijue and Chen, Yanru and Chen, Yuankun and Chen, Yutian and others},
  journal={arXiv preprint arXiv:2507.20534},
  year={2025},
  url={https://arxiv.org/abs/2507.20534}
}

@article{lasby2025reap,
  title={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},
  author={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},
  journal={arXiv preprint arXiv:2510.13999},
  year={2025},
  url={https://arxiv.org/abs/2510.13999}
}

@inproceedings{li2023merge,
  title={Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy},
  author={Li, Pingzhi and Zhang, Zhenyu and Yadav, Prateek and Sung, Yi-Lin and Cheng, Yu and Bansal, Mohit and Chen, Tianlong},
  booktitle={ICLR},
  year={2024},
  url={https://arxiv.org/abs/2310.01334},
}

@inproceedings{chen2025retrainingfree,
title={Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical Clustering},
author={I-Chun Chen and Hsu-Shen Liu and Wei-Fang Sun and Chen-Hao Chao and Yen-Chang Hsu and Chun-Yi Lee},
booktitle={ICML},
year={2025},
url={https://arxiv.org/abs/2410.08589},
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International conference on machine learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR},
  url={https://arxiv.org/abs/2203.05482}
}

@incollection{hecht1990algebraic,
  title={On the algebraic structure of feedforward network weight spaces},
  author={Hecht-Nielsen, Robert},
  booktitle={Advanced Neural Computers},
  pages={129--135},
  year={1990},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/chapter/edited-volume/abs/pii/B9780444884008500194}
}

@inproceedings{ainsworth2022git,
  title={Git Re-Basin: Merging Models modulo Permutation Symmetries},
  author={Ainsworth, Samuel K and Hayase, Jonathan and Srinivasa, Siddhartha},
  booktitle={ICLR},
  year={2023},
  url={https://arxiv.org/abs/2209.04836}
}

@inproceedings{stoica2024zipit,
  title={ZipIt! Merging Models from Different Tasks without Training},
  author={Stoica, George and Bolya, Daniel and Bjorner, Jakob Brandt and Ramesh, Pratik and Hearn, Taylor and Hoffman, Judy},
  booktitle={ICLR},
  year={2024},
  url={https://arxiv.org/abs/2305.03053}
}

@inproceedings{lim2024graph,
  title={Graph Metanetworks for Processing Diverse Neural Architectures},
  author={Lim, Derek and Maron, Haggai and Law, Marc T and Lorraine, Jonathan and Lucas, James},
  booktitle={ICLR},
  year={2024},
  url={2312.04501}
}

@inproceedings{kofinas2024graph,
  title={Graph Neural Networks for Learning Equivariant Representations of Neural Networks},
  author={Kofinas, Miltiadis and Knyazev, Boris and Zhang, Yan and Chen, Yunlu and Burghouts,
    Gertjan J. and Gavves, Efstratios and Snoek, Cees G. M. and Zhang, David W.},
  booktitle={ICLR},
  year={2024},
  url={https://arxiv.org/abs/2403.12143}
}

@inproceedings{knyazev2024accelerating,
  title={Accelerating Training with Neuron Interaction and Nowcasting Networks},
  author={Knyazev, Boris and Moudgil, Abhinav and Lajoie, Guillaume and Belilovsky, Eugene and Lacoste-Julien, Simon},
  booktitle={ICLR},
  year={2025},
  url={https://arxiv.org/abs/2409.04434}
}
