<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://bknyaz.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://bknyaz.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-27T04:54:08+00:00</updated><id>https://bknyaz.github.io//feed.xml</id><title type="html">blank</title><subtitle>Boris Knyazev&apos;s homepage. </subtitle><entry><title type="html">Optimizing LLMs Faster by Learning Connections: Neuron Interaction and Nowcasting Networks</title><link href="https://bknyaz.github.io//2025/09/30/optimizing-llms-faster-by-learning-connections-neuron-interaction-and-nowcasting-networks.html" rel="alternate" type="text/html" title="Optimizing LLMs Faster by Learning Connections: Neuron Interaction and Nowcasting Networks"/><published>2025-09-30T19:05:20+00:00</published><updated>2025-09-30T19:05:20+00:00</updated><id>https://bknyaz.github.io//2025/09/30/optimizing-llms-faster-by-learning-connections-neuron-interaction-and-nowcasting-networks</id><content type="html" xml:base="https://bknyaz.github.io//2025/09/30/optimizing-llms-faster-by-learning-connections-neuron-interaction-and-nowcasting-networks.html"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*i-R7chFd4RawxkpE"/><figcaption>ICLR 2025 was held in Singapore (image source: exploreworldwide.ca)</figcaption></figure> <p>Training large neural networks is famously slow and expensive. In our recent paper, <a href="https://arxiv.org/abs/2409.04434"><em>Accelerating Training with Neuron Interaction and Nowcasting Networks</em></a>, presented at <strong>ICLR 2025 in Singapore</strong>, we introduced a new way to speed things up: treat a neural network as what it really is‚Ää‚Äî‚Ää<a href="https://arxiv.org/abs/2403.12143">a graph of interacting neurons (or ‚Äúneural¬†graph‚Äù)</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/835/1*mWhiD4YWTqV1tkzhdIqGKQ.png"/><figcaption>Adam <strong>without </strong>and <strong>with </strong>nowcasting using our NiNo on a language task that NiNo has not seen during its training. Note the <strong>~2x reduction</strong> of the number of steps required by NiNo to achieve the same validation perplexity as by¬†Adam.</figcaption></figure> <h3>From Adam to¬†NiNo</h3> <p>Due to massive training costs, recent years have seen a surge of faster adaptive optimizers: <a href="https://arxiv.org/abs/1802.09568">Shampoo</a>, <a href="https://kellerjordan.github.io/posts/muon/">Muon</a>, <a href="https://arxiv.org/abs/2409.11321">SOAP</a>, and <a href="https://arxiv.org/pdf/2507.11005">others</a>. Each improves on Adam by smarter scaling and/or orthogonalization of parameter updates for a given weight matrix, often inspired by second-order methods or preconditioning. But they work at the level of parameters or layers, not the <em>network as a whole</em>. Moreover, they do not learn from the previous optimization runs, i.e. the optimization algorithms are based on manually-designed gradient-descent rules.</p> <p>Our method, <strong>NiNo (Neuron Interaction and Nowcasting)</strong>, is different. We model <strong>neurons as nodes</strong> and <strong>weights as edges</strong>, and use a graph neural network (GNN) to predict how weights will evolve. This lets us ‚Äúnowcast‚Äù future parameters and reduce the number of steps required to reach the same performance metric.</p> <p>So NiNo is essentially a GNN that takes a history of past parameter values along the optimization trajectory (which can be obtained by Adam or another optimizer) and making a jump by predicting future parameter values. After making the prediction, optimization is continued with Adam, then followed by NiNo‚Äôs another prediction and so on. This basic idea is borrowed from <a href="https://proceedings.mlr.press/v202/jang23b.html">Weight Nowcaster Network (WNN)</a> that revealed predictable patterns in optimization trajectories, but <strong>neural graphs synergized with GNNs</strong> make it work really¬†well.</p> <p>As NiNo is a neural network, it needs to be trained first before we can use it to speed up optimization. To do so, we collected and publicly released a <a href="https://huggingface.co/datasets/SamsungSAILMontreal/nino_metatrain">dataset of checkpoints</a> with optimization trajectories on 4 vision and language tasks. Even though collecting these checkpoints and training NiNo is computationally expensive, this single-time cost is amortized meaning <strong>the same trained NiNo can be potentially used on millions of tasks to eventually save much more¬†costs.</strong></p> <p>To make NiNo work well for LLMs and Transformers in general, it was critical to carefully construct the neural graph for multi-head self-attention, making it stand out compared to WNN (that ignores the neural network structure). This is a tricky part as the illustration below shows, but we <strong>implemented it for many different Transformer layers</strong>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/982/1*mnrqkp_vUCsN4PJQfdQ3Zw.png"/><figcaption>Constructing neural graphs for a MSA layer (see the details in our paper and¬†code).</figcaption></figure> <h3>Results that Stand¬†Out</h3> <ul><li><em>Figure 1 in the paper</em>: NiNo achieves a roughly <strong>~2x speedup compared to Adam</strong>, which is generally unreachable by manually-designed optimizers (which are considered remarkable if a 1.2‚Äì1.3x speedup is achieved).</li><li><em>Table 2 in the paper</em>: NiNo speeds up optimization across 9 vision and language tasks that we systematically evaluated, achieving the same validation performance (accuracy or perplexity) as Adam in <strong>about half the steps on average across these¬†tasks.</strong></li><li><em>Table 4 in the paper</em>: NiNo continues to speed up training for <strong>larger models</strong> beyond our main 9 tasks (that are much larger than the models in the training checkpoints used to train¬†NiNo).</li></ul> <h3>Learning to Optimize, Revisited</h3> <p>Our work connects to the broader <a href="https://people.idsia.ch/~juergen/diploma1987ocr.pdf">‚Äúlearning to optimize‚Äù literature</a>, with seminal Luke Metz‚Äôs et al. works such as <a href="https://arxiv.org/abs/2211.09760"><em>VeLO: Training Versatile Learned Optimizers by Scaling Up</em></a>. Unlike many learned optimizers that struggle with cost, stability or show only a ~1.2‚Äì1.3x speedup¬π, NiNo is conceptually lightweight and stable‚Ää‚Äî‚Ääbecause it is only applied every 1,000 steps (by default), while for all the other steps any base optimizer, such as Adam, can be applied to allow for stable convergence. At the same time, recent learned optimizers such as our recent <a href="https://openreview.net/forum?id=SLqJbt4emY"><em>Celo: Training Versatile Learned Optimizers on a Compute Diet</em></a><em> </em>and <a href="https://arxiv.org/abs/2406.00153"><em>ŒºLO: Compute-Efficient Meta-Generalization of Learned Optimizers</em></a> make a significant step in improving learned optimizers and making them more cost-effective and stable (i.e. without big loss spikes) to train and¬†use.</p> <p><em>¬πwhile a ~1.2‚Äì1.3x speedup is remarkable, in practice it usually doesn‚Äôt justify the immense amount of extra work (e.g. efficient distributed implementation, tuning, potential instabilities especially in mixed/low-bit precision, investigating unexpected side effects like overfitting or poor generalization) that is required for actual large-scale usefulness.</em></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HTwrCPlWh0vveuMha2ucKQ.png"/><figcaption>Neural graph of a Llama-3 based architecture (graph and adjacency matrix are visualized, see the paper for details).</figcaption></figure> <h3>Looking Forward</h3> <p>Even though NiNo shows great speedups, it requires further work, so we see many exciting paths¬†ahead:</p> <ul><li>Making NiNo‚Äôs step more scalable (especially memory-efficient) to larger models‚Ää‚Äî‚Ääcurrently the message passing step in its GNN is a bottleneck</li><li>Improving speedups on larger tasks‚Ää‚Äî‚Ääfor tasks with &gt;1B parameter models we currently observe no¬†speedup</li><li>Adding automatic ways to construct neural¬†graphs</li><li>Combining NiNo with learned optimizers to get the merits of¬†both</li><li>Showing theoretical guarantees for convergence</li></ul> <p>We‚Äôve open-sourced our code at <a href="https://github.com/SamsungSAILMontreal/nino">github.com/SamsungSAILMontreal/nino</a> under MIT License and welcome contributions. As we show below, applying NiNo is straightforward:</p> <pre>import torch<br />import torch.nn.functional as F<br />from transformers import AutoModelForCausalLM<br />from optim import NiNo<br /><br />model = AutoModelForCausalLM.from_config(...)  # some model<br /><br /># NiNo is implemented as a wrapper around the base optimizer<br /># any optimizer other than Adam should also be possible to use with NiNo<br />opt = NiNo(base_opt=torch.optim.AdamW(model.parameters(), <br />           lr=1e-3, <br />           weight_decay=1e-2),<br />           ckpt=&#39;checkpoints/nino.pt&#39;,<br />           subgraph=False, # can be set to True for larger models (see Llama 3.2 example below)<br />           edge_sample_ratio=0,  # can be set to a small positive number for larger models (see Llama 3.2 example below)<br />           model=model,<br />           period=1000,<br />           max_train_steps=10000)<br />for step in range(10000):<br />    if opt.need_grads:  # True/False based on the step number and period<br />        opt.zero_grad()  # zero out gradients<br />        data, targets = ...  # get some batch of data<br />        # base optimizer step (majority of the time)<br />        outputs = model(data)  # forward pass<br />        loss = F.cross_entropy(outputs, targets)  # compute some loss<br />        loss.backward()  # only compute gradients for the base optimizer            <br />    opt.step()  # base_opt step or nowcast params every 1000 steps using NiNo    <br />    ...</pre> <p>To conclude, by recognizing that neural networks are <em>networks</em>‚Ää‚Äî‚Äägraphs of interacting parts‚Ää‚Äî‚Ääwe may be opening the door to a new generation of efficient and learnable optimizers.</p> <p>See <a href="https://bknyaz.github.io/">my website</a> for my other latest research and updates (<em>if you are a student passionate about this kind of topic, contact me for a potential collaboration</em>).</p> <p>Updated on Sep 30,¬†2025.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d9a722309eab" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Can we do better than Convolutional Neural Networks?</title><link href="https://bknyaz.github.io//2019/09/30/can-we-do-better-than-convolutional-neural-networks.html" rel="alternate" type="text/html" title="Can we do better than Convolutional Neural Networks?"/><published>2019-09-30T13:08:11+00:00</published><updated>2019-09-30T13:08:11+00:00</updated><id>https://bknyaz.github.io//2019/09/30/can-we-do-better-than-convolutional-neural-networks</id><content type="html" xml:base="https://bknyaz.github.io//2019/09/30/can-we-do-better-than-convolutional-neural-networks.html"><![CDATA[<h4>PyTorch Implementation of ‚ÄúImage Classification with Hierarchical Multigraph Networks‚Äù from BMVC¬†2019</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/995/0*DQEo8wicTlkyZeC1"/><figcaption>The number of pixels in the top row is 11, 7 and 1000 times larger (from left to right) than the number of ‚Äúsuperpixels‚Äù in the bottom row. Can we use the superpixels rather than raw pixels as input and improve on convolutional neural networks?</figcaption></figure> <p>The British Machine Vision Conference (BMVC), finished about two weeks ago in Cardiff, UK, is one of the <a href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_computervisionpatternrecognition">top conferences in computer vision &amp; pattern recognition</a> with a competitive acceptance rate of 28%. Compared to others, it‚Äôs a small event, so you have plenty of time to walk around posters and talk to presenters one-on-one, which I found really¬†nice.</p> <h3>BMVC 2019 on Twitter</h3> <p>Paper decisions were released yesterday. Congratulations to all of you! In total, we received 1008 submissions, of which 815 were valid. Of these, a total of 231 papers were accepted (38 as Oral Presentations, 193 as Poster Presentations). This amounts to a 28% acceptance rate.</p> <p>I presented a poster on <a href="https://bmvc2019.org/wp-content/uploads/papers/1186-paper.pdf"><strong>Image Classification with Hierarchical Multigraph Networks</strong></a><strong> </strong>on which I mainly worked during my internship at <a href="https://www.sri.com/">SRI International</a> under the supervision of <a href="https://filebox.ece.vt.edu/~linxiao/"><em>Xiao Lin</em></a><em>,</em> <a href="https://medium.com/u/6cf41cb2c546">Mohamed Amer</a> <em>(</em><a href="https://mohamedramer.com/"><em>homepage</em></a><em>) </em>and my PhD advisor <a href="https://www.gwtaylor.ca/"><em>Graham¬†Taylor</em></a><em>.</em></p> <p>In the paper, we basically try to answer the question ‚ÄúCan we do better than Convolutional Neural Networks?‚Äù. Here I discuss this question and support my arguments by results. I also walk you through the forward pass of the whole pipeline for a single image from <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">PASCAL VOC 2012</a> using¬†PyTorch.</p> <p><strong>The complete code</strong> for this post is in <a href="https://github.com/bknyaz/bmvc_2019">my notebook on Github.</a> It should be easy to adapt it for training and validating on the whole PASCAL¬†dataset.</p> <p>So, why do we want to do better than ConvNets? Haven‚Äôt they outperformed humans in many¬†tasks?</p> <p>For example, you could say that <strong>image classification</strong> is a solved task. Well, in terms of ImageNet, yes. But despite great contribution of ImageNet, it is a weird task. Why would you want to discriminate between hundreds of dog breeds? So, in result we have models succeeding in that, but failing to discriminate between slightly rotated dogs and cats. Fortunately, we now have <a href="https://arxiv.org/abs/1903.12261">ImageNet-C</a> and <a href="https://arxiv.org/abs/1907.07484">other similar benchmarks</a> showing that we are nowhere close to solving¬†it.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/712/1*d5ZkQXZA73ASjq7B0UtDwQ.png"/><figcaption>Pipeline: We solve the classical task of image classification.</figcaption></figure> <p>Another open problem arising in related tasks, like object detection, is training on really large images (e.g., 4000√ó3000), which is addressed, for example, by <a href="https://arxiv.org/abs/1905.03711">Katharopoulos &amp; Fleuret (ICML, 2019</a>) and <a href="https://bmvc2019.org/wp-content/uploads/papers/0555-paper.pdf">Ramapuram et al. (BMVC, 2019</a>). Thanks to the latter I now know that if the background of a poster is black, then it‚Äôs likely from Apple. I should reserve some color¬†too!</p> <p>So, maybe we need something different than a Convolutional Neural Network? Instead of constantly patching its <a href="https://distill.pub/2019/advex-bugs-discussion/">bugs</a>, maybe we should use a model that has nicer properties from the beginning?</p> <p>We argue that such a model <em>can be</em><strong> </strong>a <strong>Graph Neural Network (GNN)‚Ää</strong>‚Äî‚Ääa neural network that can learn from graph-structured data. GNNs have some appealing properties. For example, compared to ConvNets, GNNs are inherently rotation and translation invariant, because there is simply no notion of rotation or translation in graphs, i.e. there is no left and right, there are only ‚Äúneighbors‚Äù in some sense (<a href="https://arxiv.org/abs/1703.00356">Khasanova &amp; Frossard, ICML, 2017</a>). So, the problem of making a <a href="https://arxiv.org/abs/1602.07576">ConvNet generalize better to different rotations</a>, that people have been trying to solve for years, is solved automatically with¬†GNNs!</p> <p>Regarding learning from large images, how about extracting <a href="https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic">superpixels</a> from images and feeding a much lower dimensional input to a GNN instead of feeding a downsampled (e.g. 224√ó224) image to a ConvNet? Superpixels seem to be a much better way to downsample an image compared to, say, bilinear interpolation, because they often preserve a lot of semantics by keeping the boundaries between objects. With a ConvNet we cannot directly learn from this kind of an input, however, there are some nice works proposing to leverage them (<a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14445">Kwak et al., AAAI,¬†2017</a>).</p> <p>So, a GNN sounds wonderful! Let‚Äôs see how it performs in practice.</p> <p>Oh no! Our baseline GNN based on (<a href="https://arxiv.org/abs/1609.02907">Kipf &amp; Welling, ICLR, 2017</a>) achieves merely 19.2% (mean average precision or mAP) on PASCAL, compared to 32.7% of a ConvNet with the same number of layers and filters in each¬†layer.</p> <p>We propose several improvements that eventually beat the¬†ConvNet!</p> <h3>1. Hierarchical Graph</h3> <p>In ConvNets, the hierarchical structure of images is implicitly modeled by pooling layers. In GNNs, you can achieve this in at least two ways. First, you can use pooling similar to ConvNets, but for graphs, defining a fast and good pooling method is really challenging. Instead, we can compute superpixels at multiple scales and pool superpixels by their correspondence to a larger parent superpixel. However, for some reasons this kind of pooling didn‚Äôt work well in our case (I still think it should work well). So, instead we model a hierarchy at the input level. In particular, we combine superpixels of all scales into a single set and compute hierarchical relations based on intersection over union (IoU), commonly used in semantic segmentation.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cIkKRFPlIxFtgto0B7B8fw.png"/><figcaption>Three scales of 1000, 300 and 7 superpixels computed by SLIC. Note that the <a href="https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic">SLIC algorithm</a> that we use often returns fewer superpixels (shown on top of each image) than we request. In the middle image I show spatial connections in yellow, while in the right image‚Ää‚Äî‚Äähierarchical ones that allow to connect remote¬†nodes.</figcaption></figure> <p>Based on that principle, I build the hierarchical graph in the code below. I also build a multiscale version of the spatial graph, but it encodes only spatial relationships, while IoU should better encode hierarchical ones. For example, using IoU we can create <strong>shortcuts between remote child nodes</strong>, i.e. connect two small superpixels (e.g., wheels) that are far away spatially, but belong to the same parent node (e.g., a car) as shown on the image¬†above.</p> <p>And indeed, the hierarchical graph boosts mAP to 31.7%, making it just 1% lower than a ConvNet while having 4 times fewer trainable parameters! If we use only the spatial multiscale graph, the results are much worse as explored in the¬†paper.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/adf95c091aff8e3f68bc54fed950fe04/href">https://medium.com/media/adf95c091aff8e3f68bc54fed950fe04/href</a></iframe> <p>Great! What else can we do to further improve¬†results?</p> <h3>2. Learnable relations</h3> <p>So far, if we visualize our filters, they will look very primitive (as Gaussians). See <a href="https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-1-3d9fada3b80d">my tutorial on GNNs</a> for more details. We want to learn some edge detectors similar to ConvNets, because they work so well. But it turned out to be very challenging to learn them with GNNs. To do that, we basically need to generate edges between superpixels depending on the difference between coordinates. By doing so, we will endow our GNN with the ability to understand the coordinate system (rotation, translation). We will use a 2 layer neural network defined in PyTorch like¬†this:</p> <pre>pred_edge = nn.Sequential(nn.Linear(2, 32),<br />                          nn.ReLU(True),<br />                          nn.Linear(32, L))</pre> <p>where <em>L</em> is the number of predicted edges or the number of filters, such as 4 in the visualization below.</p> <p>We restrict the filter to learn edges only based on the absolute difference between coordinates, |(<em>x‚ÇÅ,y‚ÇÅ</em>) - (<em>x‚ÇÇ,y‚ÇÇ</em>)|, instead of raw values, so that the filters become symmetric. This limits the capacity of filters, but it is still much better than a simple Gaussian filter used by our baseline¬†GCN.</p> <p>In <a href="https://github.com/bknyaz/bmvc_2019/blob/master/bmvc_2019.ipynb">my Jupyter notebook</a>, I created a class LearnableGraph that implements the logic to predict edges given node coordinates (or any other features) and the spatial graph. The latter is used to define a small local neighborhood around each node to avoid predicting edges for all possible node pairs, because it‚Äôs expensive and doesn‚Äôt make much sense to connect very remote superpixels.</p> <p>Below, I visualize the trained pred_edge function. To do that, I assume that the current node with index 1, where we apply the convolution, is in the center of a coordinate system, <em>(x‚ÇÅ,y‚ÇÅ)=0</em>. Then I simply sample coordinates of other nodes, <em>(x‚ÇÇ,y‚ÇÇ)</em>, and feed them to pred_edge. The color shows the strength of an edge depending on the distance from a center¬†node.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uQSsI6hajHdYvcWrdxpBpg.png"/><figcaption>Visualization of predicted edges which can be interpreted as filters, where each intensity value is an edge between two nodes at a distance specified in¬†axes.</figcaption></figure> <p>The learned graph is also very powerful, but at a larger computational cost, which is negligible if we generate a very sparse graph. The result of 32.3% is just 0.4% lower than a ConvNet and can be easily improved if we generate more¬†filters!</p> <h3>3. Multiscale GNN</h3> <p>We now have <strong>three graphs: spatial, hierarchical and learned</strong>. A single graph convolutional layer with the spatial or hierarchical graph permits feature propagation only within the ‚Äúfirst neighbors‚Äù. Neighbors are soft in our case, since we use a Gaussian to define the spatial graph and IoU for the hierarchical one. <a href="https://arxiv.org/abs/1606.09375">Defferrard et al. (NIPS, 2016</a>) proposed a multiscale (multihop) graph convolution, which aggregates features within a <em>K</em>-hop neighborhood and approximates spectral graph convolution. See <a href="https://towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49">my other post</a> for an extensive explanation of this method. For our spatial graph, it essentially corresponds to using multiple Gaussians of different width. For the hierarchical graph, this way we can create <em>K</em>-hop<strong> </strong>shortcuts between remote child nodes. For the learned graph, this method will create multiple scales of the learned filters visualized above.</p> <p>Using multiscale graph convolution, implemented in my GraphLayerMultiscale class, turned out to be extremely important allowing us to <strong>outperform</strong> the baseline ConvNet by¬†0.3%!</p> <h3>4. Improving fusion of relation types at low¬†cost</h3> <p><strong>So far, to learn from our three graphs, we have used a standard concatenation method. </strong>This method, however, has a couple of problems. <strong>First</strong>, the number of trainable parameters of such a fusion operator is linear w.r.t. the input and output feature dimensionalities, scale (<em>K)</em> and number of relation types, so it can really grow fast if we increase two or more of these parameters at once. <strong>Second</strong>, the relation types we try to fuse can have very different natures and occupy very different subspaces of a manifold. To solve both problems at the same time, we propose learnable projections similar to (<a href="https://arxiv.org/abs/1811.09595">Knyazev et al., NeurIPS-W, 2018</a>). This way we decouple the linear dependency reducing the number of parameters by a factor of 2‚Äì3 compared to concatenation. In addition, learnable projections transform multirelational features so that they should occupy nearby subspaces of the manifold, facilitating the propagation of information from one relationship to¬†another.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zBplfQQJrFs05XEzu8aKyQ.png"/><figcaption>One of the proposed relation type fusion methods, which performs very well on PASCAL and allows us to beat the ConvNet by a quite large¬†margin.</figcaption></figure> <p>By using the proposed fusion method, implemented in the GraphLayerFusion class below, we achieve 34.5% beating the ConvNet by 1.8%, while having 2 times fewer parameters! Quite impressive for the model that initially didn‚Äôt know anything about the spatial structure of images, except for information encoded in superpixels. It would be interesting to explore other fusion methods, like <a href="https://arxiv.org/abs/1803.09374">this one</a>, to get even better¬†results.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/41cea34f4340597b38a3d2eb15be9e4c/href">https://medium.com/media/41cea34f4340597b38a3d2eb15be9e4c/href</a></iframe> <h3>Conclusion</h3> <p>It turned out that with a multirelational graph network and some tricks, we can do better than a Convolutional Neural¬†Network!</p> <p>Unfortunately, during our process of improving the GNN we slowly lost its invariance properties. For example, the shape of superpixels might change after rotating the image, and superpixel coordinates that we use for node features to improve the model also make it less¬†robust.</p> <p>Nevertheless, our work is a small step towards a better image reasoning model and we show that GNNs can pave a promising direction.</p> <p>See <a href="https://github.com/bknyaz/bmvc_2019">my notebook on Github</a> for implementation details.</p> <p>I also highly recommend <a href="https://medium.com/u/24cd20b3728e">Matthias Fey</a>‚Äôs Master‚Äôs thesis with <a href="https://github.com/rusty1s/embedded_gcnn">the code</a> on a very related¬†topic.</p> <p>Find me on <a href="https://github.com/bknyaz/">Github</a>, <a href="https://www.linkedin.com/in/boris-knyazev-39690948/">LinkedIn</a> and <a href="https://twitter.com/BorisAKnyazev">Twitter</a>. <a href="https://bknyaz.github.io/">My homepage</a>.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=46ed90fed807" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/can-we-do-better-than-convolutional-neural-networks-46ed90fed807">Can we do better than Convolutional Neural Networks?</a> was originally published in <a href="https://medium.com/data-science">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Spectral Graph Convolution Explained and Implemented Step By Step</title><link href="https://bknyaz.github.io//2019/08/15/spectral-graph-convolution-explained-and-implemented-step-by-step.html" rel="alternate" type="text/html" title="Spectral Graph Convolution Explained and Implemented Step By Step"/><published>2019-08-15T22:56:19+00:00</published><updated>2019-08-15T22:56:19+00:00</updated><id>https://bknyaz.github.io//2019/08/15/spectral-graph-convolution-explained-and-implemented-step-by-step</id><content type="html" xml:base="https://bknyaz.github.io//2019/08/15/spectral-graph-convolution-explained-and-implemented-step-by-step.html"><![CDATA[<h4>As part of the ‚ÄúTutorial on Graph Neural Networks for Computer Vision and¬†Beyond‚Äù</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Vx6uqv12rzb8HeUZl8d7-g.png"/><figcaption>The Fourier basis (DFT matrix) on the left, in which each column or row is a basis vector, reshaped to 28√ó28 (on the right), i.e. 20 basis vectors are shown on the right. The Fourier basis is used to compute spectral convolution is signal processing. In graphs, the Laplacian basis is used described in this¬†post.</figcaption></figure> <p>First, let‚Äôs recall what is a graph. A graph <em>G</em> is a set of <strong>nodes</strong> (vertices) connected by directed/undirected <strong>edges</strong>. In this post, I will assume an undirected graph <em>G</em> with <em>N</em> nodes. Each <strong>node</strong> in this graph has a <em>C</em>-dimensional feature vector, and features of all nodes are represented as an <em>N</em>√ó<em>C</em> dimensional matrix <em>X‚ÅΩÀ°‚Åæ. </em><strong>Edges</strong> of a graph are represented as an <em>N</em>√ó<em>N </em>matrix A, where the entry A<em>·µ¢‚±º</em> indicates if node <em>i</em> is connected (<em>adjacent</em>) to node <em>j</em>. This matrix is called an <em>adjacency matrix</em>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/754/1*68Gcr70UTpdaX7THbZSEcA.png"/><figcaption>Two undirected graphs with N=5 and N=6 nodes. The order of nodes is arbitrary.</figcaption></figure> <p>Spectral analysis of graphs (see lecture notes <a href="http://www.cs.yale.edu/homes/spielman/561/">here</a> and earlier work <a href="https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering">here</a>) has been useful for graph clustering, community discovery and other <em>mainly unsupervised </em>learning tasks. In this post, I basically describe the work of <a href="https://arxiv.org/abs/1312.6203">Bruna et al., 2014, ICLR 2014</a> who combined spectral analysis with convolutional neural networks (ConvNets) giving rise to spectral <strong>graph convolutional networks </strong>that can be trained in a <em>supervised </em>way, for example for the graph classification task.</p> <p>Despite that <em>spectral</em> graph convolution is currently less commonly used compared to <em>spatial</em> graph convolution methods, knowing how spectral convolution works is still helpful to understand and avoid potential problems with other methods. Plus, in the conclusion I refer to some recent exciting works making spectral graph convolution more competitive.</p> <h3>1. Graph Laplacian and a little bit of¬†physics</h3> <p>While ‚Äúspectral‚Äù may sound complicated, for our purpose it‚Äôs enough to understand that it simply means <em>decomposing</em> a signal/audio/image/graph into a combination (usually, a sum) of simple elements (wavelets, graphlets). To have some nice properties of such a <em>decomposition</em>, these simple elements are usually <em>orthogonal</em>, i.e. mutually linearly independent, and therefore form a¬†<em>basis</em>.</p> <p>When we talk about ‚Äúspectral‚Äù in signal/image processing, we imply the <a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform">Fourier Transform</a>, which offers us a particular <em>basis</em> (<a href="https://en.wikipedia.org/wiki/DFT_matrix">DFT matrix</a>, e.g. scipy.linalg.dft in Python) of elementary sine and cosine waves of different frequencies, so that we can represent our signal/image as a sum of these waves. But when we talk about graphs and graph neural networks (GNNs), ‚Äúspectral‚Äù implies <em>eigen-decomposition</em> of the <a href="https://en.wikipedia.org/wiki/Laplacian_matrix"><strong>graph Laplacian</strong></a><strong> </strong><em>L.</em> You can think of the the graph Laplacian <em>L</em> as an adjacency matrix <em>A</em> normalized in a special way, whereas <em>eigen-decomposition</em> is a way to find those elementary orthogonal components that make up our¬†graph.</p> <p>Intuitively, the graph Laplacian shows in what directions and how <em>smoothly</em> the ‚Äúenergy‚Äù will diffuse over a graph if we put some ‚Äúpotential‚Äù in node <em>i</em>. A typical use-case of Laplacian in mathematics and physics is to solve how a signal (wave) propagates in a dynamic system. Diffusion is <em>smooth</em> when there is no sudden changes of values between neighbors as in the animation below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/560/1*gz2hyrcSSJG9MtDzmQLe3w.gif"/><figcaption>Diffusion of some signal (for example, it can be heat) in a regular grid graph computed based on the graph Laplacian (<a href="https://en.wikipedia.org/wiki/Laplacian_matrix">source</a>). Basically, the only things required to compute these dynamics are the Laplacian and initial values in nodes (pixels), i.e. red and yellow pixels corresponding to high intensity (of¬†heat).</figcaption></figure> <p>In the rest of the post, I‚Äôm going to assume ‚Äú<em>symmetric normalized Laplacian</em>‚Äù, which is often used in graph neural networks, because it is normalized so that when you stack many graph layers, the node features propagate in a more smooth way without explosion or vanishing of feature values or gradients. It is computed based <em>only</em> on an adjacency matrix <em>A</em> of a graph, which can be done in a few lines of Python code as¬†follows:</p> <pre><strong># Computing the graph Laplacian<br /># A is an adjacency matrix of some graph <em>G</em><br /></strong>import numpy as np</pre> <pre>N = A.shape[0] <strong># number of nodes in a graph</strong><br />D = np.sum(A, 0) <strong># node degrees</strong><br />D_hat = np.diag((D + 1e-5)**(-0.5)) <strong># normalized node degrees</strong><br />L = np.identity(N) ‚Äî np.dot(D_hat, A).dot(D_hat) <strong># Laplacian</strong></pre> <p>Here, we assume that <em>A</em> is symmetric, i.e. <em>A</em> = <em>A</em>·µÄ and our graph is undirected, otherwise node degrees are not well-defined and some assumptions must be made to compute the Laplacian. An interesting property of an adjacency matrix <em>A </em>is that <em>A‚Åø</em> (matrix product taken <em>n</em> times) exposes <em>n</em>-hop connections between nodes (see <a href="https://en.wikipedia.org/wiki/Adjacency_matrix#Matrix_powers">here</a> for more details).</p> <p>Let‚Äôs generate three graphs and visualize their adjacency matrices and Laplacians as well as their¬†powers.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WSvWVAsQsGtQQpIcrCPOOQ.png"/><figcaption>Adjacency matrices, Laplacians and their powers for a random graph (left), ‚Äústar graph‚Äù (middle) and ‚Äúpath graph‚Äù (right). I normalize A¬≤ such that the sum in each row equals 1 to have a probabilistic interpretation of 2-hop connections. Notice that Laplacians and their powers are symmetric matrices, which makes eigen-decomposition easier as well as facilitates feature propagation in a deep graph¬†network.</figcaption></figure> <p>For example, imagine that the star graph above in the middle is made from metal, so that it transfers heat well. Then, if we start to heat up node 0 (dark blue), this heat will propagate to other nodes in a way defined by the Laplacian. In the particular case of a star graph with all edges equal, heat will spread uniformly to all other nodes, which is not true for other graphs due to their structure.</p> <p>In the context of computer vision and machine learning, the graph Laplacian defines how node features will be updated if we stack several graph neural layers. Similarly to <a href="https://medium.com/p/3d9fada3b80d"><em>the first part of my tutorial</em></a>, to understand spectral graph convolution from the computer vision perspective, I‚Äôm going to use the MNIST dataset, which defines images on a 28√ó28 regular grid¬†graph.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EUFjx4cvVq4TdmU1PfXRpA.png"/><figcaption>MNIST image defining features X (left), adjacency matrix A (middle) and the Laplacian (right) of a regular 28√ó28 grid. The reason that the graph Laplacian looks like an identity matrix is that the graph has a relatively large number of nodes (784), so that after normalization values outside the diagonal become much smaller than¬†1.</figcaption></figure> <h3>2. Convolution</h3> <p>In signal processing, it can be shown that convolution in the spatial domain is multiplication in the frequency domain (a.k.a. <a href="https://en.wikipedia.org/wiki/Convolution_theorem">convolution theorem</a>). The same theorem can be applied to graphs. In signal processing, to transform a signal to the frequency domain, we use the Discrete Fourier Transform, which is basically matrix multiplication of a signal with a special matrix (basis, DFT matrix). This basis assumes a <em>regular</em> grid, so we cannot use it for <em>irregular</em> graphs, which is a typical case. Instead, we use a more general basis, which is eigenvectors <em>V</em> of the graph Laplacian <em>L</em>, which can be found by eigen-decomposition:<em> L</em>=<em>VŒõV·µÄ</em>, where <em>Œõ</em> are eigenvalues of¬†<em>L.</em></p> <p><strong>PCA vs eigen-decomposition of the graph Laplacian. </strong>To compute spectral graph convolution in practice, it‚Äôs enough to use a few eigenvectors corresponding to the <em>smallest</em> eigenvalues. At first glance, it seems to be an opposite strategy compared to frequently used in computer vision <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal component analysis (PCA)</a>, where we are more interested in the eigenvectors corresponding to the <em>largest</em> eigenvalues. However, this difference is simply due to the <em>negation</em> used to compute the Laplacian above, therefore eigenvalues computed using PCA are <em>inversely proportional</em> to eigenvalues of the graph Laplcacian (see <a href="http://outobox.cs.umn.edu/PCA_on_a_Graph.pdf">this paper</a> for a formal analysis). Note also that PCA is applied to the covariance matrix of a dataset for the purpose to extract the largest factors of variation, i.e. the dimensions along which data vary the most, like in <a href="https://en.wikipedia.org/wiki/Eigenface">Eigenfaces</a>. This variation is measured by eigenvalues, so that the smallest eigenvalues essentially correspond to noisy or ‚Äúspurious‚Äù features, which are assumed to be useless or even harmful in practice.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/643/1*k8AfLWuLW9sgOsuCarR19Q.png"/><figcaption>Eigenvalues (in a descending order) and corresponding eigenvectors for the MNIST¬†dataset.</figcaption></figure> <p>Eigen-decomposition of the graph Laplacian is applied to a single graph for the purpose to extract subgraphs or clusters (communities) of nodes, and <a href="http://blog.shriphani.com/2015/04/06/the-smallest-eigenvalues-of-a-graph-laplacian/">eigenvalues tell us a lot about graph connectivity</a>. I will use eigenvectors corresponding to the 20 smallest eigenvalues in our examples below, assuming that 20 is much smaller than the number of nodes <em>N (N</em>=784 in case of MNIST<em>)</em>. To find eigenvalues and eigenvectors below on the left, I use a 28√ó28 regular graph, whereas on the right I follow the experiment of <a href="https://arxiv.org/abs/1312.6203">Bruna et al.</a> and construct an irregular graph by sampling 400 random locations on a 28√ó28 regular grid (see their paper for more details about this experiment).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*93nVzwz_V7IPC7TPlgAsjQ.png"/><figcaption>Eigenvalues <em>Œõ (</em><strong><em>bottom</em></strong><em>) and e</em>igenvectors V (<strong>top</strong>) of the graph Laplacian L for a regular 28<em>√ó</em>28 grid (<strong>left</strong>) and non-uniformly subsampled grid with 400 points according to experiments in <a href="https://arxiv.org/abs/1312.6203">Bruna et al., 2014, ICLR 2014</a> (<strong>right</strong>). Eigenvectors corresponding to the 20 <strong>smallest</strong> <strong>eigenvalues</strong> are shown. Eigenvectors are 784 dimensional on the left and 400 dimensional on the right, so V is 784<em>√ó20 and 400√ó20 respectively. </em>Each of the 20 eigenvectors on the left was reshaped to 28<em>√ó</em>28, whereas on the right to reshape a 400 dimensional eigenvector to 28<em>√ó28, white pixels for missing nodes were added. So, e</em>ach pixel in each eigenvector corresponds to a node or a missing node (in white on the right). These eigenvectors can be viewed as a basis in which we decompose our¬†graph.</figcaption></figure> <p>So, given graph Laplacian <em>L</em>, node features <em>X</em> and filters <em>W</em>_spectral, in Python <strong>spectral convolution on graphs</strong> looks very¬†simple:</p> <pre><strong># Spectral convolution on graphs<br /># X is an <em>N√ó1 matrix of 1-dimensional node features<br /></em></strong><strong># L is an </strong><strong><em>N√óN</em> graph Laplacian computed above<br /># W_spectral are </strong><strong><em>N√ó</em></strong><strong><em>F weights (filters) that we want to train<br /></em></strong>from scipy.sparse.linalg import eigsh <strong># assumes </strong><strong><em>L</em></strong><strong> to be symmetric</strong></pre> <pre><em>Œõ</em><em>,V</em> = eigsh(L,k=20,which=‚ÄôSM‚Äô) <strong># eigen-decomposition (i.e. find <em>Œõ</em></strong><strong><em>,V)</em></strong><br />X_hat = V.T.dot(X) <strong># </strong><strong><em>20</em>√ó</strong><strong><em>1</em></strong><strong> node features in the &quot;spectral&quot; domain</strong><br />W_hat = V.T.dot(W_spectral)  <strong># 20√ó<em>F</em> filters in the </strong><strong>&quot;spectral&quot; domain</strong><br />Y = V.dot(X_hat * W_hat)  <strong># </strong><strong><em>N√ó</em></strong><strong><em>F</em></strong><strong> result of convolution</strong></pre> <p>Formally:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wBIfFw54z8usWq_merON8A.png"/><figcaption>Spectral graph convolution, where ‚äô means element-wise multiplication.</figcaption></figure> <p>where we assume that our node features <em>X‚ÅΩÀ°‚Åæ </em>are 1-dimensional, e.g. MNIST pixels, but it can be extended to a <em>C</em>-dimensional case: we will just need to repeat this convolution for each <em>channel</em> and then sum over <em>C</em> as in signal/image convolution.</p> <p>Formula (3) is essentially the same as <a href="https://en.wikipedia.org/wiki/Convolution_theorem">spectral convolution of signals on regular grids</a> using the Fourier Transform, and so creates a few problems for machine learning:</p> <ul><li>the dimensionality of trainable weights (filters) <em>W_</em>spectral depends on the number of nodes <em>N</em> in a¬†graph;</li><li><em>W_</em>spectral also depends on the graph structure encoded in eigenvectors <em>V.</em></li></ul> <p>These issues prevent scaling to datasets with large graphs of variable structure. Further efforts, summarized below, were focused on resolving these and other¬†issues.</p> <h3><strong>3. ‚ÄúSmoothing‚Äù in the spectral¬†domain</strong></h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/412/1*PcKEUB4wTOG6gtoEIZl9iA.png"/><figcaption>Strawberry and banana smoothie (source: <a href="https://joyfoodsunshine.com/strawberry-banana-smoothie/">joyfoodsunshine.com</a>). Smoothing in the spectral domain is a little bit different üòÉ.</figcaption></figure> <p><a href="https://arxiv.org/abs/1312.6203">Bruna et al.</a> were one of the first to apply spectral graph analysis to <em>learn convolutional filters</em> for the graph classification problem. The filters learned using formula (3) above act on the <em>entire graph</em>, i.e. they have <em>global support</em>. In the computer vision context, this would be the same as training convolutional filters of size 28√ó28 pixels on MNIST, i.e. filters have the same size as the input (note that we would still slide a filter, but over a zero-padded image). While for MNIST we can actually train such filters, the common wisdom suggests to avoid that, as it makes training much harder due to the potential explosion of the number of parameters and difficulty of training large filters that can capture useful features shared across different images.</p> <p>I actually successfully trained such a model using PyTorch and <a href="https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py">this code</a> from my GitHub. You should run it using mnist_fc.py --model conv. After training for 100 epochs, the filters look like mixtures of¬†digits:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/864/1*kNftNPG_J4i_pUN40DjAXQ.png"/><figcaption>Examples of filters with <strong>global support</strong> typically used in spectral convolution. In this case, these are 28√ó28 filters learned using a ConvNet with a single convolutional layer followed by ReLU, 7√ó7 MaxPooling and a fully-connected classification layer. To make it clear, the output of the convolutional layer is still 28√ó28 due to zero-padding. Surprisingly, this net achieves 96.7% on MNIST. This can be explained by the simplicity of the¬†dataset.</figcaption></figure> <p>To reiterate, we generally want to make filters smaller and more local (which is not exactly the same as I‚Äôll note¬†below).</p> <p>To enforce that implicitly, they proposed to <em>smooth</em> filters in the spectral domain, which makes them <em>more local</em> in the spatial domain according to the spectral theory. The idea is that you can represent our filter <em>W_</em>spectral from formula (3) as a sum of ùêæ predefined functions, such as splines, and instead of learning <em>N</em> values of <em>W</em>, we learn <em>K </em>coefficients <em>Œ±</em> of this¬†sum:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/832/1*sZoZfh6faYLBm7_Nq3xrQw.png"/><figcaption>We can approximate our N dimensional filter<strong> </strong><em>W_</em>spectral as a finite sum of<em> K</em> functions f, such as splines shown below. So, instead of learning N values of <em>W_</em>spectral, we can learn K coefficients (alpha) of those functions; it becomes efficient when K &lt;&lt;¬†N.</figcaption></figure> <p>While the dimensionality of <em>fk</em> does depend on the number of nodes <em>N</em>, these functions are fixed, so we don‚Äôt learn them. The only thing we learn are coefficients <em>Œ±</em>, and so <em>W_</em>spectral is no longer dependent on <em>N</em>. Neat,¬†right?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/864/1*DJWQBxMX3hZz85pKhma34w.png"/><figcaption>The spline basis used to smooth filters in the frequency domain, thereby making them more local. Splines and other polynomial functions are useful, because we can represent filters as their¬†sums.</figcaption></figure> <p>To make our approximation in formula (4) reasonable, we want <em>K</em>&lt;&lt;<em>N</em> to reduce the number of trainable parameters from <em>N</em> to <em>K</em> and, more importantly, make it independent of <em>N</em>, so that our GNN can digest graphs of any size. We can use different bases to perform this ‚Äúexpansion‚Äù, depending on which properties we need. For instance, cubic splines shown above are known as very smooth functions (i.e. you cannot see knots, i.e. where the pieces of the piecewise spline polynomial meet). The Chebyshev polynomial, which I discuss in <a href="https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49">my another post</a>, has the minimum ùëô‚àû distance between the approximating function. The Fourier basis is the one that preserves most of the signal energy after transformation. Most bases are orthogonal, because it would be redundant to have terms that can be expressed by each¬†other.</p> <p>Note that filters <em>W_</em>spectral are still as large as the input, but their <em>effective width </em>is small. In case of MNIST images, we would have 28√ó28 filters, in which only a small fraction of values would have an absolute magnitude larger than 0 and all of them should be located close to each other, i.e. the filter would be local and effectively small, something like the one below (second from the¬†left):</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1NL_awIic9m5P-IF5_3J2A.png"/><figcaption>From left to right: (first) Input image. (second) Local filter with small effective width. Most values are very close to 0. (third) The result of spectral graph convolution of the MNIST image of digit 7 and the filter. (fourth) The result of spectral convolution using the Fourier transform. These results indicate that spectral graph convolution is quite limited if applied to images, perhaps, due to the weak spatial structure of the Laplacian basis compared to the Fourier¬†basis.</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WtfLzUxDwyU8gwAD8u5HgQ.png"/><figcaption>Reconstruction of the MNIST image using the Fourier and graph Laplacian bases using only M components of V: X‚Äô=V V<em>·µÄX</em>. We can see that the bases compress different patterns in images (orientated edges in the Fourier case and global patterns in the Laplacian case). This makes results of convolutions illustrated above very different.</figcaption></figure> <p>To summarize, smoothing in the spectral domain allowed <a href="https://arxiv.org/abs/1312.6203">Bruna et al.</a> to learn more local filters. The model with such filters can achieve similar results as the model without smoothing (i.e. using our formula (3)), but with much fewer trainable parameters, because the filter size is independent of the input graph size, which is important to scale the model to datasets with larger graphs. However, learned filters <em>W</em>_spectral still depend on eigenvectors <em>V</em>, which makes it challenging to apply this model to datasets with variable graph structures.</p> <h3>Conclusion</h3> <p>Despite the drawbacks of the original spectral graph convolution method, it has been developed a lot and has remained a quite competitive method in some applications, because spectral filters can better capture global complex patterns in graphs, which local methods like GCN (<a href="https://arxiv.org/abs/1609.02907">Kipf &amp; Welling, ICLR, 2017</a>) cannot unless stacked in a deep network. For example, two ICLR 2019 papers, of <a href="https://arxiv.org/abs/1901.01484">Liao et al.</a> on ‚ÄúLanczosNet‚Äù and <a href="https://arxiv.org/abs/1904.07785">Xu et al.</a> on ‚ÄúGraph Wavelet Neural Network‚Äù, address some shortcomings of spectral graph convolution and show great results in predicting molecule properties and node classification. Another interesting work of <a href="https://arxiv.org/abs/1705.07664">Levie et al., 2018</a> on ‚ÄúCayleyNets‚Äù showed strong performance in node classification, matrix completion (recommender systems) and community detection. So, depending on your application and infrastructure, spectral graph convolution can be a good¬†choice.</p> <p>In another part of my <a href="https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49">Tutorial on Graph Neural Networks for Computer Vision and Beyond</a> I explain Chebyshev spectral graph convolution introduced by <a href="https://arxiv.org/abs/1606.09375">Defferrard et al.</a> in 2016, which is still a very strong baseline that has some nice properties and is easy to implement as I demonstrate using¬†PyTorch.</p> <p><em>Acknowledgement: A large portion of this tutorial was prepared during my internship at SRI International under the supervision of </em><a href="https://medium.com/u/6cf41cb2c546"><em>Mohamed Amer</em></a><em> (</em><a href="https://mohamedramer.com/"><em>homepage</em></a><em>) and my PhD advisor Graham Taylor (</em><a href="https://www.gwtaylor.ca/"><em>homepage</em></a><em>). I also thank </em><a href="https://www.linkedin.com/in/carolynaugusta/"><em>Carolyn Augusta</em></a><em> for useful feedback.</em></p> <p>Find me on <a href="https://github.com/bknyaz/">Github</a>, <a href="https://www.linkedin.com/in/boris-knyazev-39690948/">LinkedIn</a> and <a href="https://twitter.com/BorisAKnyazev">Twitter</a>. <a href="https://bknyaz.github.io/">My homepage</a>.</p> <p>If you want to cite this blog post in your paper, please use:<br/><a href="http://twitter.com/misc"><em>@misc</em></a><em>{knyazev2019tutorial,<br/> title={Tutorial on Graph Neural Networks for Computer Vision and Beyond},<br/> author={Knyazev, Boris and Taylor, Graham W and Amer, Mohamed R},<br/> year={2019}<br/>}</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2e495b57f801" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801">Spectral Graph Convolution Explained and Implemented Step By Step</a> was originally published in <a href="https://medium.com/data-science">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Anisotropic, Dynamic, Spectral and Multiscale Filters Defined on Graphs</title><link href="https://bknyaz.github.io//2019/08/12/anisotropic-dynamic-spectral-and-multiscale-filters-defined-on-graphs.html" rel="alternate" type="text/html" title="Anisotropic, Dynamic, Spectral and Multiscale Filters Defined on Graphs"/><published>2019-08-12T17:26:10+00:00</published><updated>2019-08-12T17:26:10+00:00</updated><id>https://bknyaz.github.io//2019/08/12/anisotropic-dynamic-spectral-and-multiscale-filters-defined-on-graphs</id><content type="html" xml:base="https://bknyaz.github.io//2019/08/12/anisotropic-dynamic-spectral-and-multiscale-filters-defined-on-graphs.html"><![CDATA[<h4>As part of the ‚ÄúTutorial on Graph Neural Networks for Computer Vision and¬†Beyond‚Äù</h4> <p><em>I‚Äôm presenting an overview of important Graph Neural Network works, by distilling key ideas and explaining simple intuition behind milestone methods using Python and PyTorch. This post continues </em><a href="https://medium.com/p/3d9fada3b80d"><em>the first part of my tutorial</em></a><em>.</em></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GMDzusqMN82diJjSxgSW8w.png"/><figcaption>Graph of Graph Neural Network (GNN) and related works. Some other important works and edges are not shown to avoid further clutter. For example, there is a large body of works on dynamic graphs that deserve a separate overview. Best viewed on a very wide screen in¬†color.</figcaption></figure> <h3>20+ years of Graph Neural¬†Networks</h3> <p>In the ‚Äú<strong>Graph of Graph Neural Network (GNN) and related works</strong>‚Äù above, I added papers on graphs that I have come across in the last year. In this graph, a directed edge between two works denotes that one paper is based on the other (while not necessary citing it) and a color of the work¬†denotes:</p> <ul><li>Red‚Ää‚Äî‚Ää<strong>spectral methods</strong> (require eigen-decomposition of the graph Laplacian, which will be explained below)</li><li>Green‚Ää‚Äî‚Äämethods that work in the <strong>spatial domain</strong> (do not require eigen-decomposition of the graph Laplacian)</li><li>Blue‚Ää‚Äî‚Ääequivalent to spectral methods, but do not require eigen-decomposition (so, effectively, spatial¬†methods)</li><li>Black‚Ää‚Äî‚Ääare methods complementary to GNNs and agnostic to the choice of a GNN itself (i.e. pooling, attention).</li></ul> <p>Note, that some other important works and edges are not shown to avoid further clutter, and only a tiny fraction of works, highlighted <strong>in bold</strong> boxes, will be covered in this post. Disclaimer: I still found room to squeeze our own recent works there¬†üòä.</p> <p>Most of the important methods are covered in this non-exhaustive list of¬†works:</p> <ul><li>Nicket et al., 2015, <a href="https://arxiv.org/abs/1503.00759">A Review of Relational Machine Learning for Knowledge Graphs</a></li><li>Bronstein et al., 2016, <a href="https://arxiv.org/abs/1611.08097">Geometric deep learning: going beyond Euclidean data</a></li><li>Hamilton et al., 2017, <a href="https://arxiv.org/abs/1709.05584">Representation Learning on Graphs: Methods and Applications</a></li><li>Kipf et al., 2018, <a href="http://tkipf.github.io/misc/SlidesCambridge.pdf">Structured deep models: Deep learning on graphs and beyond</a>, presentation slides.</li><li>Battaglia et al., 2018, <a href="https://arxiv.org/abs/1806.01261">Relational inductive biases, deep learning, and graph¬†networks</a></li><li>Zhang et al., 2018 <a href="https://arxiv.org/abs/1812.04202">Deep Learning on Graphs: A¬†Survey</a></li><li>Zhou et al., 2018, <a href="https://arxiv.org/abs/1812.08434">Graph Neural Networks: A Review of Methods and Applications</a></li><li>Wu et al., 2019, <a href="https://arxiv.org/abs/1901.00596">A Comprehensive Survey on Graph Neural<br/>Networks</a></li><li>Petar Veliƒçkoviƒá, 2019, <a href="https://www.repository.cam.ac.uk/handle/1810/292230">The resurgence of structure in deep neural networks</a>, PhD¬†Thesis.</li><li>NIPS and CVPR <a href="https://sungsoo.github.io/2018/02/01/geometric-deep-learning.html">video tutorials</a></li></ul> <p>The first work where graphs were classified using a neural network seems to be a <strong>1997</strong> paper by <a href="https://ieeexplore.ieee.org/document/572108">Alessandro Sperduti and Antonina Starita on ‚ÄúSupervised Neural Networks for the Classification of Structures‚Äù</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9CdkztOnwaRFkiZTu1gYgQ.png"/><figcaption>A figure from (<a href="https://ieeexplore.ieee.org/document/572108">Sperduti &amp; Starita, 1997</a>), which is strikingly similar to what we are doing now, after more than 20¬†years.</figcaption></figure> <blockquote><a href="https://ieeexplore.ieee.org/document/572108">Sperduti &amp; Starita, 1997</a>: ‚ÄúUntil now neural networks have been used for classifying unstructured patterns and sequences. However, standard neural networks and statistical methods are usually believed to be inadequate when dealing with complex structures because of their feature-based approach.‚Äù</blockquote> <p>From 1997, the body of works on learning from graphs has grown so much and in so many diverse directions that it is very hard to keep track without some smart automated system. I believe we are converging to using methods based on neural networks (based on our formula (2) explained in <a href="https://medium.com/p/3d9fada3b80d"><em>the first part of my tutorial</em></a><em>)</em>, or some combination of neural networks and other¬†methods.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/741/1*Ht9tBXaTrV2gkbMIe7zxMg.png"/><figcaption>Graph neural layer‚Äôs formula (2) from <a href="https://medium.com/p/3d9fada3b80d"><em>the first part of my tutorial</em></a><em> that we will also need in this part. Keep in mind, that if we need to compute a specific loss for the output features or if we need to stack these layers, we apply some activation like ReLU or¬†Softmax.</em></figcaption></figure> <p>To recap the notation we used in the first part, we have some undirected graph <em>G</em> with <em>N</em> nodes. Each node in this graph has a <em>C</em>-dimensional feature vector, and features of all nodes are represented as an <em>N</em>√ó<em>C</em> dimensional matrix <em>X‚ÅΩÀ°‚Åæ. </em>In a typical graph network, such as GCN (<a href="https://arxiv.org/abs/1609.02907">Kipf &amp; Welling, ICLR, 2017</a>), we feed these features <em>X‚ÅΩÀ°‚Åæ</em> to a graph neural layer with <em>C</em>√ó<em>F</em> dimensional trainable weights <em>W‚ÅΩÀ°‚Åæ¬†</em>, so that the output of this layer is an <em>N</em>√ó<em>F</em> matrix <em>X‚ÅΩÀ°‚Å∫¬π</em>‚Åæ encoding updated (and hopefully better in some sense) node features. ùìê is an <em>N</em>√ó<em>N </em>matrix, where the entry ùìê<em>·µ¢‚±º</em> indicates if node <em>i</em> is connected (<em>adjacent</em>) to node <em>j</em>. This matrix is called an <em>adjacency matrix</em>. I use ùìê instead of plain <em>A</em> to highlight that this matrix can be <em>normalized</em> in a way to facilitate feature propagation in a deep network. For the purpose of this tutorial, we can assume that ùìê=<em>A</em>, i.e. each <em>i</em>-th<em> </em>row of the matrix product ùìê<em>X‚ÅΩÀ°‚Åæ </em>will contain a sum of features of node <em>i</em> neighbors.</p> <p>In the rest of this part of the tutorial, I‚Äôll briefly explain works of my choice showed in <strong>bold</strong> boxes in the overview graph. I recommend <a href="https://arxiv.org/abs/1611.08097">Bronstein et al.‚Äôs review</a> for a more comprehensive and formal analysis.</p> <p>Note that even though I dive into some technical details of <strong>spectral graph convolution</strong> below, many recent works (e.g., GIN in <a href="https://arxiv.org/abs/1810.00826">Xu et al., ICLR, 2019</a>) are built without spectral convolution and show great results in some tasks. However, knowing how spectral convolution works is still helpful to understand and avoid potential problems with other¬†methods.</p> <h3><strong>1. Spectral graph convolution</strong></h3> <p><a href="https://arxiv.org/abs/1312.6203">Bruna et al., 2014, ICLR¬†2014</a></p> <p>I explain spectral graph convolution in detail in my <a href="https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801">another¬†post</a>.</p> <p>I‚Äôll briefly summarize it here for the purpose of this part of the tutorial. A formal definition of spectral graph convolution, which is very similar to the <a href="https://en.wikipedia.org/wiki/Convolution_theorem">convolution theorem</a> in signal/image processing, can be written¬†as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wBIfFw54z8usWq_merON8A.png"/><figcaption>Spectral graph convolution, where ‚äô means element-wise multiplication.</figcaption></figure> <p>where <em>V</em> are eigenvectors and <em>Œõ</em> are eigenvalues of the <a href="https://en.wikipedia.org/wiki/Laplacian_matrix"><strong>graph Laplacian</strong></a><strong> </strong><em>L</em>, which can be found by eigen-decomposition:<em> L</em>=<em>VŒõV·µÄ; W</em>_spectral are filters. Throughout this tutorial I‚Äôm going to assume ‚Äú<em>symmetric normalized Laplacian</em>‚Äù. It is computed based <em>only </em>on an adjacency matrix <em>A</em> of a graph, which can be done in a few lines of Python code as¬†follows:</p> <pre><strong># Computing the graph Laplacian<br /># A is an adjacency matrix<br /></strong>import numpy as np</pre> <pre>N = A.shape[0] <strong># number of nodes in a graph</strong><br />D = np.sum(A, 0) <strong># node degrees</strong><br />D_hat = np.diag((D + 1e-5)**(-0.5)) <strong># normalized node degrees</strong><br />L = np.identity(N) ‚Äî np.dot(D_hat, A).dot(D_hat) <strong># Laplacian</strong></pre> <p>Here, we assume that <em>A</em> is symmetric, i.e. <em>A</em> = <em>A</em>·µÄ and our graph is undirected, otherwise node degrees are not well-defined and some assumptions must be made to compute the Laplacian. In the context of computer vision and machine learning, the graph Laplacian defines how node features will be updated if we stack several graph neural layers in the form of formula¬†(2).</p> <p>So, given graph Laplacian <em>L</em>, node features <em>X</em> and filters <em>W</em>_spectral, in Python <strong>spectral convolution on graphs</strong> looks very¬†simple:</p> <pre><strong># Spectral convolution on graphs<br /># X is an <em>N√ó1 matrix of 1-dimensional node features<br /></em></strong><strong># L is an </strong><strong><em>N√óN</em> graph Laplacian computed above<br /># W_spectral are </strong><strong><em>N√ó</em></strong><strong><em>F weights (filters) that we want to train<br /></em></strong>from scipy.sparse.linalg import eigsh <strong># assumes </strong><strong><em>L</em></strong><strong> to be symmetric</strong></pre> <pre><em>Œõ</em><em>,V</em> = eigsh(L,k=20,which=‚ÄôSM‚Äô) <strong># eigen-decomposition (i.e. find <em>Œõ</em></strong><strong><em>,V)</em></strong><br />X_hat = V.T.dot(X) <strong># </strong><strong><em>20</em>√ó</strong><strong><em>1</em></strong><strong> node features in the &quot;spectral&quot; domain</strong><br />W_hat = V.T.dot(W_spectral)  <strong># 20√ó<em>F</em> filters in the </strong><strong>&quot;spectral&quot; domain</strong><br />Y = V.dot(X_hat * W_hat)  <strong># </strong><strong><em>N√ó</em></strong><strong><em>F</em></strong><strong> result of convolution</strong></pre> <p>where we assume that our node features <em>X‚ÅΩÀ°‚Åæ </em>are 1-dimensional, e.g. MNIST pixels, but it can be extended to a <em>C</em>-dimensional case: we will just need to repeat this convolution for each <em>channel</em> and then sum over <em>C</em> as in signal/image convolution.</p> <p>Formula (3) is essentially the same as <a href="https://en.wikipedia.org/wiki/Convolution_theorem">spectral convolution of signals on regular grids</a> using the Fourier Transform, and so creates a few problems for machine learning:</p> <ol><li>the dimensionality of trainable weights (filters) <em>W_</em>spectral depends on the number of nodes <em>N</em> in a¬†graph;</li><li><em>W_</em>spectral also depends on the graph structure encoded in eigenvectors <em>V.</em></li></ol> <p>These issues prevent scaling to datasets with large graphs of variable structure.</p> <p>To solve the first issue, <a href="https://arxiv.org/abs/1312.6203">Bruna et al.</a> proposed to <em>smooth</em> filters in the spectral domain, which makes them <em>more local</em> in the spatial domain according to the spectral theory. The idea is that you can represent our filter <em>W_</em>spectral from formula (3) as a sum of ùêæ predefined functions, such as splines, and instead of learning <em>N</em> values of <em>W</em>, we learn <em>K </em>coefficients <em>Œ±</em> of this¬†sum:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/832/1*sZoZfh6faYLBm7_Nq3xrQw.png"/><figcaption>We can approximate our N dimensional filter<strong> </strong><em>W_</em>spectral as a finite sum of<em> K</em> functions f, such as splines shown below. So, instead of learning N values of <em>W_</em>spectral, we can learn K coefficients (alpha) of those functions; it becomes efficient when K &lt;&lt;¬†N.</figcaption></figure> <p>While the dimensionality of <em>fk</em> does depend on the number of nodes <em>N</em>, these functions are fixed, so we don‚Äôt learn them. The only thing we learn are coefficients <em>Œ±</em>, and so <em>W_</em>spectral is no longer dependent on <em>N</em>. To make our approximation in formula (4) reasonable, we want <em>K</em>&lt;&lt;<em>N</em> to reduce the number of trainable parameters from <em>N</em> to <em>K</em> and, more importantly, make it independent of <em>N</em>, so that our GNN can digest graphs of any¬†size.</p> <p>While solves the first issue, this smoothing method does not address the second¬†issue.</p> <h3><strong>2. Chebyshev</strong> graph <strong>convolution</strong></h3> <p><a href="https://arxiv.org/abs/1606.09375">Defferrard et al., NeurIPS,¬†2016</a></p> <p>The main drawback of spectral convolution and its smooth version above is that it still requires eigen-decomposition of an <em>N</em>√ó<em>N</em> dimensional graph Laplacian <em>L</em>, which creates two main problems:</p> <ol><li>üôÅ The complexity of eigen-decomposition is huge, O(<em>N¬≥</em>). Moreover in case of large graphs, keeping the graph Laplacian in a dense format in RAM is infeasible. One solution is to use sparse matrices and find eigenvectors using scipy.sparse.linalg.eigs in Python. Additionally, you may preprocess all training graphs on a dedicated server with a lot of RAM and CPU cores. In many applications, your test graphs can also be preprocessed in advance, but if you have a constant influx of new large graphs, eigen-decomposition will make you¬†sad.</li><li>üôÅ Another problem is that the model you train ends up being closely related to the eigenvectors <em>V</em> of the graph. This can be a big problem if your training and test graphs have very different structures (numbers of nodes and edges). Otherwise, if all graphs are very similar, it is less of a problem. Moreover, if you use some smoothing of filters in the frequency domain like splines discussed above, then your filters become more localized and the problem of adapting to new graphs seems to be even less noticeable. However, the models will still be quite¬†limited.</li></ol> <p>Now, what does Chebyshev graph convolution have to do with all¬†that?</p> <p>It turns out that it solves <strong>both problems at the same time!</strong>¬†üòÉ</p> <p>That is, it avoids computing costly eigen-decomposition and the filters are no longer ‚Äúattached‚Äù to eigenvectors (yet they still are functions of eigenvalues <em>Œõ)</em>. Moreover, it has a very useful parameter, usually denoted as <em>K</em> having a similar intuition as <em>K</em> in our formula (4) above, determining the locality of filters. Informally: for <em>K</em>=1, we feed just node features <em>X‚ÅΩÀ°‚Åæ</em> to our GNN; for <em>K</em>=2, we feed <em>X‚ÅΩÀ°‚Åæ</em><strong> </strong>and ùìê<em>X‚ÅΩÀ°‚Åæ</em>; for K=3, we feed <em>X‚ÅΩÀ°‚Åæ</em><strong>,</strong> ùìê<em>X‚ÅΩÀ°‚Åæ</em><strong> </strong>and ùìê¬≤<em>X‚ÅΩÀ°‚Åæ</em>; and so forth for larger <em>K</em> (I hope you‚Äôve noticed the pattern). See more accurate and formal definition in <a href="https://arxiv.org/abs/1606.09375">Defferrard et al.</a> and my code below, plus additional analysis is given in (<a href="https://arxiv.org/abs/1811.09595">Knyazev et al., NeurIPS-W, 2018</a>).</p> <p>Due to <a href="https://en.wikipedia.org/wiki/Adjacency_matrix#Matrix_powers">the power property</a> of adjacency matrices, when we perform ùìê¬≤<em>X‚ÅΩÀ°‚Åæ</em> we actually average (or sum depending on how ùìê is normalized) over 2-hop neighbors, and analogously for any <em>n </em>in ùìê<em>‚ÅøX‚ÅΩÀ°‚Åæ</em> as illustrated below, where we average over <em>n</em>-hop neighbors.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ybJ4HOmtSwhmCB1f2JX07A.png"/><figcaption>Chebyshev convolution for <em>K</em>=3 for node 1 (dark blue). Circled nodes denote the nodes affecting feature representation of node 1. The [,] operator denotes concatenation over the feature dimension. W<em>‚ÅΩÀ°‚Åæ are 3C</em>√óF dimensional weights.</figcaption></figure> <p>Note that to satisfy the orthogonality of the Chebyshev basis, ùìê<strong> </strong>assumes no loops in the graph, so that in each <em>i</em>-th row of matrix product ùìê<em>X‚ÅΩÀ°‚Åæ</em> we will have features of the neighbors of node <em>i</em>, but <strong>not</strong> the features of node <em>i</em> itself. Features of node <em>i</em> will be fed separately as a matrix¬†<em>X‚ÅΩÀ°‚Åæ.</em></p> <p>If <em>K</em> equals the number of nodes <em>N</em>, the Chebyshev convolution closely approximates a spectral convolution, so that the receptive field of filters will be the entire graph. But, as in the case of convolutional networks, we don‚Äôt want our filters to be as big as the input images for a number of reasons that I already discussed, so in practice, <em>K</em> takes reasonably small¬†values.</p> <blockquote>In my experience, this is one of the most powerful GNNs, achieving great results in a very wide range of graph tasks. The main downside is the necessity to loop over <em>K</em> in the forward/backward pass (since Chebyshev polynomials are recursive, so it‚Äôs not possible to parallelize them), which slows down the¬†model.</blockquote> <p>Same as with Splines discussed above, instead of training filters, we train coefficients, but this time, of the Chebyshev polynomial.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/864/1*SGSYcSA5WqGYPYDwKTQT1g.png"/><figcaption>Chebyshev basis used to approximate convolution in the spectral¬†domain.</figcaption></figure> <p>To generate the Chebyshev basis, you can use the following Python¬†code:</p> <pre><strong># Set K to some integer &gt; 0, like 4 or 5 in our plots above<br /># Set n_points to a number of points on a curve (we set to 100)<br /></strong>import numpy as np</pre> <pre>x = np.linspace(-1, 1, n_points)<br />T = np.zeros((K, len(x)))<br />T[0,:] = 1<br />T[1,:] = x<br />for n in range(1, K-1):<br />    T[n+1, :] = 2*x*T[n, :] - T[n-1, :] <strong># recursive computation</strong>   <br />return T</pre> <p>The full code to generate spline and Chebyshev bases is in <a href="https://github.com/bknyaz/examples/blob/master/splines_cheb.py">my github¬†repo</a>.</p> <p>To illustrate how a Chebyshev filter can look on a irregular grid, I follow the experiment from <a href="https://arxiv.org/abs/1312.6203">Bruna et al.</a> again and sample 400 random points from the MNIST grid in the same way as I did to show eigenvectors of the graph Laplacian. I trained a Chebyshev graph convolution model on the MNIST images sampled from these 400 locations (same irregular grid is used for all images) and one of the filter for <em>K</em>=1 and <em>K</em>=20 is visualized below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/396/1*Hd0dkgJNOfOs5KAo3oIiwQ.gif"/><figcaption>A single Chebyshev filter (K=3 on the left and K=20 on the right) trained on MNIST and applied at different locations (shown as a red pixel) on a irregular grid with 400 points. Compared to filters of standard ConvNets, GNN filters have different shapes depending <em>on the node at which they are applied</em>, because each node has a different neighborhood structure.</figcaption></figure> <h3><strong>3. GCN</strong></h3> <p><a href="https://arxiv.org/abs/1609.02907">Kipf &amp; Welling, ICLR,¬†2017</a></p> <p>As you may have noticed, if you increase <em>K</em> of the Chebyshev convolution, it increases the total number of trainable parameters. For example, for <em>K</em>=2, our weights <em>W‚ÅΩÀ°‚Åæ</em> will be 2<em>C</em>√ó<em>F</em> instead of just <em>C</em>√ó<em>F</em>. This is because we concatenate features <em>X‚ÅΩÀ°‚Åæ</em><strong> </strong>and ùìê<em>X‚ÅΩÀ°‚Åæ</em> into a single <em>N</em>√ó2<em>C</em> matrix. More training parameters means the model<em> </em>is<em> </em>more difficult to train and more data must be labeled<em> </em>for training. Graph datasets are often extremely small. Whereas in computer vision, MNIST is considered a tiny dataset, because images are just 28√ó28 dimensional and there are only 60k training images, in terms of graph networks MNIST is quite large, because each graph would have <em>N</em>=784 nodes and 60k is a large number of training graphs. In contrast to computer vision tasks, many graph datasets have only around 20‚Äì100 nodes and 200‚Äì1000 training examples. These graphs can represent certain small molecules and labeling chemical/biological data is usually more expensive than labeling images. Therefore, training Chebyshev convolution models can lead to severe overfitting of the training set (i.e. the model will have the training loss close to 0 yet will have a large validation or test error). So, GCN of <a href="https://arxiv.org/abs/1609.02907">Kipf &amp; Welling</a> essentially ‚Äúmerged‚Äù matrices of node features <em>X‚ÅΩÀ°‚Åæ</em><strong> </strong>and ùìê<em>X‚ÅΩÀ°‚Åæ</em> into a single <em>N</em>√ó<em>C</em> matrix. As a result, the model has two times fewer parameters to train compared to Chebyshev convolution with <em>K</em>=2, yet has the same receptive field of 1 hop. The main trick involves adding ‚Äúself-loops‚Äù to your graph by adding an <a href="https://en.wikipedia.org/wiki/Identity_matrix">identity matrix</a> <em>I</em> to ùìê<em> </em>and normalizing it in a particular way, so now in each <em>i</em>-th row of matrix product ùìê<em>X‚ÅΩÀ°‚Åæ</em> we will have features of the neighbors of node <em>i, </em><strong>as well as</strong> features of node¬†<em>i.</em></p> <blockquote>This model seems to be a standard baseline choice well-suited for many application due to its lightweight, good performance and scalability to larger¬†graphs.</blockquote> <h4>3.1. GCN vs Chebyshev layer</h4> <p>The difference between GCN and Chebyshev convolution is illustrated below.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2a9263af373ed75b1d578f00612e0ef8/href">https://medium.com/media/2a9263af373ed75b1d578f00612e0ef8/href</a></iframe> <p>The code above follows the same structure as in <a href="https://medium.com/p/3d9fada3b80d"><em>the first part of my tutorial</em></a>, where I compared classical NN and GNN. One of the main steps both in GCN and Chebyshev convolution is computation of the rescaled graph Laplacian <em>L</em>. This rescaling is done to make eigenvalues in the range [-1,1] to facilitate training (this might be not a very important step in practice as weights can adapt during training). In GCN, self-loops are added to the graph by adding an identity matrix before computing the Laplacian as discussed above. The main difference between the two methods is that in the Chebyshev convolution we <em>recursively</em> loop over <em>K</em> to capture features in the <em>K</em>-hop neighborhood. We can stack such GCN or Chebyshev layers interleaved with nonlinearities to build a Graph Neural¬†Network.</p> <p>Now, let me politely interrupt üòÉ our spectral discussion and give a general idea behind two other exciting methods: Edge-conditioned filters by <a href="https://arxiv.org/abs/1704.02901">Simonovsky &amp; Komodakis, CVPR, 2017</a> and MoNet by <a href="https://arxiv.org/abs/1611.08402">Monti et al., CVPR, 2017</a>, which share some similar concepts.</p> <h3><strong>4. Edge-conditioned</strong> filters</h3> <p><a href="https://arxiv.org/abs/1704.02901">Simonovsky &amp; Komodakis, CVPR,¬†2017</a></p> <p>As you know, in ConvNets we learn the weights (filters) by optimizing some loss like <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss">Cross Entropy</a>. In the same way, we learn our <em>W‚ÅΩÀ°‚Åæ </em>in GNNs. Imagine that instead of learning these weights, you have <em>another network</em> that predicts them. So during training, we learn the weights of that auxiliary network, which takes an image or a graph as an input and returns weights <em>W‚ÅΩÀ°‚Åæ </em>(Œò in their work) as the output. The idea is based on <strong>Dynamic Filter Networks</strong> (<a href="https://arxiv.org/abs/1605.09673">Brabandere et al., NIPS, 2016</a>), where ‚Äúdynamic‚Äù means that filters <em>W‚ÅΩÀ°‚Åæ </em>will be different depending on the input as opposed to standard models in which filters are fixed (or static) after training.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/779/1*0v1xygb2cN-3do55tAh1eg.png"/><figcaption>Using an auxiliary ‚Äúfilter generating network‚Äù FÀ° to predict edge-specific weights Œò for the main network. XÀ°‚Åª¬π are input node features and XÀ° are output features. The figure shows a single iteration of ‚Äúdynamic convolution‚Äù for node 1 (in yellow). Standard GNNs typically would simply average (or sum) features of node 1 neighbors (nodes 2, 3, 4, 5)¬†, which would correspond to having an isotropic filter (Œò would be a constant vector). In contrast, this model has anisotropic filters, because it predicts different edge values between node 1 and all it‚Äôs neighbors based on edge labels L, so that features XÀ°(1) are computed as a weighted average of neighbors‚Äô features. Figure from (<a href="https://arxiv.org/abs/1704.02901">Simonovsky &amp; Komodakis, CVPR,¬†2017</a>).</figcaption></figure> <p>This is a very general form of convolution that, besides images, can be easily applied to graphs or point clouds as they did in their CVPR paper and got excellent results. However, there is no ‚Äú<a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem">free lunch</a>‚Äù, and training such models is quite challenging, because the regular grid constraint is now relaxed and the scope of solutions increases dramatically. This is especially true for larger graphs with many edges or for convolution in deeper layers, which often have hundreds of channels (number of features, <em>C)</em>, so you might end up generating thousands of numbers in total for each input! In this regard, standard ConvNets are so good, because we don‚Äôt waste the model‚Äôs capacity on training to predict these weights, instead we directly enforce that the filters should be the same for all inputs. But this prior makes ConvNets limited and we cannot directly apply them to graphs or point clouds. So, as always, there‚Äôs some trade-off between flexibility and performance in a particular task.</p> <blockquote>When applied to images, like MNIST, the Edge-conditioned model can learn to predict <em>anisotropic</em> filters‚Ää‚Äî‚Ääfilters that are sensitive to orientation, such as edge detectors. Compared to Gaussian filters discussed in <a href="https://medium.com/p/3d9fada3b80d"><em>the first part of my tutorial</em></a>, these filters are able to better capture certain patterns in images, such as strokes in¬†digits.</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/519/1*aApSWc8LXLpEwvO312yuUg.png"/><figcaption>Convolutional filters learned on MNIST sampled in low (left) and high (right) resolutions. Figure from (<a href="https://arxiv.org/abs/1704.02901">Simonovsky &amp; Komodakis, CVPR,¬†2017</a>).</figcaption></figure> <p>I want to highlight one more time that whenever we have a complicated model with auxiliary networks, it becomes a chicken-or-the-egg problem in some sense. To solve it, one of the networks‚Ää‚Äî‚Ääthe auxiliary or the main one‚Ää‚Äî‚Ääshould receive a very strong signal, so that it can implicitly supervise another network. In our <a href="https://arxiv.org/abs/1907.09000">BMVC paper</a>, which is similar to <a href="https://arxiv.org/abs/1704.02901">Simonovsky &amp; Komodakis</a>‚Äôs work, we apply additional constraints on the edge-generating network to facilitate training. I will describe our work in detail in later¬†posts.</p> <h3><strong>5. MoNet</strong></h3> <p><a href="https://arxiv.org/abs/1611.08402">Monti et al., CVPR,¬†2017</a></p> <p>MoNet is different from other works discussed in this post, as it assumes to have the notion of node coordinates, and therefore is more suited for geometric tasks such as 3D mesh analysis or image/video reasoning. It is somewhat similar to edge-conditioned filters of <a href="https://arxiv.org/abs/1704.02901">Simonovsky &amp; Komodakis</a>, since they also introduce an auxiliary learnable function ùê∑(ùë§, ùúÉ<em>, œÅ</em>) that predicts weights. The difference is that these weights depend on the node polar coordinates (angle ùúÉ and radius <em>œÅ</em>); and trainable parameters ùë§ of that function are constrained to be means and variances of Gaussians, so that instead of learning <em>N</em>√ó<em>N</em> matrices, we only learn fixed-size vectors (means and variances) independently of the graph size <em>N</em>. In terms of standard ConvNets, it would be the same as learning only 2 values (the mean and variance of a Gaussian) for each filter instead of learning 9, 25 or 121 values for 3√ó3, 5√ó5 or 11√ó11 dimensional filters respectively. This <em>parameterization</em> would greatly reduce the number of parameters in a ConvNet, but the filters would be very limited in their power to capture image features.</p> <p><a href="https://arxiv.org/abs/1611.08402">Monti et al.</a> train ùêΩ means and variances of Gaussians and the process of transforming node coordinates is similar to fitting them into a <a href="https://scikit-learn.org/stable/modules/mixture.html">Gaussian Mixture Model</a>. The model is quite computationally intensive to train if we want our filters to be global enough, but it can be a good choice for visual tasks (see our <a href="https://arxiv.org/abs/1907.09000">BMVC paper</a> for comparison), yet it is often worse than simple GCN on non-visual tasks (<a href="https://arxiv.org/abs/1811.09595">Knyazev et al., NeurIPS-W, 2018</a>). Since function <em>D</em> depends on coordinates, the generated filters are also anisotropic and have a shape of oriented and elongated Gaussians as illustrated below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/288/1*TM_3zmnc4esqwqIdfgTvvA.png"/><figcaption>Filters trained with MoNet in polar coordinates ùúÉ and <em>œÅ</em>. Each ellipse corresponds to a slice of a Gaussian at some fixed level. The idea is that if the coordinates of the i-th node are close to the middle of the j-th Gaussian, then the generated weight at index (i,j) will have a value close to¬†1.</figcaption></figure> <pre><strong><em>Pseudo-code of the MoNet layer using PyTorch</em></strong></pre> <pre><strong># assume X to be input <em>N</em></strong>√ó<strong><em>C</em> node features</strong><br /><strong># coord are <em>N</em>√ó<em>N</em>√ó<em>2</em> node coordinate differences between all pairs of nodes (node degrees for non-geometric tasks)<br /># coord can be viewed as angular and radial edges between nodes</strong></pre> <pre>1. Generate <em>J</em> Gaussian-shaped filters based on coordinates of nodes    using some trainable function D<br />   weights = D(coord)  # weights: <em>J</em>√ó<em>N</em>√ó<em>N</em><br />2. Multiply node features X by these weights<br />   X = torch.bmm(weights, X.expand(J, N, C))  # X: <em>J</em>√ó<em>N</em>√ó<em>C</em><br />3. Project features by a learnable linear transformation<br />   X = fc(X.permute(1, 2, 0).view(N, J*C))  # X: <em>N</em>√ó<em>F<br /></em>4. Feed X to the next layer</pre> <h3>Conclusion</h3> <p>Despite a lengthy discussion, we have only scratched the surface. Applications of graph neural networks are expanding far beyond typical graph reasoning tasks, like molecule classification. The number of different graph neural layers is increasing very quickly, similar to how it was for convolutional networks a few years ago, so it‚Äôs hard to keep track of them. On that note, <a href="https://github.com/rusty1s/pytorch_geometric">PyTorch Geometric (PyG)</a>‚Ää‚Äî‚Ääa nice toolbox to learn from graphs‚Ää‚Äî‚Ääfrequently populates its collection with novel layers and¬†tricks.</p> <p><em>Acknowledgement: A large portion of this tutorial was prepared during my internship at SRI International under the supervision of </em><a href="https://medium.com/u/6cf41cb2c546"><em>Mohamed Amer</em></a><em> (</em><a href="https://mohamedramer.com/"><em>homepage</em></a><em>) and my PhD advisor Graham Taylor (</em><a href="https://www.gwtaylor.ca/"><em>homepage</em></a><em>). I also thank </em><a href="https://www.linkedin.com/in/carolynaugusta/"><em>Carolyn Augusta</em></a><em> for useful feedback.</em></p> <p>Find me on <a href="https://github.com/bknyaz/">Github</a>, <a href="https://www.linkedin.com/in/boris-knyazev-39690948/">LinkedIn</a> and <a href="https://twitter.com/BorisAKnyazev">Twitter</a>. <a href="https://bknyaz.github.io/">My homepage</a>.</p> <p>If you want to cite this blog post in your paper, please use:<br/><a href="http://twitter.com/misc"><em>@misc</em></a><em>{knyazev2019tutorial,<br/> title={Tutorial on Graph Neural Networks for Computer Vision and Beyond},<br/> author={Knyazev, Boris and Taylor, Graham W and Amer, Mohamed R},<br/> year={2019}<br/>}</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=be6d71d70f49" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49">Anisotropic, Dynamic, Spectral and Multiscale Filters Defined on Graphs</a> was originally published in <a href="https://medium.com/data-science">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Tutorial on Graph Neural Networks for Computer Vision and Beyond (Part 1)</title><link href="https://bknyaz.github.io//2019/08/04/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-1.html" rel="alternate" type="text/html" title="Tutorial on Graph Neural Networks for Computer Vision and Beyond (Part 1)"/><published>2019-08-04T00:06:21+00:00</published><updated>2019-08-04T00:06:21+00:00</updated><id>https://bknyaz.github.io//2019/08/04/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-1</id><content type="html" xml:base="https://bknyaz.github.io//2019/08/04/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-1.html"><![CDATA[<h3>Tutorial on Graph Neural Networks for Computer Vision and¬†Beyond</h3> <p><em>I‚Äôm answering questions that AI/ML/CV people not familiar with graphs or graph neural networks typically ask. I provide PyTorch examples to clarify the idea behind this relatively new and exciting kind of¬†model.</em></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6AB8X6dumCaBBYQZ6vfRuw.png"/><figcaption>A figure from (<a href="https://arxiv.org/abs/1312.6203">Bruna et al., ICLR, 2014</a>) depicting an MNIST image on the 3D sphere. While it‚Äôs hard to adapt <a href="https://arxiv.org/abs/1801.10130">Convolutional Networks to classify spherical data</a>, Graph Networks can naturally handle it. This is a toy example, but similar tasks arise in many real applications.</figcaption></figure> <p>The questions addressed in this part of my tutorial¬†are:</p> <ol><li><strong>Why are graphs¬†useful?</strong></li><li><strong>Why is it difficult to define convolution on¬†graphs?</strong></li><li><strong>What makes a neural network a graph neural¬†network?</strong></li></ol> <p>To answer them, I‚Äôll provide motivating examples, papers and Python code making it a tutorial on Graph Neural Networks (GNNs). Some basic knowledge of machine learning and computer vision is expected, however, I‚Äôll provide some background and intuitive explanation as we¬†go.</p> <p>First of all, let‚Äôs briefly recall what is a <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graph</a>? A graph <em>G</em> is a set of nodes (vertices) connected by directed/undirected edges. Nodes and edges typically come from some expert knowledge or intuition about the problem. So, it can be atoms in molecules, users in a social network, cities in a transportation system, players in team sport, neurons in the brain, interacting objects in a dynamic physical system, pixels, bounding boxes or segmentation masks in images. In other words, in many practical cases, it is actually <strong>you</strong> who gets to decide what are the nodes and edges in a¬†graph.</p> <blockquote>In many practical cases, it is actually you who gets to decide what are the nodes and edges in a¬†graph.</blockquote> <p>This is a very flexible data structure that generalizes many other data structures. For example, if there are no edges, then it becomes a <a href="https://en.wikipedia.org/wiki/Set_(abstract_data_type)">set</a>; if there are only ‚Äúvertical‚Äù edges and any two nodes are connected by exactly one path, then we have a <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)">tree</a>. Such flexibility is both good and bad as I‚Äôll discuss in this tutorial.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/754/1*68Gcr70UTpdaX7THbZSEcA.png"/><figcaption>Two undirected graphs with 5 and 6 nodes. The order of nodes is arbitrary.</figcaption></figure> <h3>1. Why graphs can be¬†useful?</h3> <p>In the context of computer vision (CV) and machine learning (ML), studying graphs and the models to learn from them can give us at least four benefits:</p> <ol><li>We can become closer to solving important problems that previously were too challenging, such as: drug discovery for cancer (<a href="https://www.nature.com/articles/s41598-019-45349-y">Veselkov et al., Nature, 2019</a>); better understanding of the human brain connectome (<a href="https://www.nature.com/articles/s41467-018-06346-3">Diez &amp; Sepulcre, Nature Communications, 2019</a>); materials discovery for energy and environmental challenges (<a href="https://www.nature.com/articles/s41467-019-10663-6">Xie et al., Nature Communications, 2019</a>).</li><li>In most CV/ML applications, data can be actually viewed as graphs even though you used to represent them as another data structure. Representing your data as graph(s) gives you a lot of flexibility and can give you a very different and interesting perspective on your problem. For instance, instead of learning from image pixels you can learn from ‚Äú<a href="https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic">superpixels</a>‚Äù as in (<a href="https://arxiv.org/abs/1603.07063">Liang et al., ECCV, 2016</a>) and in our forthcoming <a href="https://arxiv.org/abs/1907.09000">BMVC paper</a>. Graphs also let you impose a relational inductive bias in data‚Ää‚Äî‚Ääsome prior knowledge you have about the problem. For instance, if you want to reason about a human pose, your relational bias can be a graph of skeleton joints of a human body (<a href="https://arxiv.org/abs/1801.07455">Yan et al., AAAI, 2018</a>); or if you want to reason about videos, your relational bias can be a graph of moving bounding boxes (<a href="https://arxiv.org/abs/1806.01810">Wang &amp; Gupta, ECCV, 2018</a>). Another example can be representing facial landmarks as a graph (<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Antonakos_Active_Pictorial_Structures_2015_CVPR_paper.html">Antonakos et al., CVPR, 2015</a>) to make reasoning about <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">facial attributes</a> and identity.</li><li><a href="https://arxiv.org/abs/2403.12143">Your favourite neural network itself can be viewed as a graph, where nodes are neurons and edges are weights</a>, or <a href="https://arxiv.org/abs/2303.04143">where nodes are layers and edges denote flow of forward/backward pass</a> (in which case we are talking about a computational graph used in TensorFlow, PyTorch and other DL frameworks). An application can be optimization of a computational graph, <a href="https://arxiv.org/abs/2110.13100">neural architecture search</a>, <a href="https://ai.googleblog.com/2018/06/how-can-neural-network-similarity-help.html">analyzing training behavior</a>, <a href="https://arxiv.org/abs/2409.04434">accelerating training</a>, etc.</li><li>Finally, you can solve many problems, where data can be more naturally represented as graphs, <em>more effectively</em>. This includes, but is not limited to, molecule and social network classification (<a href="https://arxiv.org/abs/1811.09595">Knyazev et al., NeurIPS-W, 2018</a>) and generation (<a href="https://arxiv.org/abs/1802.03480">Simonovsky &amp; Komodakis, ICANN, 2018</a>), 3D Mesh classification and correspondence (<a href="https://arxiv.org/abs/1711.08920">Fey et al., CVPR, 2018</a>) and generation (<a href="https://arxiv.org/abs/1804.01654">Wang et al., ECCV, 2018</a>), modeling behavior of dynamic interacting objects (<a href="https://arxiv.org/abs/1802.04687">Kipf et al., ICML, 2018</a>), visual scene graph modeling (see the upcoming <a href="https://cs.stanford.edu/people/ranjaykrishna/sgrl/index.html">ICCV Workshop</a>) and question answering (<a href="https://arxiv.org/abs/1811.00538">Narasimhan, NeurIPS, 2018</a>), program synthesis (<a href="https://arxiv.org/abs/1711.00740">Allamanis et al., ICLR, 2018</a>), different reinforcement learning tasks (<a href="https://arxiv.org/abs/1904.03177">Bapst et al., ICML, 2019</a>) and many other exciting problems.</li></ol> <p>As my previous research was related to recognizing and analyzing faces and emotions, I particularly like this figure¬†below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/607/1*yBuAXZIqq_7VknWEZdyUNQ.png"/><figcaption>A figure from (<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Antonakos_Active_Pictorial_Structures_2015_CVPR_paper.html">Antonakos et al., CVPR, 2015</a>) showing representation of a face as a graph of landmarks. This is an interesting approach, but it is not a sufficient facial representation in many cases, since a lot can be told from the face texture captured well by convolutional networks. In contrast, reasoning over 3D meshes of a face looks like a more sensible approach compared to 2D landmarks (<a href="https://arxiv.org/abs/1807.10267">Ranjan et al., ECCV,¬†2018</a>).</figcaption></figure> <h3>2. Why is it difficult to define convolution on¬†graphs?</h3> <p>To answer this question, I first give some motivation for using convolution in general and then describe ‚Äúconvolution on images‚Äù using the graph terminology which should make the transition to ‚Äúconvolution on graphs‚Äù more¬†smooth.</p> <h4>2.1. Why is convolution useful?</h4> <p>Let‚Äôs understand why we care about convolution so much and why we want to use it for graphs. Compared to fully-connected neural networks (a.k.a. NNs or MLPs), convolutional networks (a.k.a. CNNs or ConvNets) have certain advantages explained below based on the image of a nice old¬†Chevy.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*Cf5Wwx1-Z7gQHUq4EOgQSg.jpeg"/><figcaption>‚ÄúChevrolet Vega‚Äù according to Google Image¬†Search.</figcaption></figure> <p><strong>First</strong>, ConvNets exploit a natural prior in images, more formally described in (<a href="https://arxiv.org/abs/1611.08097">Bronstein et al., 2016</a>), such¬†as:</p> <ol><li>Shift-invariance‚Ää‚Äî‚Ääif we translate the car on the image above to the left/right/up/down, we still should be able to detect and recognize it as a car. This is exploited by sharing filters across all locations, i.e. applying convolution.</li><li>Locality‚Ää‚Äî‚Äänearby pixels are closely related and often represent some semantic concept, such as a wheel or a window. This is exploited by using relatively large filters, which can capture image features in a local spatial neighborhood.</li><li>Compositionality (or hierarchy)‚Äî a larger region in the image is often a semantic parent of smaller regions it contains. For example, a car is a parent of doors, windows, wheels, driver, etc. And a driver is a parent of head, arms, etc. This is implicitly exploited by stacking convolutional layers and applying¬†pooling.</li></ol> <p><strong>Second</strong>, the number of trainable parameters (i.e. filters) in convolutional layers does not depend on the input dimensionality, so technically we can train exactly the same model on 28√ó28 and 512√ó512 images. In other words, the model is <em>parametric</em>.</p> <blockquote>Ideally, our goal is to develop a model that is as flexible as Graph Neural Nets and can digest and learn from any data, but at the same time we want to control (regularize) factors of this flexibility by turning on/off certain¬†priors.</blockquote> <p>All these nice properties make ConvNets less prone to overfitting (high accuracy on the training set and low accuracy on the validation/test set), more accurate in different visual tasks, and easily scalable to large images and datasets. So, when we want to solve important tasks where input data are graph-structured, it is appealing to transfer all these properties to <strong>graph neural networks (GNNs) </strong>to regularize their flexibility and make them scalable. Ideally, our goal is to develop a model that is as flexible as GNNs and can digest and learn from any data, but at the same time we want to control (regularize) factors of this flexibility by turning on/off certain priors. This can open research in many interesting directions. However, controlling of this trade-off is challenging.</p> <h4>2.2. Convolution on images in terms of¬†graphs</h4> <p>Let‚Äôs consider an undirected graph <em>G</em> with <em>N</em> nodes. Edges <em>E</em> represent undirected connections between nodes. Nodes and edges typically come from your intuition about the problem. Our intuition in the case of images is that nodes are pixels or <a href="https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic">superpixels</a> (a group of pixels of weird shape) and edges are spatial distances between them. For example, the <a href="https://arxiv.org/abs/1905.10498">MNIST</a> image below on the left is typically represented as an 28√ó28 dimensional matrix. We can also represent it as a set of <em>N</em>=28*28=784 pixels. So, our graph <em>G</em> is going to have <em>N</em>=784 nodes and edges will have large values (thicker edges in the Figure below) for closely located pixels and small values (thinner edges) for remote¬†pixels.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Kji3yJN0cT6RwO11h0Mh6A.png"/><figcaption>An image from the MNIST dataset on the left and an example of its graph representation on the right. Darker and larger nodes on the right correspond to higher pixel intensities. The figure on the right is inspired by Figure 5 in (<a href="https://arxiv.org/abs/1711.08920">Fey et al., CVPR,¬†2018</a>)</figcaption></figure> <p>When we train our neural networks or ConvNets on images, we implicitly define images on a graph‚Ää‚Äî‚Ääa <em>regular</em> two-dimensional grid as the one on the figure below. Since this grid is the same for all training and test images and is <em>regular</em>, i.e. all pixels of the grid are connected to each other in exactly the same way across all images (i.e. have the same number of neighbors, length of edges, etc.), this regular grid graph has no information that will help us to tell one image from another. Below I visualize some 2D and 3D regular grids, where the order of nodes is color-coded. By the way, I‚Äôm using <a href="https://networkx.github.io/">NetworkX</a> in Python to do that, e.g. G = networkx.grid_graph([4, 4]).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/880/0*XlNlsS2iG45j-adq"/><figcaption>Examples of regular 2D and 3D grids. Images are defined on 2D grids and videos are on 3D¬†grids.</figcaption></figure> <p>Given this 4√ó4 regular grid, let‚Äôs briefly look at how 2D convolution works to understand why it‚Äôs difficult to transfer this operator to graphs. A filter on a regular grid has the same order of nodes, but modern convolutional nets typically have small filters, such as 3√ó3 in the example below. This filter has 9 values: <em>W</em>‚ÇÅ,<em>W</em>‚ÇÇ,‚Ä¶, <em>W</em>‚Çâ, which is what we are updating during training using backprop to minimize the loss and solve the downstream task. In our example below, we just heuristically initialize this filter to be an edge detector (see other possible filters¬†<a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">here</a>):</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/576/1*Y2xcReChWy3JYGSw0_Rt0Q.png"/><figcaption>Example of a 3√ó3 filter on a regular 2D grid with arbitrary weights w on the left and an edge detector on the¬†right.</figcaption></figure> <p>When we perform convolution, we slide this filter in both directions: to the right and to the bottom, but nothing prevents us from starting in the bottom corner‚Ää‚Äî‚Ääthe important thing is to slide over all possible locations. At each location, we compute the <a href="https://en.wikipedia.org/wiki/Dot_product"><em>dot product</em></a><em> </em>between the values on the grid (let‚Äôs denote them as <em>X</em>) and the values of filters, <em>W</em>: <em>X</em>‚ÇÅ<em>W</em>‚ÇÅ+<em>X</em>‚ÇÇ<em>W</em>‚ÇÇ+‚Ä¶+<em>X</em>‚Çâ<em>W</em>‚Çâ, and store the result in the output image. In our visualization, we change the color of nodes during sliding to match the colors of nodes in the grid. In a regular grid, we always can match a node of the filter with a node of the grid. Unfortunately, this is not true for graphs as I‚Äôll explain later¬†below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VxofpMT7GwLKZGVGWc2XHQ.png"/><figcaption>2 steps of 2D convolution on a regular grid. If we don‚Äôt apply padding, there will be 4 steps in total, so the result will be a 2√ó2 image. To make the resulting image larger, we need to apply <a href="https://deeplizard.com/learn/video/qSTv_m-KFk0">padding</a>. See a comprehensive guide to convolution in deep learning¬†<a href="https://arxiv.org/abs/1603.07285">here</a>.</figcaption></figure> <p>The dot product used above is one of so called ‚Äúaggregator operators‚Äù. Broadly speaking, the goal of an aggregator operator is to summarize data to a reduced form. In our example above, the dot product summarizes a 3√ó3 matrix to a <em>single</em> value. Another example is pooling in ConvNets. Keep in mind, that such methods as max or sum pooling are <em>permutation-invariant</em>, i.e. they will pool the same value from a spatial region even if you randomly shuffle all pixels inside that region. To make it clear, the dot product is <em>not</em> permutation-invariant simply because in general: <em>X</em>‚ÇÅ<em>W</em>‚ÇÅ+<em>X</em>‚ÇÇ<em>W</em>‚ÇÇ ‚â†<em>X</em>‚ÇÇ<em>W</em>‚ÇÅ+<em>X</em>‚ÇÅ<em>W</em>‚ÇÇ.</p> <p>Now let‚Äôs use our MNIST image and illustrate the meaning of a regular grid, a filter and convolution. Keeping in mind our graph terminology, this regular 28√ó28 grid will be our graph <em>G</em>, so that every cell in this grid is a node, and node features are an actual image <em>X</em>, i.e. every node will have just a single feature‚Ää‚Äî‚Ääpixel intensity from 0 (black) to 1¬†(white).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*8nWUYV-nIuwL1bGCw3TnEw.png"/><figcaption>Regular 28√ó28 grid (left) and an image on that grid¬†(right).</figcaption></figure> <p>Next, we define a filter and let it be a famous <a href="https://en.wikipedia.org/wiki/Gabor_filter">Gabor filter</a> with some (almost) arbitrary parameters. Once we have an image and a filter, we can perform <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html">convolution</a> by sliding the filter over that image (of digit 7 in our case) and putting the result of the dot product to the output matrix after each¬†step.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*9pzcalk9Lu4e-jqQHgcIUg.png"/><figcaption>A 28√ó28 filter (left) and the result of 2D convolution of this filter with the image of digit 7¬†(right).</figcaption></figure> <p>This is all cool, but as I mentioned before, it becomes tricky when you try to generalize convolution to¬†graphs.</p> <blockquote>Nodes are a set, and any permutation of this set does not change it. Therefore, the <em>aggregator</em> operator that people apply should be <em>permutation-invariant</em>.</blockquote> <p>As I have already mentioned, the dot product used above to compute convolution at each step is <em>sensitive</em> to the order. This sensitivity permits us to learn edge detectors similar to Gabor filters important to capture image features. The problem is that in graphs <em>there is no well-defined order of nodes</em> unless you learn to order them, or come up with some heuristic that will result in a consistent (canonical) order from graph to graph. In short, nodes are a set, and any permutation of this set does not change it. Therefore, the <em>aggregator</em> operator that people apply should be <em>permutation-invariant</em>. The most popular choices are averaging (GCN, <a href="https://arxiv.org/abs/1609.02907">Kipf &amp; Welling, ICLR, 2017</a>) and summation (GIN, <a href="https://arxiv.org/abs/1810.00826">Xu et al., ICLR, 2019</a>) of <strong>all</strong> neighbors, i.e. sum or mean pooling, followed by projection by a trainable vector <em>W</em>. See <a href="https://arxiv.org/abs/1706.02216">Hamilton et al., NIPS, 2017</a> for some other aggregators.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/746/1*r91KCqXWXm3ltrixv_kUcA.png"/><figcaption>Illustration of ‚Äúconvolution on graphs‚Äù of node features <em>X with </em>filter <em>W</em> centered at node 1 (dark¬†blue).</figcaption></figure> <p>For example, for the graph above on the left, the output of the summation aggregator for node 1 will be <em>X</em>‚ÇÅ=(<em>X</em>‚ÇÅ+<em>X</em>‚ÇÇ+<em>X</em>‚ÇÉ+<em>X</em>‚ÇÑ)<em>W</em>‚ÇÅ, for node 2: <em>X</em>‚ÇÇ=(<em>X</em>‚ÇÅ+<em>X</em>‚ÇÇ+<em>X</em>‚ÇÉ+<em>X</em>‚ÇÖ)<em>W</em>‚ÇÅ and so forth for nodes 3, 4 and 5, i.e. we need to apply this aggregator for all nodes. In result, we will have the graph with the same structure, but node features will now contain features of neighbors. We can process the graph on the right using the same¬†idea.</p> <p>Colloquially, people call this averaging or summation ‚Äúconvolution‚Äù, since we also ‚Äúslide‚Äù from one node to another and apply an aggregator operator in each step. However, it‚Äôs important to keep in mind that this is a very specific form of convolution, where filters don‚Äôt have a sense of orientation. Below I‚Äôll show how those filters look like and give an idea how to make them¬†better.</p> <h3>3. What makes a neural network a graph neural¬†network?</h3> <p>You know how a classical neural network works, right? We have some <em>C</em>-dimensional features <em>X</em> as the input to the net. Using our running MNIST example, <em>X</em><strong> </strong>will be our <em>C</em>=784 dimensional pixel features (i.e. a ‚Äúflattened‚Äù image). These features get multiplied by <em>C</em>√ó<em>F </em>dimensional weights <em>W</em> that we update during training to get the output closer to what we expect<em>. </em>The result can be directly used to solve the task (e.g. in case of regression) or can be further fed to some nonlinearity (activation), like ReLU, or other differentiable (or more precisely, sub-differentiable) functions to form a multi-layer network. In general, the output of some layer <em>l</em>¬†is:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/741/1*PK-poDpkwSxcifQcO7Lidw.png"/><figcaption>Fully-connected layer with learnable weights W. ‚ÄúFully-connected‚Äù means that each output value in X<em>‚ÅΩÀ°‚Å∫¬π‚Åæ</em> depends on, or ‚Äúconnected to‚Äù, all inputs X<em>‚ÅΩÀ°‚Åæ</em>. Typically, although not always, we add a bias term to the¬†output.</figcaption></figure> <p>The signal in MNIST is so strong, that you can get an accuracy of 91% by just using the formula above and the Cross Entropy loss without any nonlinearities and other tricks (I used a slightly modified <a href="https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py">PyTorch example</a> to do that). Such model is called multinomial (or multiclass, since we have 10 classes of digits) logistic regression.</p> <p>Now, how do we transform our vanilla neural network to a graph neural network? As you already know, the core idea behind GNNs is aggregation over ‚Äúneighbors‚Äù. Here, it is important to understand that in many cases, it is actually <strong>you</strong> who specifies ‚Äúneighbors‚Äù.</p> <p>Let‚Äôs consider a simple case first, when you are given some graph. For example, this can be a fragment (subgraph) of a social network with 5 persons and an edge between a pair of nodes denotes if two people are friends (or at least one of them think so). An <a href="https://en.wikipedia.org/wiki/Adjacency_matrix">adjacency matrix</a> (usually denoted as <em>A</em>) in the figure below on the right is a way to represent these edges in a matrix form, convenient for our deep learning frameworks. Yellow cells in the matrix represent the edge and blue‚Ää‚Äî‚Ääthe absence of the¬†edge.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7cmkI4y_CYsF-sj0qIJrEA.png"/><figcaption>Example of a graph and its adjacency matrix. The order of nodes we defined in both cases is random, while the graph is still the¬†same.</figcaption></figure> <p>Now, let‚Äôs create an adjacency matrix <em>A</em> for our MNIST example based on coordinates of pixels (complete code is provided in the end of the¬†post):</p> <pre><em>import numpy as np<br />from scipy.spatial.distance import cdist</em></pre> <pre>img_size = 28  <strong># MNIST image width and height</strong><br />col, row = np.meshgrid(np.arange(img_size), np.arange(img_size))<br />coord = np.stack((col, row), axis=2).reshape(-1, 2) / img_size<br />dist = cdist(coord, coord)  <strong># see figure below on the left</strong><br />sigma = 0.2 * np.pi  <strong># width of a Gaussian</strong><br />A = np.exp(- dist ** 2 / sigma ** 2)  <strong># see figure below in the middle</strong></pre> <p>This is a typical, but not the only, way to define an adjacency matrix for visual tasks (<a href="https://arxiv.org/abs/1606.09375">Defferrard et al., NIPS, 2016</a>, <a href="https://arxiv.org/abs/1611.08097">Bronstein et al., 2016</a>). This adjacency matrix is our prior, or our inductive bias, we impose on the model based on our intuition that nearby pixels should be connected and remote pixels shouldn‚Äôt or should have very thin edge (edge of a small value). This is motivated by observations that in natural images nearby pixels often correspond to the same object or objects that interact frequently (the locality principle we mentioned in Section 2.1.), so it makes a lot of sense to connect such¬†pixels.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FD55QIOgm7CRGMG565KucQ.png"/><figcaption>Adjacency matrix (<em>N</em>x<em>N) in the form of distances (left) and closeness (middle) between all pairs of nodes. (right) A subgraph with 16 neighboring pixels corresponding to the adjacency matrix in the middle. Since it‚Äôs a complete subgraph, it‚Äôs also called a ‚Äúclique‚Äù.</em></figcaption></figure> <p>So, now instead of having just features <em>X</em> we have some fancy matrix <em>A</em> with values in the range [0,1]. It‚Äôs important to note that once we know that our input is a graph, we assume that there is no canonical order of nodes that will be consistent across all other graphs in the dataset. In terms of images, it means that <em>pixels are assumed to be randomly shuffled</em>. Finding the canonical order of nodes is combinatorially unsolvable in practice. Even though for MNIST we technically can cheat by knowing this order (because data are originally from a regular grid), it‚Äôs not going to work on actual graph datasets.</p> <p>Remember that our matrix of features <em>X</em> has ùëÅ rows and C columns. So, in terms of graphs, each row corresponds to one node and <em>C</em> is the dimensionality of node features. But now the problem is that we don‚Äôt know the order of nodes, so we don‚Äôt know in which row to put features of a particular node. If we just pretend to ignore this problem and feed <em>X</em> directly to an MLP as we did before, the effect will be the same as feeding images with randomly shuffled pixels with <em>independent</em> (yet the same for each epoch) shuffling for each image! Surprisingly, a neural network can in principle still fit such random data (<a href="https://arxiv.org/abs/1611.03530">Zhang et al., ICLR, 2017</a>), however test performance will be close to random prediction. One of the solutions is to simply use the adjacency matrix <em>A,</em> we created before, in the following way:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/741/1*Ht9tBXaTrV2gkbMIe7zxMg.png"/><figcaption>Graph neural layer with adjacency matrix A, input/output features X and learnable weights¬†W.</figcaption></figure> <p>We just need to make sure that row <em>i</em> in <em>A</em> corresponds to features of node in row <em>i</em> of <em>X</em>. Here, I‚Äôm using ùìê instead of plain <em>A</em>, because often you want to normalize <em>A</em>. If ùìê=<em>A</em>, the matrix multiplication ùìê<em>X‚ÅΩÀ°‚Åæ </em>will be equivalent to summing features of neighbors, which turned out to be useful in many tasks (<a href="https://arxiv.org/abs/1810.00826">Xu et al., ICLR, 2019</a>). Most commonly, you normalize it so that ùìê<em>X‚ÅΩÀ°‚Åæ </em>averages features of neighbors, i.e. ùìê=<em>A</em><strong>/</strong>Œ£·µ¢<em>A</em>·µ¢. A better way to normalize matrix <em>A</em> can be found in (<a href="https://arxiv.org/abs/1609.02907">Kipf &amp; Welling, ICLR,¬†2017</a>).</p> <p>Below is the comparison of NN and GNN in terms of PyTorch¬†code:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/025a3cd1ab937d3f9fc09f062c0ed5a0/href">https://medium.com/media/025a3cd1ab937d3f9fc09f062c0ed5a0/href</a></iframe> <p>And <a href="https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py">HERE</a> is the full PyTorch code to train two models above: python mnist_fc.py --model fc to train the NN case; python mnist_fc.py --model graph to train the GNN case. As an exercise, try to randomly shuffle pixels in code in the --model graph case (don‚Äôt forget to shuffle <em>A</em> in the same way) and make sure that it will not affect the result. Is it going to be true for the --model fc¬†case?</p> <blockquote><a href="https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py">Here</a> is the full PyTorch code to train two¬†models.</blockquote> <p>After running the code, you may notice that the classification accuracy is actually about the same. What‚Äôs the problem? Aren‚Äôt graph networks supposed to work better? Well, they are, in many cases. But not in this one, because the ùìê<em>X‚ÅΩÀ°‚Åæ </em>operator we added is actually nothing else, but a Gaussian¬†filter:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*njbAaq3jLybNb7kV5x7big.png"/><figcaption>2D visualization of a filter used in a graph neural network and it‚Äôs effect on the¬†image.</figcaption></figure> <p>So, our graph neural network turned out to be equivalent to a convolutional neural network with a single Gaussian filter, that we never update during training, followed by the fully-connected layer. This filter basically blurs/smooths the image, which is not a particularly useful thing to do (see the image above on the right). However, this is the simplest variant of a graph neural network, which nevertheless works great on graph-structured data. To make GNNs work better on regular graphs, like images, we need to apply a bunch of tricks. For example, instead of using a predefined Gaussian filter, we can learn to predict an edge between any pair of pixels by using a differentiable function like¬†this:</p> <pre>import torch.nn as nn  # using PyTorch</pre> <pre>nn.Sequential(nn.Linear(4, 64),  <strong># map coordinates to a hidden layer</strong><br />              nn.ReLU(),         <strong># nonlinearity</strong><br />              nn.Linear(64, 1),  <strong># map hidden representation to edge</strong><br />              nn.Tanh())         <strong># squash edge values to [-1, 1]</strong></pre> <blockquote>To make GNNs work better on regular graphs, like images, we need to apply a bunch of tricks. For example, instead of using a predefined Gaussian filter, we can learn to predict an edge between any pair of¬†pixels.</blockquote> <p>This idea is similar to Dynamic Filter Networks (<a href="https://arxiv.org/abs/1605.09673">Brabander et al., NIPS, 2016</a>), Edge-conditioned Graph Networks (ECC, <a href="https://arxiv.org/abs/1704.02901">Simonovsky &amp; Komodakis, CVPR, 2017</a>) and (<a href="https://arxiv.org/abs/1811.09595">Knyazev et al., NeurIPS-W, 2018</a>). To try it using <a href="https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py">my code</a>, you just need to add the --pred_edge flag, so the entire command is python mnist_fc.py --model graph --pred_edge. Below I show the animation of the predefined Gaussian and learned filters. You may notice that the filter we just learned (in the middle) looks weird. That‚Äôs because the task is quite complicated since we optimize two models at the same time: the model that predicts edges and the model that predicts a digit class. To learn better filters (like the one on the right), we need to apply some other tricks from our <a href="https://arxiv.org/abs/1907.09000">BMVC paper</a>, which is beyond the scope of this part of the tutorial.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/292/1*fmwFNf4MaDL0MrdQEk1xqQ.gif"/><figcaption>2D filter of a graph neural network centered in the red point. Averaging (left, accuracy 92.24%), learned based on coordinates (middle, accuracy 91.05%), learned based on coordinates with some tricks (right, accuracy¬†92.39%).</figcaption></figure> <p>The code to generate these GIFs is quite¬†simple:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/35d81808306e1fff2dbd090b7cf08a5e/href">https://medium.com/media/35d81808306e1fff2dbd090b7cf08a5e/href</a></iframe> <p>I‚Äôm also sharing an <a href="https://nbviewer.jupyter.org/github/bknyaz/examples/blob/master/2d_convolution.ipynb">IPython notebook</a> showing 2D convolution of an image with a Gabor filter in terms of graphs (using an adjacency matrix) compared to using <a href="https://en.wikipedia.org/wiki/Circulant_matrix">circulant matrices</a>, which is often used in signal processing.</p> <p>In <a href="https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49">the next part of the tutorial</a>, I‚Äôll tell you about more advanced graph layers that can lead to better filters on¬†graphs.</p> <p><strong>Update:</strong></p> <p>Throught this blog post and in the code the dist variable should have been squared to make it a Gaussian. Thanks <a href="https://medium.com/u/96dddd48cbe9">Alfredo Canziani</a> for spotting that. All figures and results were generated without squaring it. If you observe very different results after squaring it, I suggest to tune¬†sigma.</p> <h3>Conclusion</h3> <p>Graph Neural Networks are a very flexible and interesting family of neural networks that can be applied to really complex data. As always, such flexibility must come at a certain cost. In case of GNNs it is the difficulty of regularizing the model by defining such operators as convolution. Research in that direction is advancing quite fast, so that GNNs will see application in increasingly wider areas of machine learning and computer¬†vision.</p> <p>See another <a href="https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications">nice blog post about GNNs</a> from <a href="http://neptune.ai">Neptune.ai</a>.</p> <p><em>Acknowledgement: A large portion of this tutorial was prepared during my internship at SRI International under the supervision of </em><a href="https://medium.com/u/6cf41cb2c546"><em>Mohamed Amer</em></a><em> (</em><a href="https://mohamedramer.com/"><em>homepage</em></a><em>) and my PhD advisor Graham Taylor (</em><a href="https://www.gwtaylor.ca/"><em>homepage</em></a><em>).</em></p> <p>Find me on <a href="https://github.com/bknyaz/">Github</a>, <a href="https://www.linkedin.com/in/boris-knyazev-39690948/">LinkedIn</a> and <a href="https://twitter.com/BorisAKnyazev">Twitter</a>. <a href="https://bknyaz.github.io/">My homepage</a>.</p> <p>If you want to cite this tutorial in your paper, please use:<br/><em>@misc{knyazev2019tutorial,<br/> title={Tutorial on Graph Neural Networks for Computer Vision and Beyond},<br/> author={Knyazev, Boris and Taylor, Graham W and Amer, Mohamed R},<br/> year={2019}<br/>}</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3d9fada3b80d" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>