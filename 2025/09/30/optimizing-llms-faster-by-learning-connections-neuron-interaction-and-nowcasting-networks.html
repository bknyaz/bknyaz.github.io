<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*i-R7chFd4RawxkpE"><figcaption>ICLR 2025 was held in Singapore (image source: exploreworldwide.ca)</figcaption></figure> <p>Training large neural networks is famously slow and expensive. In our recent paper, <a href="https://arxiv.org/abs/2409.04434" rel="external nofollow noopener" target="_blank"><em>Accelerating Training with Neuron Interaction and Nowcasting Networks</em></a>, presented at <strong>ICLR 2025 in Singapore</strong>, we introduced a new way to speed things up: treat a neural network as what it really is — <a href="https://arxiv.org/abs/2403.12143" rel="external nofollow noopener" target="_blank">a graph of interacting neurons (or “neural graph”)</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/835/1*mWhiD4YWTqV1tkzhdIqGKQ.png"><figcaption>Adam <strong>without </strong>and <strong>with </strong>nowcasting using our NiNo on a language task that NiNo has not seen during its training. Note the <strong>~2x reduction</strong> of the number of steps required by NiNo to achieve the same validation perplexity as by Adam.</figcaption></figure> <h3>From Adam to NiNo</h3> <p>Due to massive training costs, recent years have seen a surge of faster adaptive optimizers: <a href="https://arxiv.org/abs/1802.09568" rel="external nofollow noopener" target="_blank">Shampoo</a>, <a href="https://kellerjordan.github.io/posts/muon/" rel="external nofollow noopener" target="_blank">Muon</a>, <a href="https://arxiv.org/abs/2409.11321" rel="external nofollow noopener" target="_blank">SOAP</a>, and <a href="https://arxiv.org/pdf/2507.11005" rel="external nofollow noopener" target="_blank">others</a>. Each improves on Adam by smarter scaling and/or orthogonalization of parameter updates for a given weight matrix, often inspired by second-order methods or preconditioning. But they work at the level of parameters or layers, not the <em>network as a whole</em>. Moreover, they do not learn from the previous optimization runs, i.e. the optimization algorithms are based on manually-designed gradient-descent rules.</p> <p>Our method, <strong>NiNo (Neuron Interaction and Nowcasting)</strong>, is different. We model <strong>neurons as nodes</strong> and <strong>weights as edges</strong>, and use a graph neural network (GNN) to predict how weights will evolve. This lets us “nowcast” future parameters and reduce the number of steps required to reach the same performance metric.</p> <p>So NiNo is essentially a GNN that takes a history of past parameter values along the optimization trajectory (which can be obtained by Adam or another optimizer) and making a jump by predicting future parameter values. After making the prediction, optimization is continued with Adam, then followed by NiNo’s another prediction and so on. This basic idea is borrowed from <a href="https://proceedings.mlr.press/v202/jang23b.html" rel="external nofollow noopener" target="_blank">Weight Nowcaster Network (WNN)</a> that revealed predictable patterns in optimization trajectories, but <strong>neural graphs synergized with GNNs</strong> make it work really well.</p> <p>As NiNo is a neural network, it needs to be trained first before we can use it to speed up optimization. To do so, we collected and publicly released a <a href="https://huggingface.co/datasets/SamsungSAILMontreal/nino_metatrain" rel="external nofollow noopener" target="_blank">dataset of checkpoints</a> with optimization trajectories on 4 vision and language tasks. Even though collecting these checkpoints and training NiNo is computationally expensive, this single-time cost is amortized meaning <strong>the same trained NiNo can be potentially used on millions of tasks to eventually save much more costs.</strong></p> <p>To make NiNo work well for LLMs and Transformers in general, it was critical to carefully construct the neural graph for multi-head self-attention, making it stand out compared to WNN (that ignores the neural network structure). This is a tricky part as the illustration below shows, but we <strong>implemented it for many different Transformer layers</strong>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/982/1*mnrqkp_vUCsN4PJQfdQ3Zw.png"><figcaption>Constructing neural graphs for a MSA layer (see the details in our paper and code).</figcaption></figure> <h3>Results that Stand Out</h3> <ul> <li> <em>Figure 1 in the paper</em>: NiNo achieves a roughly <strong>~2x speedup compared to Adam</strong>, which is generally unreachable by manually-designed optimizers (which are considered remarkable if a 1.2–1.3x speedup is achieved).</li> <li> <em>Table 2 in the paper</em>: NiNo speeds up optimization across 9 vision and language tasks that we systematically evaluated, achieving the same validation performance (accuracy or perplexity) as Adam in <strong>about half the steps on average across these tasks.</strong> </li> <li> <em>Table 4 in the paper</em>: NiNo continues to speed up training for <strong>larger models</strong> beyond our main 9 tasks (that are much larger than the models in the training checkpoints used to train NiNo).</li> </ul> <h3>Learning to Optimize, Revisited</h3> <p>Our work connects to the broader <a href="https://people.idsia.ch/~juergen/diploma1987ocr.pdf" rel="external nofollow noopener" target="_blank">“learning to optimize” literature</a>, with seminal Luke Metz’s et al. works such as <a href="https://arxiv.org/abs/2211.09760" rel="external nofollow noopener" target="_blank"><em>VeLO: Training Versatile Learned Optimizers by Scaling Up</em></a>. Unlike many learned optimizers that struggle with cost, stability or show only a ~1.2–1.3x speedup¹, NiNo is conceptually lightweight and stable — because it is only applied every 1,000 steps (by default), while for all the other steps any base optimizer, such as Adam, can be applied to allow for stable convergence. At the same time, recent learned optimizers such as our recent <a href="https://openreview.net/forum?id=SLqJbt4emY" rel="external nofollow noopener" target="_blank"><em>Celo: Training Versatile Learned Optimizers on a Compute Diet</em></a><em> </em>and <a href="https://arxiv.org/abs/2406.00153" rel="external nofollow noopener" target="_blank"><em>μLO: Compute-Efficient Meta-Generalization of Learned Optimizers</em></a> make a significant step in improving learned optimizers and making them more cost-effective and stable (i.e. without big loss spikes) to train and use.</p> <p><em>¹while a ~1.2–1.3x speedup is remarkable, in practice it usually doesn’t justify the immense amount of extra work (e.g. efficient distributed implementation, tuning, potential instabilities especially in mixed/low-bit precision, investigating unexpected side effects like overfitting or poor generalization) that is required for actual large-scale usefulness.</em></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HTwrCPlWh0vveuMha2ucKQ.png"><figcaption>Neural graph of a Llama-3 based architecture (graph and adjacency matrix are visualized, see the paper for details).</figcaption></figure> <h3>Looking Forward</h3> <p>Even though NiNo shows great speedups, it requires further work, so we see many exciting paths ahead:</p> <ul> <li>Making NiNo’s step more scalable (especially memory-efficient) to larger models — currently the message passing step in its GNN is a bottleneck</li> <li>Improving speedups on larger tasks — for tasks with &gt;1B parameter models we currently observe no speedup</li> <li>Adding automatic ways to construct neural graphs</li> <li>Combining NiNo with learned optimizers to get the merits of both</li> <li>Showing theoretical guarantees for convergence</li> </ul> <p>We’ve open-sourced our code at <a href="https://github.com/SamsungSAILMontreal/nino" rel="external nofollow noopener" target="_blank">github.com/SamsungSAILMontreal/nino</a> under MIT License and welcome contributions. As we show below, applying NiNo is straightforward:</p> <pre>import torch<br>import torch.nn.functional as F<br>from transformers import AutoModelForCausalLM<br>from optim import NiNo<br><br>model = AutoModelForCausalLM.from_config(...)  # some model<br><br># NiNo is implemented as a wrapper around the base optimizer<br># any optimizer other than Adam should also be possible to use with NiNo<br>opt = NiNo(base_opt=torch.optim.AdamW(model.parameters(), <br>           lr=1e-3, <br>           weight_decay=1e-2),<br>           ckpt='checkpoints/nino.pt',<br>           subgraph=False, # can be set to True for larger models (see Llama 3.2 example below)<br>           edge_sample_ratio=0,  # can be set to a small positive number for larger models (see Llama 3.2 example below)<br>           model=model,<br>           period=1000,<br>           max_train_steps=10000)<br>for step in range(10000):<br>    if opt.need_grads:  # True/False based on the step number and period<br>        opt.zero_grad()  # zero out gradients<br>        data, targets = ...  # get some batch of data<br>        # base optimizer step (majority of the time)<br>        outputs = model(data)  # forward pass<br>        loss = F.cross_entropy(outputs, targets)  # compute some loss<br>        loss.backward()  # only compute gradients for the base optimizer            <br>    opt.step()  # base_opt step or nowcast params every 1000 steps using NiNo    <br>    ...</pre> <p>To conclude, by recognizing that neural networks are <em>networks</em> — graphs of interacting parts — we may be opening the door to a new generation of efficient and learnable optimizers.</p> <p>See <a href="https://bknyaz.github.io/">my website</a> for my other latest research and updates (<em>if you are a student passionate about this kind of topic, contact me for a potential collaboration</em>).</p> <p>Updated on Sep 30, 2025.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d9a722309eab" width="1" height="1" alt=""></p> </body></html>