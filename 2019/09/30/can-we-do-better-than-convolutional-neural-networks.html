<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4>PyTorch Implementation of “Image Classification with Hierarchical Multigraph Networks” from BMVC 2019</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/995/0*DQEo8wicTlkyZeC1"><figcaption>The number of pixels in the top row is 11, 7 and 1000 times larger (from left to right) than the number of “superpixels” in the bottom row. Can we use the superpixels rather than raw pixels as input and improve on convolutional neural networks?</figcaption></figure> <p>The British Machine Vision Conference (BMVC), finished about two weeks ago in Cardiff, UK, is one of the <a href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_computervisionpatternrecognition" rel="external nofollow noopener" target="_blank">top conferences in computer vision &amp; pattern recognition</a> with a competitive acceptance rate of 28%. Compared to others, it’s a small event, so you have plenty of time to walk around posters and talk to presenters one-on-one, which I found really nice.</p> <h3>BMVC 2019 on Twitter</h3> <p>Paper decisions were released yesterday. Congratulations to all of you! In total, we received 1008 submissions, of which 815 were valid. Of these, a total of 231 papers were accepted (38 as Oral Presentations, 193 as Poster Presentations). This amounts to a 28% acceptance rate.</p> <p>I presented a poster on <a href="https://bmvc2019.org/wp-content/uploads/papers/1186-paper.pdf" rel="external nofollow noopener" target="_blank"><strong>Image Classification with Hierarchical Multigraph Networks</strong></a><strong> </strong>on which I mainly worked during my internship at <a href="https://www.sri.com/" rel="external nofollow noopener" target="_blank">SRI International</a> under the supervision of <a href="https://filebox.ece.vt.edu/~linxiao/" rel="external nofollow noopener" target="_blank"><em>Xiao Lin</em></a><em>,</em> <a href="https://medium.com/u/6cf41cb2c546" rel="external nofollow noopener" target="_blank">Mohamed Amer</a> <em>(</em><a href="https://mohamedramer.com/" rel="external nofollow noopener" target="_blank"><em>homepage</em></a><em>) </em>and my PhD advisor <a href="https://www.gwtaylor.ca/" rel="external nofollow noopener" target="_blank"><em>Graham Taylor</em></a><em>.</em></p> <p>In the paper, we basically try to answer the question “Can we do better than Convolutional Neural Networks?”. Here I discuss this question and support my arguments by results. I also walk you through the forward pass of the whole pipeline for a single image from <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" rel="external nofollow noopener" target="_blank">PASCAL VOC 2012</a> using PyTorch.</p> <p><strong>The complete code</strong> for this post is in <a href="https://github.com/bknyaz/bmvc_2019" rel="external nofollow noopener" target="_blank">my notebook on Github.</a> It should be easy to adapt it for training and validating on the whole PASCAL dataset.</p> <p>So, why do we want to do better than ConvNets? Haven’t they outperformed humans in many tasks?</p> <p>For example, you could say that <strong>image classification</strong> is a solved task. Well, in terms of ImageNet, yes. But despite great contribution of ImageNet, it is a weird task. Why would you want to discriminate between hundreds of dog breeds? So, in result we have models succeeding in that, but failing to discriminate between slightly rotated dogs and cats. Fortunately, we now have <a href="https://arxiv.org/abs/1903.12261" rel="external nofollow noopener" target="_blank">ImageNet-C</a> and <a href="https://arxiv.org/abs/1907.07484" rel="external nofollow noopener" target="_blank">other similar benchmarks</a> showing that we are nowhere close to solving it.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/712/1*d5ZkQXZA73ASjq7B0UtDwQ.png"><figcaption>Pipeline: We solve the classical task of image classification.</figcaption></figure> <p>Another open problem arising in related tasks, like object detection, is training on really large images (e.g., 4000×3000), which is addressed, for example, by <a href="https://arxiv.org/abs/1905.03711" rel="external nofollow noopener" target="_blank">Katharopoulos &amp; Fleuret (ICML, 2019</a>) and <a href="https://bmvc2019.org/wp-content/uploads/papers/0555-paper.pdf" rel="external nofollow noopener" target="_blank">Ramapuram et al. (BMVC, 2019</a>). Thanks to the latter I now know that if the background of a poster is black, then it’s likely from Apple. I should reserve some color too!</p> <p>So, maybe we need something different than a Convolutional Neural Network? Instead of constantly patching its <a href="https://distill.pub/2019/advex-bugs-discussion/" rel="external nofollow noopener" target="_blank">bugs</a>, maybe we should use a model that has nicer properties from the beginning?</p> <p>We argue that such a model <em>can be</em><strong> </strong>a <strong>Graph Neural Network (GNN) </strong>— a neural network that can learn from graph-structured data. GNNs have some appealing properties. For example, compared to ConvNets, GNNs are inherently rotation and translation invariant, because there is simply no notion of rotation or translation in graphs, i.e. there is no left and right, there are only “neighbors” in some sense (<a href="https://arxiv.org/abs/1703.00356" rel="external nofollow noopener" target="_blank">Khasanova &amp; Frossard, ICML, 2017</a>). So, the problem of making a <a href="https://arxiv.org/abs/1602.07576" rel="external nofollow noopener" target="_blank">ConvNet generalize better to different rotations</a>, that people have been trying to solve for years, is solved automatically with GNNs!</p> <p>Regarding learning from large images, how about extracting <a href="https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic" rel="external nofollow noopener" target="_blank">superpixels</a> from images and feeding a much lower dimensional input to a GNN instead of feeding a downsampled (e.g. 224×224) image to a ConvNet? Superpixels seem to be a much better way to downsample an image compared to, say, bilinear interpolation, because they often preserve a lot of semantics by keeping the boundaries between objects. With a ConvNet we cannot directly learn from this kind of an input, however, there are some nice works proposing to leverage them (<a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14445" rel="external nofollow noopener" target="_blank">Kwak et al., AAAI, 2017</a>).</p> <p>So, a GNN sounds wonderful! Let’s see how it performs in practice.</p> <p>Oh no! Our baseline GNN based on (<a href="https://arxiv.org/abs/1609.02907" rel="external nofollow noopener" target="_blank">Kipf &amp; Welling, ICLR, 2017</a>) achieves merely 19.2% (mean average precision or mAP) on PASCAL, compared to 32.7% of a ConvNet with the same number of layers and filters in each layer.</p> <p>We propose several improvements that eventually beat the ConvNet!</p> <h3>1. Hierarchical Graph</h3> <p>In ConvNets, the hierarchical structure of images is implicitly modeled by pooling layers. In GNNs, you can achieve this in at least two ways. First, you can use pooling similar to ConvNets, but for graphs, defining a fast and good pooling method is really challenging. Instead, we can compute superpixels at multiple scales and pool superpixels by their correspondence to a larger parent superpixel. However, for some reasons this kind of pooling didn’t work well in our case (I still think it should work well). So, instead we model a hierarchy at the input level. In particular, we combine superpixels of all scales into a single set and compute hierarchical relations based on intersection over union (IoU), commonly used in semantic segmentation.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cIkKRFPlIxFtgto0B7B8fw.png"><figcaption>Three scales of 1000, 300 and 7 superpixels computed by SLIC. Note that the <a href="https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic" rel="external nofollow noopener" target="_blank">SLIC algorithm</a> that we use often returns fewer superpixels (shown on top of each image) than we request. In the middle image I show spatial connections in yellow, while in the right image — hierarchical ones that allow to connect remote nodes.</figcaption></figure> <p>Based on that principle, I build the hierarchical graph in the code below. I also build a multiscale version of the spatial graph, but it encodes only spatial relationships, while IoU should better encode hierarchical ones. For example, using IoU we can create <strong>shortcuts between remote child nodes</strong>, i.e. connect two small superpixels (e.g., wheels) that are far away spatially, but belong to the same parent node (e.g., a car) as shown on the image above.</p> <p>And indeed, the hierarchical graph boosts mAP to 31.7%, making it just 1% lower than a ConvNet while having 4 times fewer trainable parameters! If we use only the spatial multiscale graph, the results are much worse as explored in the paper.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/adf95c091aff8e3f68bc54fed950fe04/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/adf95c091aff8e3f68bc54fed950fe04/href</a></iframe> <p>Great! What else can we do to further improve results?</p> <h3>2. Learnable relations</h3> <p>So far, if we visualize our filters, they will look very primitive (as Gaussians). See <a href="https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-1-3d9fada3b80d" rel="external nofollow noopener" target="_blank">my tutorial on GNNs</a> for more details. We want to learn some edge detectors similar to ConvNets, because they work so well. But it turned out to be very challenging to learn them with GNNs. To do that, we basically need to generate edges between superpixels depending on the difference between coordinates. By doing so, we will endow our GNN with the ability to understand the coordinate system (rotation, translation). We will use a 2 layer neural network defined in PyTorch like this:</p> <pre>pred_edge = nn.Sequential(nn.Linear(2, 32),<br>                          nn.ReLU(True),<br>                          nn.Linear(32, L))</pre> <p>where <em>L</em> is the number of predicted edges or the number of filters, such as 4 in the visualization below.</p> <p>We restrict the filter to learn edges only based on the absolute difference between coordinates, |(<em>x₁,y₁</em>) - (<em>x₂,y₂</em>)|, instead of raw values, so that the filters become symmetric. This limits the capacity of filters, but it is still much better than a simple Gaussian filter used by our baseline GCN.</p> <p>In <a href="https://github.com/bknyaz/bmvc_2019/blob/master/bmvc_2019.ipynb" rel="external nofollow noopener" target="_blank">my Jupyter notebook</a>, I created a class LearnableGraph that implements the logic to predict edges given node coordinates (or any other features) and the spatial graph. The latter is used to define a small local neighborhood around each node to avoid predicting edges for all possible node pairs, because it’s expensive and doesn’t make much sense to connect very remote superpixels.</p> <p>Below, I visualize the trained pred_edge function. To do that, I assume that the current node with index 1, where we apply the convolution, is in the center of a coordinate system, <em>(x₁,y₁)=0</em>. Then I simply sample coordinates of other nodes, <em>(x₂,y₂)</em>, and feed them to pred_edge. The color shows the strength of an edge depending on the distance from a center node.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uQSsI6hajHdYvcWrdxpBpg.png"><figcaption>Visualization of predicted edges which can be interpreted as filters, where each intensity value is an edge between two nodes at a distance specified in axes.</figcaption></figure> <p>The learned graph is also very powerful, but at a larger computational cost, which is negligible if we generate a very sparse graph. The result of 32.3% is just 0.4% lower than a ConvNet and can be easily improved if we generate more filters!</p> <h3>3. Multiscale GNN</h3> <p>We now have <strong>three graphs: spatial, hierarchical and learned</strong>. A single graph convolutional layer with the spatial or hierarchical graph permits feature propagation only within the “first neighbors”. Neighbors are soft in our case, since we use a Gaussian to define the spatial graph and IoU for the hierarchical one. <a href="https://arxiv.org/abs/1606.09375" rel="external nofollow noopener" target="_blank">Defferrard et al. (NIPS, 2016</a>) proposed a multiscale (multihop) graph convolution, which aggregates features within a <em>K</em>-hop neighborhood and approximates spectral graph convolution. See <a href="https://towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49" rel="external nofollow noopener" target="_blank">my other post</a> for an extensive explanation of this method. For our spatial graph, it essentially corresponds to using multiple Gaussians of different width. For the hierarchical graph, this way we can create <em>K</em>-hop<strong> </strong>shortcuts between remote child nodes. For the learned graph, this method will create multiple scales of the learned filters visualized above.</p> <p>Using multiscale graph convolution, implemented in my GraphLayerMultiscale class, turned out to be extremely important allowing us to <strong>outperform</strong> the baseline ConvNet by 0.3%!</p> <h3>4. Improving fusion of relation types at low cost</h3> <p><strong>So far, to learn from our three graphs, we have used a standard concatenation method. </strong>This method, however, has a couple of problems. <strong>First</strong>, the number of trainable parameters of such a fusion operator is linear w.r.t. the input and output feature dimensionalities, scale (<em>K)</em> and number of relation types, so it can really grow fast if we increase two or more of these parameters at once. <strong>Second</strong>, the relation types we try to fuse can have very different natures and occupy very different subspaces of a manifold. To solve both problems at the same time, we propose learnable projections similar to (<a href="https://arxiv.org/abs/1811.09595" rel="external nofollow noopener" target="_blank">Knyazev et al., NeurIPS-W, 2018</a>). This way we decouple the linear dependency reducing the number of parameters by a factor of 2–3 compared to concatenation. In addition, learnable projections transform multirelational features so that they should occupy nearby subspaces of the manifold, facilitating the propagation of information from one relationship to another.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zBplfQQJrFs05XEzu8aKyQ.png"><figcaption>One of the proposed relation type fusion methods, which performs very well on PASCAL and allows us to beat the ConvNet by a quite large margin.</figcaption></figure> <p>By using the proposed fusion method, implemented in the GraphLayerFusion class below, we achieve 34.5% beating the ConvNet by 1.8%, while having 2 times fewer parameters! Quite impressive for the model that initially didn’t know anything about the spatial structure of images, except for information encoded in superpixels. It would be interesting to explore other fusion methods, like <a href="https://arxiv.org/abs/1803.09374" rel="external nofollow noopener" target="_blank">this one</a>, to get even better results.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/41cea34f4340597b38a3d2eb15be9e4c/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/41cea34f4340597b38a3d2eb15be9e4c/href</a></iframe> <h3>Conclusion</h3> <p>It turned out that with a multirelational graph network and some tricks, we can do better than a Convolutional Neural Network!</p> <p>Unfortunately, during our process of improving the GNN we slowly lost its invariance properties. For example, the shape of superpixels might change after rotating the image, and superpixel coordinates that we use for node features to improve the model also make it less robust.</p> <p>Nevertheless, our work is a small step towards a better image reasoning model and we show that GNNs can pave a promising direction.</p> <p>See <a href="https://github.com/bknyaz/bmvc_2019" rel="external nofollow noopener" target="_blank">my notebook on Github</a> for implementation details.</p> <p>I also highly recommend <a href="https://medium.com/u/24cd20b3728e" rel="external nofollow noopener" target="_blank">Matthias Fey</a>’s Master’s thesis with <a href="https://github.com/rusty1s/embedded_gcnn" rel="external nofollow noopener" target="_blank">the code</a> on a very related topic.</p> <p>Find me on <a href="https://github.com/bknyaz/" rel="external nofollow noopener" target="_blank">Github</a>, <a href="https://www.linkedin.com/in/boris-knyazev-39690948/" rel="external nofollow noopener" target="_blank">LinkedIn</a> and <a href="https://twitter.com/BorisAKnyazev" rel="external nofollow noopener" target="_blank">Twitter</a>. <a href="https://bknyaz.github.io/">My homepage</a>.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=46ed90fed807" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/can-we-do-better-than-convolutional-neural-networks-46ed90fed807" rel="external nofollow noopener" target="_blank">Can we do better than Convolutional Neural Networks?</a> was originally published in <a href="https://medium.com/data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>