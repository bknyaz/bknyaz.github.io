<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4>As part of the ‚ÄúTutorial on Graph Neural Networks for Computer Vision and¬†Beyond‚Äù</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Vx6uqv12rzb8HeUZl8d7-g.png"><figcaption>The Fourier basis (DFT matrix) on the left, in which each column or row is a basis vector, reshaped to 28√ó28 (on the right), i.e. 20 basis vectors are shown on the right. The Fourier basis is used to compute spectral convolution is signal processing. In graphs, the Laplacian basis is used described in this¬†post.</figcaption></figure> <p>First, let‚Äôs recall what is a graph. A graph <em>G</em> is a set of <strong>nodes</strong> (vertices) connected by directed/undirected <strong>edges</strong>. In this post, I will assume an undirected graph <em>G</em> with <em>N</em> nodes. Each <strong>node</strong> in this graph has a <em>C</em>-dimensional feature vector, and features of all nodes are represented as an <em>N</em>√ó<em>C</em> dimensional matrix <em>X‚ÅΩÀ°‚Åæ. </em><strong>Edges</strong> of a graph are represented as an <em>N</em>√ó<em>N </em>matrix A, where the entry A<em>·µ¢‚±º</em> indicates if node <em>i</em> is connected (<em>adjacent</em>) to node <em>j</em>. This matrix is called an <em>adjacency matrix</em>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/754/1*68Gcr70UTpdaX7THbZSEcA.png"><figcaption>Two undirected graphs with N=5 and N=6 nodes. The order of nodes is arbitrary.</figcaption></figure> <p>Spectral analysis of graphs (see lecture notes <a href="http://www.cs.yale.edu/homes/spielman/561/" rel="external nofollow noopener" target="_blank">here</a> and earlier work <a href="https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering" rel="external nofollow noopener" target="_blank">here</a>) has been useful for graph clustering, community discovery and other <em>mainly unsupervised </em>learning tasks. In this post, I basically describe the work of <a href="https://arxiv.org/abs/1312.6203" rel="external nofollow noopener" target="_blank">Bruna et al., 2014, ICLR 2014</a> who combined spectral analysis with convolutional neural networks (ConvNets) giving rise to spectral <strong>graph convolutional networks </strong>that can be trained in a <em>supervised </em>way, for example for the graph classification task.</p> <p>Despite that <em>spectral</em> graph convolution is currently less commonly used compared to <em>spatial</em> graph convolution methods, knowing how spectral convolution works is still helpful to understand and avoid potential problems with other methods. Plus, in the conclusion I refer to some recent exciting works making spectral graph convolution more competitive.</p> <h3>1. Graph Laplacian and a little bit of¬†physics</h3> <p>While ‚Äúspectral‚Äù may sound complicated, for our purpose it‚Äôs enough to understand that it simply means <em>decomposing</em> a signal/audio/image/graph into a combination (usually, a sum) of simple elements (wavelets, graphlets). To have some nice properties of such a <em>decomposition</em>, these simple elements are usually <em>orthogonal</em>, i.e. mutually linearly independent, and therefore form a¬†<em>basis</em>.</p> <p>When we talk about ‚Äúspectral‚Äù in signal/image processing, we imply the <a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform" rel="external nofollow noopener" target="_blank">Fourier Transform</a>, which offers us a particular <em>basis</em> (<a href="https://en.wikipedia.org/wiki/DFT_matrix" rel="external nofollow noopener" target="_blank">DFT matrix</a>, e.g. scipy.linalg.dft in Python) of elementary sine and cosine waves of different frequencies, so that we can represent our signal/image as a sum of these waves. But when we talk about graphs and graph neural networks (GNNs), ‚Äúspectral‚Äù implies <em>eigen-decomposition</em> of the <a href="https://en.wikipedia.org/wiki/Laplacian_matrix" rel="external nofollow noopener" target="_blank"><strong>graph Laplacian</strong></a><strong> </strong><em>L.</em> You can think of the the graph Laplacian <em>L</em> as an adjacency matrix <em>A</em> normalized in a special way, whereas <em>eigen-decomposition</em> is a way to find those elementary orthogonal components that make up our¬†graph.</p> <p>Intuitively, the graph Laplacian shows in what directions and how <em>smoothly</em> the ‚Äúenergy‚Äù will diffuse over a graph if we put some ‚Äúpotential‚Äù in node <em>i</em>. A typical use-case of Laplacian in mathematics and physics is to solve how a signal (wave) propagates in a dynamic system. Diffusion is <em>smooth</em> when there is no sudden changes of values between neighbors as in the animation below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/560/1*gz2hyrcSSJG9MtDzmQLe3w.gif"><figcaption>Diffusion of some signal (for example, it can be heat) in a regular grid graph computed based on the graph Laplacian (<a href="https://en.wikipedia.org/wiki/Laplacian_matrix" rel="external nofollow noopener" target="_blank">source</a>). Basically, the only things required to compute these dynamics are the Laplacian and initial values in nodes (pixels), i.e. red and yellow pixels corresponding to high intensity (of¬†heat).</figcaption></figure> <p>In the rest of the post, I‚Äôm going to assume ‚Äú<em>symmetric normalized Laplacian</em>‚Äù, which is often used in graph neural networks, because it is normalized so that when you stack many graph layers, the node features propagate in a more smooth way without explosion or vanishing of feature values or gradients. It is computed based <em>only</em> on an adjacency matrix <em>A</em> of a graph, which can be done in a few lines of Python code as¬†follows:</p> <pre><strong># Computing the graph Laplacian<br># A is an adjacency matrix of some graph <em>G</em><br></strong>import numpy as np</pre> <pre>N = A.shape[0] <strong># number of nodes in a graph</strong><br>D = np.sum(A, 0) <strong># node degrees</strong><br>D_hat = np.diag((D + 1e-5)**(-0.5)) <strong># normalized node degrees</strong><br>L = np.identity(N) ‚Äî np.dot(D_hat, A).dot(D_hat) <strong># Laplacian</strong></pre> <p>Here, we assume that <em>A</em> is symmetric, i.e. <em>A</em> = <em>A</em>·µÄ and our graph is undirected, otherwise node degrees are not well-defined and some assumptions must be made to compute the Laplacian. An interesting property of an adjacency matrix <em>A </em>is that <em>A‚Åø</em> (matrix product taken <em>n</em> times) exposes <em>n</em>-hop connections between nodes (see <a href="https://en.wikipedia.org/wiki/Adjacency_matrix#Matrix_powers" rel="external nofollow noopener" target="_blank">here</a> for more details).</p> <p>Let‚Äôs generate three graphs and visualize their adjacency matrices and Laplacians as well as their¬†powers.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WSvWVAsQsGtQQpIcrCPOOQ.png"><figcaption>Adjacency matrices, Laplacians and their powers for a random graph (left), ‚Äústar graph‚Äù (middle) and ‚Äúpath graph‚Äù (right). I normalize A¬≤ such that the sum in each row equals 1 to have a probabilistic interpretation of 2-hop connections. Notice that Laplacians and their powers are symmetric matrices, which makes eigen-decomposition easier as well as facilitates feature propagation in a deep graph¬†network.</figcaption></figure> <p>For example, imagine that the star graph above in the middle is made from metal, so that it transfers heat well. Then, if we start to heat up node 0 (dark blue), this heat will propagate to other nodes in a way defined by the Laplacian. In the particular case of a star graph with all edges equal, heat will spread uniformly to all other nodes, which is not true for other graphs due to their structure.</p> <p>In the context of computer vision and machine learning, the graph Laplacian defines how node features will be updated if we stack several graph neural layers. Similarly to <a href="https://medium.com/p/3d9fada3b80d" rel="external nofollow noopener" target="_blank"><em>the first part of my tutorial</em></a>, to understand spectral graph convolution from the computer vision perspective, I‚Äôm going to use the MNIST dataset, which defines images on a 28√ó28 regular grid¬†graph.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EUFjx4cvVq4TdmU1PfXRpA.png"><figcaption>MNIST image defining features X (left), adjacency matrix A (middle) and the Laplacian (right) of a regular 28√ó28 grid. The reason that the graph Laplacian looks like an identity matrix is that the graph has a relatively large number of nodes (784), so that after normalization values outside the diagonal become much smaller than¬†1.</figcaption></figure> <h3>2. Convolution</h3> <p>In signal processing, it can be shown that convolution in the spatial domain is multiplication in the frequency domain (a.k.a. <a href="https://en.wikipedia.org/wiki/Convolution_theorem" rel="external nofollow noopener" target="_blank">convolution theorem</a>). The same theorem can be applied to graphs. In signal processing, to transform a signal to the frequency domain, we use the Discrete Fourier Transform, which is basically matrix multiplication of a signal with a special matrix (basis, DFT matrix). This basis assumes a <em>regular</em> grid, so we cannot use it for <em>irregular</em> graphs, which is a typical case. Instead, we use a more general basis, which is eigenvectors <em>V</em> of the graph Laplacian <em>L</em>, which can be found by eigen-decomposition:<em> L</em>=<em>VŒõV·µÄ</em>, where <em>Œõ</em> are eigenvalues of¬†<em>L.</em></p> <p><strong>PCA vs eigen-decomposition of the graph Laplacian. </strong>To compute spectral graph convolution in practice, it‚Äôs enough to use a few eigenvectors corresponding to the <em>smallest</em> eigenvalues. At first glance, it seems to be an opposite strategy compared to frequently used in computer vision <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="external nofollow noopener" target="_blank">Principal component analysis (PCA)</a>, where we are more interested in the eigenvectors corresponding to the <em>largest</em> eigenvalues. However, this difference is simply due to the <em>negation</em> used to compute the Laplacian above, therefore eigenvalues computed using PCA are <em>inversely proportional</em> to eigenvalues of the graph Laplcacian (see <a href="http://outobox.cs.umn.edu/PCA_on_a_Graph.pdf" rel="external nofollow noopener" target="_blank">this paper</a> for a formal analysis). Note also that PCA is applied to the covariance matrix of a dataset for the purpose to extract the largest factors of variation, i.e. the dimensions along which data vary the most, like in <a href="https://en.wikipedia.org/wiki/Eigenface" rel="external nofollow noopener" target="_blank">Eigenfaces</a>. This variation is measured by eigenvalues, so that the smallest eigenvalues essentially correspond to noisy or ‚Äúspurious‚Äù features, which are assumed to be useless or even harmful in practice.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/643/1*k8AfLWuLW9sgOsuCarR19Q.png"><figcaption>Eigenvalues (in a descending order) and corresponding eigenvectors for the MNIST¬†dataset.</figcaption></figure> <p>Eigen-decomposition of the graph Laplacian is applied to a single graph for the purpose to extract subgraphs or clusters (communities) of nodes, and <a href="http://blog.shriphani.com/2015/04/06/the-smallest-eigenvalues-of-a-graph-laplacian/" rel="external nofollow noopener" target="_blank">eigenvalues tell us a lot about graph connectivity</a>. I will use eigenvectors corresponding to the 20 smallest eigenvalues in our examples below, assuming that 20 is much smaller than the number of nodes <em>N (N</em>=784 in case of MNIST<em>)</em>. To find eigenvalues and eigenvectors below on the left, I use a 28√ó28 regular graph, whereas on the right I follow the experiment of <a href="https://arxiv.org/abs/1312.6203" rel="external nofollow noopener" target="_blank">Bruna et al.</a> and construct an irregular graph by sampling 400 random locations on a 28√ó28 regular grid (see their paper for more details about this experiment).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*93nVzwz_V7IPC7TPlgAsjQ.png"><figcaption>Eigenvalues <em>Œõ (</em><strong><em>bottom</em></strong><em>) and e</em>igenvectors V (<strong>top</strong>) of the graph Laplacian L for a regular 28<em>√ó</em>28 grid (<strong>left</strong>) and non-uniformly subsampled grid with 400 points according to experiments in <a href="https://arxiv.org/abs/1312.6203" rel="external nofollow noopener" target="_blank">Bruna et al., 2014, ICLR 2014</a> (<strong>right</strong>). Eigenvectors corresponding to the 20 <strong>smallest</strong> <strong>eigenvalues</strong> are shown. Eigenvectors are 784 dimensional on the left and 400 dimensional on the right, so V is 784<em>√ó20 and 400√ó20 respectively. </em>Each of the 20 eigenvectors on the left was reshaped to 28<em>√ó</em>28, whereas on the right to reshape a 400 dimensional eigenvector to 28<em>√ó28, white pixels for missing nodes were added. So, e</em>ach pixel in each eigenvector corresponds to a node or a missing node (in white on the right). These eigenvectors can be viewed as a basis in which we decompose our¬†graph.</figcaption></figure> <p>So, given graph Laplacian <em>L</em>, node features <em>X</em> and filters <em>W</em>_spectral, in Python <strong>spectral convolution on graphs</strong> looks very¬†simple:</p> <pre><strong># Spectral convolution on graphs<br># X is an <em>N√ó1 matrix of 1-dimensional node features<br></em></strong><strong># L is an </strong><strong><em>N√óN</em> graph Laplacian computed above<br># W_spectral are </strong><strong><em>N√ó</em></strong><strong><em>F weights (filters) that we want to train<br></em></strong>from scipy.sparse.linalg import eigsh <strong># assumes </strong><strong><em>L</em></strong><strong> to be symmetric</strong></pre> <pre><em>Œõ</em><em>,V</em> = eigsh(L,k=20,which=‚ÄôSM‚Äô) <strong># eigen-decomposition (i.e. find <em>Œõ</em></strong><strong><em>,V)</em></strong><br>X_hat = V.T.dot(X) <strong># </strong><strong><em>20</em>√ó</strong><strong><em>1</em></strong><strong> node features in the "spectral" domain</strong><br>W_hat = V.T.dot(W_spectral)  <strong># 20√ó<em>F</em> filters in the </strong><strong>"spectral" domain</strong><br>Y = V.dot(X_hat * W_hat)  <strong># </strong><strong><em>N√ó</em></strong><strong><em>F</em></strong><strong> result of convolution</strong></pre> <p>Formally:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wBIfFw54z8usWq_merON8A.png"><figcaption>Spectral graph convolution, where ‚äô means element-wise multiplication.</figcaption></figure> <p>where we assume that our node features <em>X‚ÅΩÀ°‚Åæ </em>are 1-dimensional, e.g. MNIST pixels, but it can be extended to a <em>C</em>-dimensional case: we will just need to repeat this convolution for each <em>channel</em> and then sum over <em>C</em> as in signal/image convolution.</p> <p>Formula (3) is essentially the same as <a href="https://en.wikipedia.org/wiki/Convolution_theorem" rel="external nofollow noopener" target="_blank">spectral convolution of signals on regular grids</a> using the Fourier Transform, and so creates a few problems for machine learning:</p> <ul> <li>the dimensionality of trainable weights (filters) <em>W_</em>spectral depends on the number of nodes <em>N</em> in a¬†graph;</li> <li> <em>W_</em>spectral also depends on the graph structure encoded in eigenvectors <em>V.</em> </li> </ul> <p>These issues prevent scaling to datasets with large graphs of variable structure. Further efforts, summarized below, were focused on resolving these and other¬†issues.</p> <h3><strong>3. ‚ÄúSmoothing‚Äù in the spectral¬†domain</strong></h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/412/1*PcKEUB4wTOG6gtoEIZl9iA.png"><figcaption>Strawberry and banana smoothie (source: <a href="https://joyfoodsunshine.com/strawberry-banana-smoothie/" rel="external nofollow noopener" target="_blank">joyfoodsunshine.com</a>). Smoothing in the spectral domain is a little bit different üòÉ.</figcaption></figure> <p><a href="https://arxiv.org/abs/1312.6203" rel="external nofollow noopener" target="_blank">Bruna et al.</a> were one of the first to apply spectral graph analysis to <em>learn convolutional filters</em> for the graph classification problem. The filters learned using formula (3) above act on the <em>entire graph</em>, i.e. they have <em>global support</em>. In the computer vision context, this would be the same as training convolutional filters of size 28√ó28 pixels on MNIST, i.e. filters have the same size as the input (note that we would still slide a filter, but over a zero-padded image). While for MNIST we can actually train such filters, the common wisdom suggests to avoid that, as it makes training much harder due to the potential explosion of the number of parameters and difficulty of training large filters that can capture useful features shared across different images.</p> <p>I actually successfully trained such a model using PyTorch and <a href="https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py" rel="external nofollow noopener" target="_blank">this code</a> from my GitHub. You should run it using mnist_fc.py --model conv. After training for 100 epochs, the filters look like mixtures of¬†digits:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/864/1*kNftNPG_J4i_pUN40DjAXQ.png"><figcaption>Examples of filters with <strong>global support</strong> typically used in spectral convolution. In this case, these are 28√ó28 filters learned using a ConvNet with a single convolutional layer followed by ReLU, 7√ó7 MaxPooling and a fully-connected classification layer. To make it clear, the output of the convolutional layer is still 28√ó28 due to zero-padding. Surprisingly, this net achieves 96.7% on MNIST. This can be explained by the simplicity of the¬†dataset.</figcaption></figure> <p>To reiterate, we generally want to make filters smaller and more local (which is not exactly the same as I‚Äôll note¬†below).</p> <p>To enforce that implicitly, they proposed to <em>smooth</em> filters in the spectral domain, which makes them <em>more local</em> in the spatial domain according to the spectral theory. The idea is that you can represent our filter <em>W_</em>spectral from formula (3) as a sum of ùêæ predefined functions, such as splines, and instead of learning <em>N</em> values of <em>W</em>, we learn <em>K </em>coefficients <em>Œ±</em> of this¬†sum:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/832/1*sZoZfh6faYLBm7_Nq3xrQw.png"><figcaption>We can approximate our N dimensional filter<strong> </strong><em>W_</em>spectral as a finite sum of<em> K</em> functions f, such as splines shown below. So, instead of learning N values of <em>W_</em>spectral, we can learn K coefficients (alpha) of those functions; it becomes efficient when K &lt;&lt;¬†N.</figcaption></figure> <p>While the dimensionality of <em>fk</em> does depend on the number of nodes <em>N</em>, these functions are fixed, so we don‚Äôt learn them. The only thing we learn are coefficients <em>Œ±</em>, and so <em>W_</em>spectral is no longer dependent on <em>N</em>. Neat,¬†right?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/864/1*DJWQBxMX3hZz85pKhma34w.png"><figcaption>The spline basis used to smooth filters in the frequency domain, thereby making them more local. Splines and other polynomial functions are useful, because we can represent filters as their¬†sums.</figcaption></figure> <p>To make our approximation in formula (4) reasonable, we want <em>K</em>&lt;&lt;<em>N</em> to reduce the number of trainable parameters from <em>N</em> to <em>K</em> and, more importantly, make it independent of <em>N</em>, so that our GNN can digest graphs of any size. We can use different bases to perform this ‚Äúexpansion‚Äù, depending on which properties we need. For instance, cubic splines shown above are known as very smooth functions (i.e. you cannot see knots, i.e. where the pieces of the piecewise spline polynomial meet). The Chebyshev polynomial, which I discuss in <a href="https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49" rel="external nofollow noopener" target="_blank">my another post</a>, has the minimum ùëô‚àû distance between the approximating function. The Fourier basis is the one that preserves most of the signal energy after transformation. Most bases are orthogonal, because it would be redundant to have terms that can be expressed by each¬†other.</p> <p>Note that filters <em>W_</em>spectral are still as large as the input, but their <em>effective width </em>is small. In case of MNIST images, we would have 28√ó28 filters, in which only a small fraction of values would have an absolute magnitude larger than 0 and all of them should be located close to each other, i.e. the filter would be local and effectively small, something like the one below (second from the¬†left):</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1NL_awIic9m5P-IF5_3J2A.png"><figcaption>From left to right: (first) Input image. (second) Local filter with small effective width. Most values are very close to 0. (third) The result of spectral graph convolution of the MNIST image of digit 7 and the filter. (fourth) The result of spectral convolution using the Fourier transform. These results indicate that spectral graph convolution is quite limited if applied to images, perhaps, due to the weak spatial structure of the Laplacian basis compared to the Fourier¬†basis.</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WtfLzUxDwyU8gwAD8u5HgQ.png"><figcaption>Reconstruction of the MNIST image using the Fourier and graph Laplacian bases using only M components of V: X‚Äô=V V<em>·µÄX</em>. We can see that the bases compress different patterns in images (orientated edges in the Fourier case and global patterns in the Laplacian case). This makes results of convolutions illustrated above very different.</figcaption></figure> <p>To summarize, smoothing in the spectral domain allowed <a href="https://arxiv.org/abs/1312.6203" rel="external nofollow noopener" target="_blank">Bruna et al.</a> to learn more local filters. The model with such filters can achieve similar results as the model without smoothing (i.e. using our formula (3)), but with much fewer trainable parameters, because the filter size is independent of the input graph size, which is important to scale the model to datasets with larger graphs. However, learned filters <em>W</em>_spectral still depend on eigenvectors <em>V</em>, which makes it challenging to apply this model to datasets with variable graph structures.</p> <h3>Conclusion</h3> <p>Despite the drawbacks of the original spectral graph convolution method, it has been developed a lot and has remained a quite competitive method in some applications, because spectral filters can better capture global complex patterns in graphs, which local methods like GCN (<a href="https://arxiv.org/abs/1609.02907" rel="external nofollow noopener" target="_blank">Kipf &amp; Welling, ICLR, 2017</a>) cannot unless stacked in a deep network. For example, two ICLR 2019 papers, of <a href="https://arxiv.org/abs/1901.01484" rel="external nofollow noopener" target="_blank">Liao et al.</a> on ‚ÄúLanczosNet‚Äù and <a href="https://arxiv.org/abs/1904.07785" rel="external nofollow noopener" target="_blank">Xu et al.</a> on ‚ÄúGraph Wavelet Neural Network‚Äù, address some shortcomings of spectral graph convolution and show great results in predicting molecule properties and node classification. Another interesting work of <a href="https://arxiv.org/abs/1705.07664" rel="external nofollow noopener" target="_blank">Levie et al., 2018</a> on ‚ÄúCayleyNets‚Äù showed strong performance in node classification, matrix completion (recommender systems) and community detection. So, depending on your application and infrastructure, spectral graph convolution can be a good¬†choice.</p> <p>In another part of my <a href="https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49" rel="external nofollow noopener" target="_blank">Tutorial on Graph Neural Networks for Computer Vision and Beyond</a> I explain Chebyshev spectral graph convolution introduced by <a href="https://arxiv.org/abs/1606.09375" rel="external nofollow noopener" target="_blank">Defferrard et al.</a> in 2016, which is still a very strong baseline that has some nice properties and is easy to implement as I demonstrate using¬†PyTorch.</p> <p><em>Acknowledgement: A large portion of this tutorial was prepared during my internship at SRI International under the supervision of </em><a href="https://medium.com/u/6cf41cb2c546" rel="external nofollow noopener" target="_blank"><em>Mohamed Amer</em></a><em> (</em><a href="https://mohamedramer.com/" rel="external nofollow noopener" target="_blank"><em>homepage</em></a><em>) and my PhD advisor Graham Taylor (</em><a href="https://www.gwtaylor.ca/" rel="external nofollow noopener" target="_blank"><em>homepage</em></a><em>). I also thank </em><a href="https://www.linkedin.com/in/carolynaugusta/" rel="external nofollow noopener" target="_blank"><em>Carolyn Augusta</em></a><em> for useful feedback.</em></p> <p>Find me on <a href="https://github.com/bknyaz/" rel="external nofollow noopener" target="_blank">Github</a>, <a href="https://www.linkedin.com/in/boris-knyazev-39690948/" rel="external nofollow noopener" target="_blank">LinkedIn</a> and <a href="https://twitter.com/BorisAKnyazev" rel="external nofollow noopener" target="_blank">Twitter</a>. <a href="https://bknyaz.github.io/">My homepage</a>.</p> <p>If you want to cite this blog post in your paper, please use:<br><a href="http://twitter.com/misc" rel="external nofollow noopener" target="_blank"><em>@misc</em></a><em>{knyazev2019tutorial,<br> title={Tutorial on Graph Neural Networks for Computer Vision and Beyond},<br> author={Knyazev, Boris and Taylor, Graham W and Amer, Mohamed R},<br> year={2019}<br>}</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2e495b57f801" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801" rel="external nofollow noopener" target="_blank">Spectral Graph Convolution Explained and Implemented Step By Step</a> was originally published in <a href="https://medium.com/data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>