<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4>As part of the â€œTutorial on Graph Neural Networks for Computer Vision andÂ Beyondâ€</h4> <p><em>Iâ€™m presenting an overview of important Graph Neural Network works, by distilling key ideas and explaining simple intuition behind milestone methods using Python and PyTorch. This post continues </em><a href="https://medium.com/p/3d9fada3b80d" rel="external nofollow noopener" target="_blank"><em>the first part of my tutorial</em></a><em>.</em></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GMDzusqMN82diJjSxgSW8w.png"><figcaption>Graph of Graph Neural Network (GNN) and related works. Some other important works and edges are not shown to avoid further clutter. For example, there is a large body of works on dynamic graphs that deserve a separate overview. Best viewed on a very wide screen inÂ color.</figcaption></figure> <h3>20+ years of Graph NeuralÂ Networks</h3> <p>In the â€œ<strong>Graph of Graph Neural Network (GNN) and related works</strong>â€ above, I added papers on graphs that I have come across in the last year. In this graph, a directed edge between two works denotes that one paper is based on the other (while not necessary citing it) and a color of the workÂ denotes:</p> <ul> <li>Redâ€Šâ€”â€Š<strong>spectral methods</strong> (require eigen-decomposition of the graph Laplacian, which will be explained below)</li> <li>Greenâ€Šâ€”â€Šmethods that work in the <strong>spatial domain</strong> (do not require eigen-decomposition of the graph Laplacian)</li> <li>Blueâ€Šâ€”â€Šequivalent to spectral methods, but do not require eigen-decomposition (so, effectively, spatialÂ methods)</li> <li>Blackâ€Šâ€”â€Šare methods complementary to GNNs and agnostic to the choice of a GNN itself (i.e. pooling, attention).</li> </ul> <p>Note, that some other important works and edges are not shown to avoid further clutter, and only a tiny fraction of works, highlighted <strong>in bold</strong> boxes, will be covered in this post. Disclaimer: I still found room to squeeze our own recent works thereÂ ğŸ˜Š.</p> <p>Most of the important methods are covered in this non-exhaustive list ofÂ works:</p> <ul> <li>Nicket et al., 2015, <a href="https://arxiv.org/abs/1503.00759" rel="external nofollow noopener" target="_blank">A Review of Relational Machine Learning for Knowledge Graphs</a> </li> <li>Bronstein et al., 2016, <a href="https://arxiv.org/abs/1611.08097" rel="external nofollow noopener" target="_blank">Geometric deep learning: going beyond Euclidean data</a> </li> <li>Hamilton et al., 2017, <a href="https://arxiv.org/abs/1709.05584" rel="external nofollow noopener" target="_blank">Representation Learning on Graphs: Methods and Applications</a> </li> <li>Kipf et al., 2018, <a href="http://tkipf.github.io/misc/SlidesCambridge.pdf" rel="external nofollow noopener" target="_blank">Structured deep models: Deep learning on graphs and beyond</a>, presentation slides.</li> <li>Battaglia et al., 2018, <a href="https://arxiv.org/abs/1806.01261" rel="external nofollow noopener" target="_blank">Relational inductive biases, deep learning, and graphÂ networks</a> </li> <li>Zhang et al., 2018 <a href="https://arxiv.org/abs/1812.04202" rel="external nofollow noopener" target="_blank">Deep Learning on Graphs: AÂ Survey</a> </li> <li>Zhou et al., 2018, <a href="https://arxiv.org/abs/1812.08434" rel="external nofollow noopener" target="_blank">Graph Neural Networks: A Review of Methods and Applications</a> </li> <li>Wu et al., 2019, <a href="https://arxiv.org/abs/1901.00596" rel="external nofollow noopener" target="_blank">A Comprehensive Survey on Graph Neural<br>Networks</a> </li> <li>Petar VeliÄkoviÄ‡, 2019, <a href="https://www.repository.cam.ac.uk/handle/1810/292230" rel="external nofollow noopener" target="_blank">The resurgence of structure in deep neural networks</a>, PhDÂ Thesis.</li> <li>NIPS and CVPR <a href="https://sungsoo.github.io/2018/02/01/geometric-deep-learning.html" rel="external nofollow noopener" target="_blank">video tutorials</a> </li> </ul> <p>The first work where graphs were classified using a neural network seems to be a <strong>1997</strong> paper by <a href="https://ieeexplore.ieee.org/document/572108" rel="external nofollow noopener" target="_blank">Alessandro Sperduti and Antonina Starita on â€œSupervised Neural Networks for the Classification of Structuresâ€</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9CdkztOnwaRFkiZTu1gYgQ.png"><figcaption>A figure from (<a href="https://ieeexplore.ieee.org/document/572108" rel="external nofollow noopener" target="_blank">Sperduti &amp; Starita, 1997</a>), which is strikingly similar to what we are doing now, after more than 20Â years.</figcaption></figure> <blockquote> <a href="https://ieeexplore.ieee.org/document/572108" rel="external nofollow noopener" target="_blank">Sperduti &amp; Starita, 1997</a>: â€œUntil now neural networks have been used for classifying unstructured patterns and sequences. However, standard neural networks and statistical methods are usually believed to be inadequate when dealing with complex structures because of their feature-based approach.â€</blockquote> <p>From 1997, the body of works on learning from graphs has grown so much and in so many diverse directions that it is very hard to keep track without some smart automated system. I believe we are converging to using methods based on neural networks (based on our formula (2) explained in <a href="https://medium.com/p/3d9fada3b80d" rel="external nofollow noopener" target="_blank"><em>the first part of my tutorial</em></a><em>)</em>, or some combination of neural networks and otherÂ methods.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/741/1*Ht9tBXaTrV2gkbMIe7zxMg.png"><figcaption>Graph neural layerâ€™s formula (2) from <a href="https://medium.com/p/3d9fada3b80d" rel="external nofollow noopener" target="_blank"><em>the first part of my tutorial</em></a><em> that we will also need in this part. Keep in mind, that if we need to compute a specific loss for the output features or if we need to stack these layers, we apply some activation like ReLU orÂ Softmax.</em></figcaption></figure> <p>To recap the notation we used in the first part, we have some undirected graph <em>G</em> with <em>N</em> nodes. Each node in this graph has a <em>C</em>-dimensional feature vector, and features of all nodes are represented as an <em>N</em>Ã—<em>C</em> dimensional matrix <em>Xâ½Ë¡â¾. </em>In a typical graph network, such as GCN (<a href="https://arxiv.org/abs/1609.02907" rel="external nofollow noopener" target="_blank">Kipf &amp; Welling, ICLR, 2017</a>), we feed these features <em>Xâ½Ë¡â¾</em> to a graph neural layer with <em>C</em>Ã—<em>F</em> dimensional trainable weights <em>Wâ½Ë¡â¾Â </em>, so that the output of this layer is an <em>N</em>Ã—<em>F</em> matrix <em>Xâ½Ë¡âºÂ¹</em>â¾ encoding updated (and hopefully better in some sense) node features. ğ“ is an <em>N</em>Ã—<em>N </em>matrix, where the entry ğ“<em>áµ¢â±¼</em> indicates if node <em>i</em> is connected (<em>adjacent</em>) to node <em>j</em>. This matrix is called an <em>adjacency matrix</em>. I use ğ“ instead of plain <em>A</em> to highlight that this matrix can be <em>normalized</em> in a way to facilitate feature propagation in a deep network. For the purpose of this tutorial, we can assume that ğ“=<em>A</em>, i.e. each <em>i</em>-th<em> </em>row of the matrix product ğ“<em>Xâ½Ë¡â¾ </em>will contain a sum of features of node <em>i</em> neighbors.</p> <p>In the rest of this part of the tutorial, Iâ€™ll briefly explain works of my choice showed in <strong>bold</strong> boxes in the overview graph. I recommend <a href="https://arxiv.org/abs/1611.08097" rel="external nofollow noopener" target="_blank">Bronstein et al.â€™s review</a> for a more comprehensive and formal analysis.</p> <p>Note that even though I dive into some technical details of <strong>spectral graph convolution</strong> below, many recent works (e.g., GIN in <a href="https://arxiv.org/abs/1810.00826" rel="external nofollow noopener" target="_blank">Xu et al., ICLR, 2019</a>) are built without spectral convolution and show great results in some tasks. However, knowing how spectral convolution works is still helpful to understand and avoid potential problems with otherÂ methods.</p> <h3><strong>1. Spectral graph convolution</strong></h3> <p><a href="https://arxiv.org/abs/1312.6203" rel="external nofollow noopener" target="_blank">Bruna et al., 2014, ICLRÂ 2014</a></p> <p>I explain spectral graph convolution in detail in my <a href="https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801" rel="external nofollow noopener" target="_blank">anotherÂ post</a>.</p> <p>Iâ€™ll briefly summarize it here for the purpose of this part of the tutorial. A formal definition of spectral graph convolution, which is very similar to the <a href="https://en.wikipedia.org/wiki/Convolution_theorem" rel="external nofollow noopener" target="_blank">convolution theorem</a> in signal/image processing, can be writtenÂ as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wBIfFw54z8usWq_merON8A.png"><figcaption>Spectral graph convolution, where âŠ™ means element-wise multiplication.</figcaption></figure> <p>where <em>V</em> are eigenvectors and <em>Î›</em> are eigenvalues of the <a href="https://en.wikipedia.org/wiki/Laplacian_matrix" rel="external nofollow noopener" target="_blank"><strong>graph Laplacian</strong></a><strong> </strong><em>L</em>, which can be found by eigen-decomposition:<em> L</em>=<em>VÎ›Váµ€; W</em>_spectral are filters. Throughout this tutorial Iâ€™m going to assume â€œ<em>symmetric normalized Laplacian</em>â€. It is computed based <em>only </em>on an adjacency matrix <em>A</em> of a graph, which can be done in a few lines of Python code asÂ follows:</p> <pre><strong># Computing the graph Laplacian<br># A is an adjacency matrix<br></strong>import numpy as np</pre> <pre>N = A.shape[0] <strong># number of nodes in a graph</strong><br>D = np.sum(A, 0) <strong># node degrees</strong><br>D_hat = np.diag((D + 1e-5)**(-0.5)) <strong># normalized node degrees</strong><br>L = np.identity(N) â€” np.dot(D_hat, A).dot(D_hat) <strong># Laplacian</strong></pre> <p>Here, we assume that <em>A</em> is symmetric, i.e. <em>A</em> = <em>A</em>áµ€ and our graph is undirected, otherwise node degrees are not well-defined and some assumptions must be made to compute the Laplacian. In the context of computer vision and machine learning, the graph Laplacian defines how node features will be updated if we stack several graph neural layers in the form of formulaÂ (2).</p> <p>So, given graph Laplacian <em>L</em>, node features <em>X</em> and filters <em>W</em>_spectral, in Python <strong>spectral convolution on graphs</strong> looks veryÂ simple:</p> <pre><strong># Spectral convolution on graphs<br># X is an <em>NÃ—1 matrix of 1-dimensional node features<br></em></strong><strong># L is an </strong><strong><em>NÃ—N</em> graph Laplacian computed above<br># W_spectral are </strong><strong><em>NÃ—</em></strong><strong><em>F weights (filters) that we want to train<br></em></strong>from scipy.sparse.linalg import eigsh <strong># assumes </strong><strong><em>L</em></strong><strong> to be symmetric</strong></pre> <pre><em>Î›</em><em>,V</em> = eigsh(L,k=20,which=â€™SMâ€™) <strong># eigen-decomposition (i.e. find <em>Î›</em></strong><strong><em>,V)</em></strong><br>X_hat = V.T.dot(X) <strong># </strong><strong><em>20</em>Ã—</strong><strong><em>1</em></strong><strong> node features in the "spectral" domain</strong><br>W_hat = V.T.dot(W_spectral)  <strong># 20Ã—<em>F</em> filters in the </strong><strong>"spectral" domain</strong><br>Y = V.dot(X_hat * W_hat)  <strong># </strong><strong><em>NÃ—</em></strong><strong><em>F</em></strong><strong> result of convolution</strong></pre> <p>where we assume that our node features <em>Xâ½Ë¡â¾ </em>are 1-dimensional, e.g. MNIST pixels, but it can be extended to a <em>C</em>-dimensional case: we will just need to repeat this convolution for each <em>channel</em> and then sum over <em>C</em> as in signal/image convolution.</p> <p>Formula (3) is essentially the same as <a href="https://en.wikipedia.org/wiki/Convolution_theorem" rel="external nofollow noopener" target="_blank">spectral convolution of signals on regular grids</a> using the Fourier Transform, and so creates a few problems for machine learning:</p> <ol> <li>the dimensionality of trainable weights (filters) <em>W_</em>spectral depends on the number of nodes <em>N</em> in aÂ graph;</li> <li> <em>W_</em>spectral also depends on the graph structure encoded in eigenvectors <em>V.</em> </li> </ol> <p>These issues prevent scaling to datasets with large graphs of variable structure.</p> <p>To solve the first issue, <a href="https://arxiv.org/abs/1312.6203" rel="external nofollow noopener" target="_blank">Bruna et al.</a> proposed to <em>smooth</em> filters in the spectral domain, which makes them <em>more local</em> in the spatial domain according to the spectral theory. The idea is that you can represent our filter <em>W_</em>spectral from formula (3) as a sum of ğ¾ predefined functions, such as splines, and instead of learning <em>N</em> values of <em>W</em>, we learn <em>K </em>coefficients <em>Î±</em> of thisÂ sum:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/832/1*sZoZfh6faYLBm7_Nq3xrQw.png"><figcaption>We can approximate our N dimensional filter<strong> </strong><em>W_</em>spectral as a finite sum of<em> K</em> functions f, such as splines shown below. So, instead of learning N values of <em>W_</em>spectral, we can learn K coefficients (alpha) of those functions; it becomes efficient when K &lt;&lt;Â N.</figcaption></figure> <p>While the dimensionality of <em>fk</em> does depend on the number of nodes <em>N</em>, these functions are fixed, so we donâ€™t learn them. The only thing we learn are coefficients <em>Î±</em>, and so <em>W_</em>spectral is no longer dependent on <em>N</em>. To make our approximation in formula (4) reasonable, we want <em>K</em>&lt;&lt;<em>N</em> to reduce the number of trainable parameters from <em>N</em> to <em>K</em> and, more importantly, make it independent of <em>N</em>, so that our GNN can digest graphs of anyÂ size.</p> <p>While solves the first issue, this smoothing method does not address the secondÂ issue.</p> <h3> <strong>2. Chebyshev</strong> graph <strong>convolution</strong> </h3> <p><a href="https://arxiv.org/abs/1606.09375" rel="external nofollow noopener" target="_blank">Defferrard et al., NeurIPS,Â 2016</a></p> <p>The main drawback of spectral convolution and its smooth version above is that it still requires eigen-decomposition of an <em>N</em>Ã—<em>N</em> dimensional graph Laplacian <em>L</em>, which creates two main problems:</p> <ol> <li>ğŸ™ The complexity of eigen-decomposition is huge, O(<em>NÂ³</em>). Moreover in case of large graphs, keeping the graph Laplacian in a dense format in RAM is infeasible. One solution is to use sparse matrices and find eigenvectors using scipy.sparse.linalg.eigs in Python. Additionally, you may preprocess all training graphs on a dedicated server with a lot of RAM and CPU cores. In many applications, your test graphs can also be preprocessed in advance, but if you have a constant influx of new large graphs, eigen-decomposition will make youÂ sad.</li> <li>ğŸ™ Another problem is that the model you train ends up being closely related to the eigenvectors <em>V</em> of the graph. This can be a big problem if your training and test graphs have very different structures (numbers of nodes and edges). Otherwise, if all graphs are very similar, it is less of a problem. Moreover, if you use some smoothing of filters in the frequency domain like splines discussed above, then your filters become more localized and the problem of adapting to new graphs seems to be even less noticeable. However, the models will still be quiteÂ limited.</li> </ol> <p>Now, what does Chebyshev graph convolution have to do with allÂ that?</p> <p>It turns out that it solves <strong>both problems at the same time!</strong>Â ğŸ˜ƒ</p> <p>That is, it avoids computing costly eigen-decomposition and the filters are no longer â€œattachedâ€ to eigenvectors (yet they still are functions of eigenvalues <em>Î›)</em>. Moreover, it has a very useful parameter, usually denoted as <em>K</em> having a similar intuition as <em>K</em> in our formula (4) above, determining the locality of filters. Informally: for <em>K</em>=1, we feed just node features <em>Xâ½Ë¡â¾</em> to our GNN; for <em>K</em>=2, we feed <em>Xâ½Ë¡â¾</em><strong> </strong>and ğ“<em>Xâ½Ë¡â¾</em>; for K=3, we feed <em>Xâ½Ë¡â¾</em><strong>,</strong> ğ“<em>Xâ½Ë¡â¾</em><strong> </strong>and ğ“Â²<em>Xâ½Ë¡â¾</em>; and so forth for larger <em>K</em> (I hope youâ€™ve noticed the pattern). See more accurate and formal definition in <a href="https://arxiv.org/abs/1606.09375" rel="external nofollow noopener" target="_blank">Defferrard et al.</a> and my code below, plus additional analysis is given in (<a href="https://arxiv.org/abs/1811.09595" rel="external nofollow noopener" target="_blank">Knyazev et al., NeurIPS-W, 2018</a>).</p> <p>Due to <a href="https://en.wikipedia.org/wiki/Adjacency_matrix#Matrix_powers" rel="external nofollow noopener" target="_blank">the power property</a> of adjacency matrices, when we perform ğ“Â²<em>Xâ½Ë¡â¾</em> we actually average (or sum depending on how ğ“ is normalized) over 2-hop neighbors, and analogously for any <em>n </em>in ğ“<em>â¿Xâ½Ë¡â¾</em> as illustrated below, where we average over <em>n</em>-hop neighbors.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ybJ4HOmtSwhmCB1f2JX07A.png"><figcaption>Chebyshev convolution for <em>K</em>=3 for node 1 (dark blue). Circled nodes denote the nodes affecting feature representation of node 1. The [,] operator denotes concatenation over the feature dimension. W<em>â½Ë¡â¾ are 3C</em>Ã—F dimensional weights.</figcaption></figure> <p>Note that to satisfy the orthogonality of the Chebyshev basis, ğ“<strong> </strong>assumes no loops in the graph, so that in each <em>i</em>-th row of matrix product ğ“<em>Xâ½Ë¡â¾</em> we will have features of the neighbors of node <em>i</em>, but <strong>not</strong> the features of node <em>i</em> itself. Features of node <em>i</em> will be fed separately as a matrixÂ <em>Xâ½Ë¡â¾.</em></p> <p>If <em>K</em> equals the number of nodes <em>N</em>, the Chebyshev convolution closely approximates a spectral convolution, so that the receptive field of filters will be the entire graph. But, as in the case of convolutional networks, we donâ€™t want our filters to be as big as the input images for a number of reasons that I already discussed, so in practice, <em>K</em> takes reasonably smallÂ values.</p> <blockquote>In my experience, this is one of the most powerful GNNs, achieving great results in a very wide range of graph tasks. The main downside is the necessity to loop over <em>K</em> in the forward/backward pass (since Chebyshev polynomials are recursive, so itâ€™s not possible to parallelize them), which slows down theÂ model.</blockquote> <p>Same as with Splines discussed above, instead of training filters, we train coefficients, but this time, of the Chebyshev polynomial.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/864/1*SGSYcSA5WqGYPYDwKTQT1g.png"><figcaption>Chebyshev basis used to approximate convolution in the spectralÂ domain.</figcaption></figure> <p>To generate the Chebyshev basis, you can use the following PythonÂ code:</p> <pre><strong># Set K to some integer &gt; 0, like 4 or 5 in our plots above<br># Set n_points to a number of points on a curve (we set to 100)<br></strong>import numpy as np</pre> <pre>x = np.linspace(-1, 1, n_points)<br>T = np.zeros((K, len(x)))<br>T[0,:] = 1<br>T[1,:] = x<br>for n in range(1, K-1):<br>    T[n+1, :] = 2*x*T[n, :] - T[n-1, :] <strong># recursive computation</strong>   <br>return T</pre> <p>The full code to generate spline and Chebyshev bases is in <a href="https://github.com/bknyaz/examples/blob/master/splines_cheb.py" rel="external nofollow noopener" target="_blank">my githubÂ repo</a>.</p> <p>To illustrate how a Chebyshev filter can look on a irregular grid, I follow the experiment from <a href="https://arxiv.org/abs/1312.6203" rel="external nofollow noopener" target="_blank">Bruna et al.</a> again and sample 400 random points from the MNIST grid in the same way as I did to show eigenvectors of the graph Laplacian. I trained a Chebyshev graph convolution model on the MNIST images sampled from these 400 locations (same irregular grid is used for all images) and one of the filter for <em>K</em>=1 and <em>K</em>=20 is visualized below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/396/1*Hd0dkgJNOfOs5KAo3oIiwQ.gif"><figcaption>A single Chebyshev filter (K=3 on the left and K=20 on the right) trained on MNIST and applied at different locations (shown as a red pixel) on a irregular grid with 400 points. Compared to filters of standard ConvNets, GNN filters have different shapes depending <em>on the node at which they are applied</em>, because each node has a different neighborhood structure.</figcaption></figure> <h3><strong>3. GCN</strong></h3> <p><a href="https://arxiv.org/abs/1609.02907" rel="external nofollow noopener" target="_blank">Kipf &amp; Welling, ICLR,Â 2017</a></p> <p>As you may have noticed, if you increase <em>K</em> of the Chebyshev convolution, it increases the total number of trainable parameters. For example, for <em>K</em>=2, our weights <em>Wâ½Ë¡â¾</em> will be 2<em>C</em>Ã—<em>F</em> instead of just <em>C</em>Ã—<em>F</em>. This is because we concatenate features <em>Xâ½Ë¡â¾</em><strong> </strong>and ğ“<em>Xâ½Ë¡â¾</em> into a single <em>N</em>Ã—2<em>C</em> matrix. More training parameters means the model<em> </em>is<em> </em>more difficult to train and more data must be labeled<em> </em>for training. Graph datasets are often extremely small. Whereas in computer vision, MNIST is considered a tiny dataset, because images are just 28Ã—28 dimensional and there are only 60k training images, in terms of graph networks MNIST is quite large, because each graph would have <em>N</em>=784 nodes and 60k is a large number of training graphs. In contrast to computer vision tasks, many graph datasets have only around 20â€“100 nodes and 200â€“1000 training examples. These graphs can represent certain small molecules and labeling chemical/biological data is usually more expensive than labeling images. Therefore, training Chebyshev convolution models can lead to severe overfitting of the training set (i.e. the model will have the training loss close to 0 yet will have a large validation or test error). So, GCN of <a href="https://arxiv.org/abs/1609.02907" rel="external nofollow noopener" target="_blank">Kipf &amp; Welling</a> essentially â€œmergedâ€ matrices of node features <em>Xâ½Ë¡â¾</em><strong> </strong>and ğ“<em>Xâ½Ë¡â¾</em> into a single <em>N</em>Ã—<em>C</em> matrix. As a result, the model has two times fewer parameters to train compared to Chebyshev convolution with <em>K</em>=2, yet has the same receptive field of 1 hop. The main trick involves adding â€œself-loopsâ€ to your graph by adding an <a href="https://en.wikipedia.org/wiki/Identity_matrix" rel="external nofollow noopener" target="_blank">identity matrix</a> <em>I</em> to ğ“<em> </em>and normalizing it in a particular way, so now in each <em>i</em>-th row of matrix product ğ“<em>Xâ½Ë¡â¾</em> we will have features of the neighbors of node <em>i, </em><strong>as well as</strong> features of nodeÂ <em>i.</em></p> <blockquote>This model seems to be a standard baseline choice well-suited for many application due to its lightweight, good performance and scalability to largerÂ graphs.</blockquote> <h4>3.1. GCN vs Chebyshev layer</h4> <p>The difference between GCN and Chebyshev convolution is illustrated below.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2a9263af373ed75b1d578f00612e0ef8/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/2a9263af373ed75b1d578f00612e0ef8/href</a></iframe> <p>The code above follows the same structure as in <a href="https://medium.com/p/3d9fada3b80d" rel="external nofollow noopener" target="_blank"><em>the first part of my tutorial</em></a>, where I compared classical NN and GNN. One of the main steps both in GCN and Chebyshev convolution is computation of the rescaled graph Laplacian <em>L</em>. This rescaling is done to make eigenvalues in the range [-1,1] to facilitate training (this might be not a very important step in practice as weights can adapt during training). In GCN, self-loops are added to the graph by adding an identity matrix before computing the Laplacian as discussed above. The main difference between the two methods is that in the Chebyshev convolution we <em>recursively</em> loop over <em>K</em> to capture features in the <em>K</em>-hop neighborhood. We can stack such GCN or Chebyshev layers interleaved with nonlinearities to build a Graph NeuralÂ Network.</p> <p>Now, let me politely interrupt ğŸ˜ƒ our spectral discussion and give a general idea behind two other exciting methods: Edge-conditioned filters by <a href="https://arxiv.org/abs/1704.02901" rel="external nofollow noopener" target="_blank">Simonovsky &amp; Komodakis, CVPR, 2017</a> and MoNet by <a href="https://arxiv.org/abs/1611.08402" rel="external nofollow noopener" target="_blank">Monti et al., CVPR, 2017</a>, which share some similar concepts.</p> <h3> <strong>4. Edge-conditioned</strong> filters</h3> <p><a href="https://arxiv.org/abs/1704.02901" rel="external nofollow noopener" target="_blank">Simonovsky &amp; Komodakis, CVPR,Â 2017</a></p> <p>As you know, in ConvNets we learn the weights (filters) by optimizing some loss like <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss" rel="external nofollow noopener" target="_blank">Cross Entropy</a>. In the same way, we learn our <em>Wâ½Ë¡â¾ </em>in GNNs. Imagine that instead of learning these weights, you have <em>another network</em> that predicts them. So during training, we learn the weights of that auxiliary network, which takes an image or a graph as an input and returns weights <em>Wâ½Ë¡â¾ </em>(Î˜ in their work) as the output. The idea is based on <strong>Dynamic Filter Networks</strong> (<a href="https://arxiv.org/abs/1605.09673" rel="external nofollow noopener" target="_blank">Brabandere et al., NIPS, 2016</a>), where â€œdynamicâ€ means that filters <em>Wâ½Ë¡â¾ </em>will be different depending on the input as opposed to standard models in which filters are fixed (or static) after training.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/779/1*0v1xygb2cN-3do55tAh1eg.png"><figcaption>Using an auxiliary â€œfilter generating networkâ€ FË¡ to predict edge-specific weights Î˜ for the main network. XË¡â»Â¹ are input node features and XË¡ are output features. The figure shows a single iteration of â€œdynamic convolutionâ€ for node 1 (in yellow). Standard GNNs typically would simply average (or sum) features of node 1 neighbors (nodes 2, 3, 4, 5)Â , which would correspond to having an isotropic filter (Î˜ would be a constant vector). In contrast, this model has anisotropic filters, because it predicts different edge values between node 1 and all itâ€™s neighbors based on edge labels L, so that features XË¡(1) are computed as a weighted average of neighborsâ€™ features. Figure from (<a href="https://arxiv.org/abs/1704.02901" rel="external nofollow noopener" target="_blank">Simonovsky &amp; Komodakis, CVPR,Â 2017</a>).</figcaption></figure> <p>This is a very general form of convolution that, besides images, can be easily applied to graphs or point clouds as they did in their CVPR paper and got excellent results. However, there is no â€œ<a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem" rel="external nofollow noopener" target="_blank">free lunch</a>â€, and training such models is quite challenging, because the regular grid constraint is now relaxed and the scope of solutions increases dramatically. This is especially true for larger graphs with many edges or for convolution in deeper layers, which often have hundreds of channels (number of features, <em>C)</em>, so you might end up generating thousands of numbers in total for each input! In this regard, standard ConvNets are so good, because we donâ€™t waste the modelâ€™s capacity on training to predict these weights, instead we directly enforce that the filters should be the same for all inputs. But this prior makes ConvNets limited and we cannot directly apply them to graphs or point clouds. So, as always, thereâ€™s some trade-off between flexibility and performance in a particular task.</p> <blockquote>When applied to images, like MNIST, the Edge-conditioned model can learn to predict <em>anisotropic</em> filtersâ€Šâ€”â€Šfilters that are sensitive to orientation, such as edge detectors. Compared to Gaussian filters discussed in <a href="https://medium.com/p/3d9fada3b80d" rel="external nofollow noopener" target="_blank"><em>the first part of my tutorial</em></a>, these filters are able to better capture certain patterns in images, such as strokes inÂ digits.</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/519/1*aApSWc8LXLpEwvO312yuUg.png"><figcaption>Convolutional filters learned on MNIST sampled in low (left) and high (right) resolutions. Figure from (<a href="https://arxiv.org/abs/1704.02901" rel="external nofollow noopener" target="_blank">Simonovsky &amp; Komodakis, CVPR,Â 2017</a>).</figcaption></figure> <p>I want to highlight one more time that whenever we have a complicated model with auxiliary networks, it becomes a chicken-or-the-egg problem in some sense. To solve it, one of the networksâ€Šâ€”â€Šthe auxiliary or the main oneâ€Šâ€”â€Šshould receive a very strong signal, so that it can implicitly supervise another network. In our <a href="https://arxiv.org/abs/1907.09000" rel="external nofollow noopener" target="_blank">BMVC paper</a>, which is similar to <a href="https://arxiv.org/abs/1704.02901" rel="external nofollow noopener" target="_blank">Simonovsky &amp; Komodakis</a>â€™s work, we apply additional constraints on the edge-generating network to facilitate training. I will describe our work in detail in laterÂ posts.</p> <h3><strong>5. MoNet</strong></h3> <p><a href="https://arxiv.org/abs/1611.08402" rel="external nofollow noopener" target="_blank">Monti et al., CVPR,Â 2017</a></p> <p>MoNet is different from other works discussed in this post, as it assumes to have the notion of node coordinates, and therefore is more suited for geometric tasks such as 3D mesh analysis or image/video reasoning. It is somewhat similar to edge-conditioned filters of <a href="https://arxiv.org/abs/1704.02901" rel="external nofollow noopener" target="_blank">Simonovsky &amp; Komodakis</a>, since they also introduce an auxiliary learnable function ğ·(ğ‘¤, ğœƒ<em>, Ï</em>) that predicts weights. The difference is that these weights depend on the node polar coordinates (angle ğœƒ and radius <em>Ï</em>); and trainable parameters ğ‘¤ of that function are constrained to be means and variances of Gaussians, so that instead of learning <em>N</em>Ã—<em>N</em> matrices, we only learn fixed-size vectors (means and variances) independently of the graph size <em>N</em>. In terms of standard ConvNets, it would be the same as learning only 2 values (the mean and variance of a Gaussian) for each filter instead of learning 9, 25 or 121 values for 3Ã—3, 5Ã—5 or 11Ã—11 dimensional filters respectively. This <em>parameterization</em> would greatly reduce the number of parameters in a ConvNet, but the filters would be very limited in their power to capture image features.</p> <p><a href="https://arxiv.org/abs/1611.08402" rel="external nofollow noopener" target="_blank">Monti et al.</a> train ğ½ means and variances of Gaussians and the process of transforming node coordinates is similar to fitting them into a <a href="https://scikit-learn.org/stable/modules/mixture.html" rel="external nofollow noopener" target="_blank">Gaussian Mixture Model</a>. The model is quite computationally intensive to train if we want our filters to be global enough, but it can be a good choice for visual tasks (see our <a href="https://arxiv.org/abs/1907.09000" rel="external nofollow noopener" target="_blank">BMVC paper</a> for comparison), yet it is often worse than simple GCN on non-visual tasks (<a href="https://arxiv.org/abs/1811.09595" rel="external nofollow noopener" target="_blank">Knyazev et al., NeurIPS-W, 2018</a>). Since function <em>D</em> depends on coordinates, the generated filters are also anisotropic and have a shape of oriented and elongated Gaussians as illustrated below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/288/1*TM_3zmnc4esqwqIdfgTvvA.png"><figcaption>Filters trained with MoNet in polar coordinates ğœƒ and <em>Ï</em>. Each ellipse corresponds to a slice of a Gaussian at some fixed level. The idea is that if the coordinates of the i-th node are close to the middle of the j-th Gaussian, then the generated weight at index (i,j) will have a value close toÂ 1.</figcaption></figure> <pre><strong><em>Pseudo-code of the MoNet layer using PyTorch</em></strong></pre> <pre><strong># assume X to be input <em>N</em></strong>Ã—<strong><em>C</em> node features</strong><br><strong># coord are <em>N</em>Ã—<em>N</em>Ã—<em>2</em> node coordinate differences between all pairs of nodes (node degrees for non-geometric tasks)<br># coord can be viewed as angular and radial edges between nodes</strong></pre> <pre>1. Generate <em>J</em> Gaussian-shaped filters based on coordinates of nodes    using some trainable function D<br>   weights = D(coord)  # weights: <em>J</em>Ã—<em>N</em>Ã—<em>N</em><br>2. Multiply node features X by these weights<br>   X = torch.bmm(weights, X.expand(J, N, C))  # X: <em>J</em>Ã—<em>N</em>Ã—<em>C</em><br>3. Project features by a learnable linear transformation<br>   X = fc(X.permute(1, 2, 0).view(N, J*C))  # X: <em>N</em>Ã—<em>F<br></em>4. Feed X to the next layer</pre> <h3>Conclusion</h3> <p>Despite a lengthy discussion, we have only scratched the surface. Applications of graph neural networks are expanding far beyond typical graph reasoning tasks, like molecule classification. The number of different graph neural layers is increasing very quickly, similar to how it was for convolutional networks a few years ago, so itâ€™s hard to keep track of them. On that note, <a href="https://github.com/rusty1s/pytorch_geometric" rel="external nofollow noopener" target="_blank">PyTorch Geometric (PyG)</a>â€Šâ€”â€Ša nice toolbox to learn from graphsâ€Šâ€”â€Šfrequently populates its collection with novel layers andÂ tricks.</p> <p><em>Acknowledgement: A large portion of this tutorial was prepared during my internship at SRI International under the supervision of </em><a href="https://medium.com/u/6cf41cb2c546" rel="external nofollow noopener" target="_blank"><em>Mohamed Amer</em></a><em> (</em><a href="https://mohamedramer.com/" rel="external nofollow noopener" target="_blank"><em>homepage</em></a><em>) and my PhD advisor Graham Taylor (</em><a href="https://www.gwtaylor.ca/" rel="external nofollow noopener" target="_blank"><em>homepage</em></a><em>). I also thank </em><a href="https://www.linkedin.com/in/carolynaugusta/" rel="external nofollow noopener" target="_blank"><em>Carolyn Augusta</em></a><em> for useful feedback.</em></p> <p>Find me on <a href="https://github.com/bknyaz/" rel="external nofollow noopener" target="_blank">Github</a>, <a href="https://www.linkedin.com/in/boris-knyazev-39690948/" rel="external nofollow noopener" target="_blank">LinkedIn</a> and <a href="https://twitter.com/BorisAKnyazev" rel="external nofollow noopener" target="_blank">Twitter</a>. <a href="https://bknyaz.github.io/">My homepage</a>.</p> <p>If you want to cite this blog post in your paper, please use:<br><a href="http://twitter.com/misc" rel="external nofollow noopener" target="_blank"><em>@misc</em></a><em>{knyazev2019tutorial,<br> title={Tutorial on Graph Neural Networks for Computer Vision and Beyond},<br> author={Knyazev, Boris and Taylor, Graham W and Amer, Mohamed R},<br> year={2019}<br>}</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=be6d71d70f49" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49" rel="external nofollow noopener" target="_blank">Anisotropic, Dynamic, Spectral and Multiscale Filters Defined on Graphs</a> was originally published in <a href="https://medium.com/data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>