<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>Tutorial on Graph Neural Networks for Computer Vision and¬†Beyond</h3> <p><em>I‚Äôm answering questions that AI/ML/CV people not familiar with graphs or graph neural networks typically ask. I provide PyTorch examples to clarify the idea behind this relatively new and exciting kind of¬†model.</em></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6AB8X6dumCaBBYQZ6vfRuw.png"><figcaption>A figure from (<a href="https://arxiv.org/abs/1312.6203" rel="external nofollow noopener" target="_blank">Bruna et al., ICLR, 2014</a>) depicting an MNIST image on the 3D sphere. While it‚Äôs hard to adapt <a href="https://arxiv.org/abs/1801.10130" rel="external nofollow noopener" target="_blank">Convolutional Networks to classify spherical data</a>, Graph Networks can naturally handle it. This is a toy example, but similar tasks arise in many real applications.</figcaption></figure> <p>The questions addressed in this part of my tutorial¬†are:</p> <ol> <li><strong>Why are graphs¬†useful?</strong></li> <li><strong>Why is it difficult to define convolution on¬†graphs?</strong></li> <li><strong>What makes a neural network a graph neural¬†network?</strong></li> </ol> <p>To answer them, I‚Äôll provide motivating examples, papers and Python code making it a tutorial on Graph Neural Networks (GNNs). Some basic knowledge of machine learning and computer vision is expected, however, I‚Äôll provide some background and intuitive explanation as we¬†go.</p> <p>First of all, let‚Äôs briefly recall what is a <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)" rel="external nofollow noopener" target="_blank">graph</a>? A graph <em>G</em> is a set of nodes (vertices) connected by directed/undirected edges. Nodes and edges typically come from some expert knowledge or intuition about the problem. So, it can be atoms in molecules, users in a social network, cities in a transportation system, players in team sport, neurons in the brain, interacting objects in a dynamic physical system, pixels, bounding boxes or segmentation masks in images. In other words, in many practical cases, it is actually <strong>you</strong> who gets to decide what are the nodes and edges in a¬†graph.</p> <blockquote>In many practical cases, it is actually you who gets to decide what are the nodes and edges in a¬†graph.</blockquote> <p>This is a very flexible data structure that generalizes many other data structures. For example, if there are no edges, then it becomes a <a href="https://en.wikipedia.org/wiki/Set_(abstract_data_type)" rel="external nofollow noopener" target="_blank">set</a>; if there are only ‚Äúvertical‚Äù edges and any two nodes are connected by exactly one path, then we have a <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)" rel="external nofollow noopener" target="_blank">tree</a>. Such flexibility is both good and bad as I‚Äôll discuss in this tutorial.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/754/1*68Gcr70UTpdaX7THbZSEcA.png"><figcaption>Two undirected graphs with 5 and 6 nodes. The order of nodes is arbitrary.</figcaption></figure> <h3>1. Why graphs can be¬†useful?</h3> <p>In the context of computer vision (CV) and machine learning (ML), studying graphs and the models to learn from them can give us at least four benefits:</p> <ol> <li>We can become closer to solving important problems that previously were too challenging, such as: drug discovery for cancer (<a href="https://www.nature.com/articles/s41598-019-45349-y" rel="external nofollow noopener" target="_blank">Veselkov et al., Nature, 2019</a>); better understanding of the human brain connectome (<a href="https://www.nature.com/articles/s41467-018-06346-3" rel="external nofollow noopener" target="_blank">Diez &amp; Sepulcre, Nature Communications, 2019</a>); materials discovery for energy and environmental challenges (<a href="https://www.nature.com/articles/s41467-019-10663-6" rel="external nofollow noopener" target="_blank">Xie et al., Nature Communications, 2019</a>).</li> <li>In most CV/ML applications, data can be actually viewed as graphs even though you used to represent them as another data structure. Representing your data as graph(s) gives you a lot of flexibility and can give you a very different and interesting perspective on your problem. For instance, instead of learning from image pixels you can learn from ‚Äú<a href="https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic" rel="external nofollow noopener" target="_blank">superpixels</a>‚Äù as in (<a href="https://arxiv.org/abs/1603.07063" rel="external nofollow noopener" target="_blank">Liang et al., ECCV, 2016</a>) and in our forthcoming <a href="https://arxiv.org/abs/1907.09000" rel="external nofollow noopener" target="_blank">BMVC paper</a>. Graphs also let you impose a relational inductive bias in data‚Ää‚Äî‚Ääsome prior knowledge you have about the problem. For instance, if you want to reason about a human pose, your relational bias can be a graph of skeleton joints of a human body (<a href="https://arxiv.org/abs/1801.07455" rel="external nofollow noopener" target="_blank">Yan et al., AAAI, 2018</a>); or if you want to reason about videos, your relational bias can be a graph of moving bounding boxes (<a href="https://arxiv.org/abs/1806.01810" rel="external nofollow noopener" target="_blank">Wang &amp; Gupta, ECCV, 2018</a>). Another example can be representing facial landmarks as a graph (<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Antonakos_Active_Pictorial_Structures_2015_CVPR_paper.html" rel="external nofollow noopener" target="_blank">Antonakos et al., CVPR, 2015</a>) to make reasoning about <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" rel="external nofollow noopener" target="_blank">facial attributes</a> and identity.</li> <li> <a href="https://arxiv.org/abs/2403.12143" rel="external nofollow noopener" target="_blank">Your favourite neural network itself can be viewed as a graph, where nodes are neurons and edges are weights</a>, or <a href="https://arxiv.org/abs/2303.04143" rel="external nofollow noopener" target="_blank">where nodes are layers and edges denote flow of forward/backward pass</a> (in which case we are talking about a computational graph used in TensorFlow, PyTorch and other DL frameworks). An application can be optimization of a computational graph, <a href="https://arxiv.org/abs/2110.13100" rel="external nofollow noopener" target="_blank">neural architecture search</a>, <a href="https://ai.googleblog.com/2018/06/how-can-neural-network-similarity-help.html" rel="external nofollow noopener" target="_blank">analyzing training behavior</a>, <a href="https://arxiv.org/abs/2409.04434" rel="external nofollow noopener" target="_blank">accelerating training</a>, etc.</li> <li>Finally, you can solve many problems, where data can be more naturally represented as graphs, <em>more effectively</em>. This includes, but is not limited to, molecule and social network classification (<a href="https://arxiv.org/abs/1811.09595" rel="external nofollow noopener" target="_blank">Knyazev et al., NeurIPS-W, 2018</a>) and generation (<a href="https://arxiv.org/abs/1802.03480" rel="external nofollow noopener" target="_blank">Simonovsky &amp; Komodakis, ICANN, 2018</a>), 3D Mesh classification and correspondence (<a href="https://arxiv.org/abs/1711.08920" rel="external nofollow noopener" target="_blank">Fey et al., CVPR, 2018</a>) and generation (<a href="https://arxiv.org/abs/1804.01654" rel="external nofollow noopener" target="_blank">Wang et al., ECCV, 2018</a>), modeling behavior of dynamic interacting objects (<a href="https://arxiv.org/abs/1802.04687" rel="external nofollow noopener" target="_blank">Kipf et al., ICML, 2018</a>), visual scene graph modeling (see the upcoming <a href="https://cs.stanford.edu/people/ranjaykrishna/sgrl/index.html" rel="external nofollow noopener" target="_blank">ICCV Workshop</a>) and question answering (<a href="https://arxiv.org/abs/1811.00538" rel="external nofollow noopener" target="_blank">Narasimhan, NeurIPS, 2018</a>), program synthesis (<a href="https://arxiv.org/abs/1711.00740" rel="external nofollow noopener" target="_blank">Allamanis et al., ICLR, 2018</a>), different reinforcement learning tasks (<a href="https://arxiv.org/abs/1904.03177" rel="external nofollow noopener" target="_blank">Bapst et al., ICML, 2019</a>) and many other exciting problems.</li> </ol> <p>As my previous research was related to recognizing and analyzing faces and emotions, I particularly like this figure¬†below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/607/1*yBuAXZIqq_7VknWEZdyUNQ.png"><figcaption>A figure from (<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Antonakos_Active_Pictorial_Structures_2015_CVPR_paper.html" rel="external nofollow noopener" target="_blank">Antonakos et al., CVPR, 2015</a>) showing representation of a face as a graph of landmarks. This is an interesting approach, but it is not a sufficient facial representation in many cases, since a lot can be told from the face texture captured well by convolutional networks. In contrast, reasoning over 3D meshes of a face looks like a more sensible approach compared to 2D landmarks (<a href="https://arxiv.org/abs/1807.10267" rel="external nofollow noopener" target="_blank">Ranjan et al., ECCV,¬†2018</a>).</figcaption></figure> <h3>2. Why is it difficult to define convolution on¬†graphs?</h3> <p>To answer this question, I first give some motivation for using convolution in general and then describe ‚Äúconvolution on images‚Äù using the graph terminology which should make the transition to ‚Äúconvolution on graphs‚Äù more¬†smooth.</p> <h4>2.1. Why is convolution useful?</h4> <p>Let‚Äôs understand why we care about convolution so much and why we want to use it for graphs. Compared to fully-connected neural networks (a.k.a. NNs or MLPs), convolutional networks (a.k.a. CNNs or ConvNets) have certain advantages explained below based on the image of a nice old¬†Chevy.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*Cf5Wwx1-Z7gQHUq4EOgQSg.jpeg"><figcaption>‚ÄúChevrolet Vega‚Äù according to Google Image¬†Search.</figcaption></figure> <p><strong>First</strong>, ConvNets exploit a natural prior in images, more formally described in (<a href="https://arxiv.org/abs/1611.08097" rel="external nofollow noopener" target="_blank">Bronstein et al., 2016</a>), such¬†as:</p> <ol> <li>Shift-invariance‚Ää‚Äî‚Ääif we translate the car on the image above to the left/right/up/down, we still should be able to detect and recognize it as a car. This is exploited by sharing filters across all locations, i.e. applying convolution.</li> <li>Locality‚Ää‚Äî‚Äänearby pixels are closely related and often represent some semantic concept, such as a wheel or a window. This is exploited by using relatively large filters, which can capture image features in a local spatial neighborhood.</li> <li>Compositionality (or hierarchy)‚Äî a larger region in the image is often a semantic parent of smaller regions it contains. For example, a car is a parent of doors, windows, wheels, driver, etc. And a driver is a parent of head, arms, etc. This is implicitly exploited by stacking convolutional layers and applying¬†pooling.</li> </ol> <p><strong>Second</strong>, the number of trainable parameters (i.e. filters) in convolutional layers does not depend on the input dimensionality, so technically we can train exactly the same model on 28√ó28 and 512√ó512 images. In other words, the model is <em>parametric</em>.</p> <blockquote>Ideally, our goal is to develop a model that is as flexible as Graph Neural Nets and can digest and learn from any data, but at the same time we want to control (regularize) factors of this flexibility by turning on/off certain¬†priors.</blockquote> <p>All these nice properties make ConvNets less prone to overfitting (high accuracy on the training set and low accuracy on the validation/test set), more accurate in different visual tasks, and easily scalable to large images and datasets. So, when we want to solve important tasks where input data are graph-structured, it is appealing to transfer all these properties to <strong>graph neural networks (GNNs) </strong>to regularize their flexibility and make them scalable. Ideally, our goal is to develop a model that is as flexible as GNNs and can digest and learn from any data, but at the same time we want to control (regularize) factors of this flexibility by turning on/off certain priors. This can open research in many interesting directions. However, controlling of this trade-off is challenging.</p> <h4>2.2. Convolution on images in terms of¬†graphs</h4> <p>Let‚Äôs consider an undirected graph <em>G</em> with <em>N</em> nodes. Edges <em>E</em> represent undirected connections between nodes. Nodes and edges typically come from your intuition about the problem. Our intuition in the case of images is that nodes are pixels or <a href="https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic" rel="external nofollow noopener" target="_blank">superpixels</a> (a group of pixels of weird shape) and edges are spatial distances between them. For example, the <a href="https://arxiv.org/abs/1905.10498" rel="external nofollow noopener" target="_blank">MNIST</a> image below on the left is typically represented as an 28√ó28 dimensional matrix. We can also represent it as a set of <em>N</em>=28*28=784 pixels. So, our graph <em>G</em> is going to have <em>N</em>=784 nodes and edges will have large values (thicker edges in the Figure below) for closely located pixels and small values (thinner edges) for remote¬†pixels.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Kji3yJN0cT6RwO11h0Mh6A.png"><figcaption>An image from the MNIST dataset on the left and an example of its graph representation on the right. Darker and larger nodes on the right correspond to higher pixel intensities. The figure on the right is inspired by Figure 5 in (<a href="https://arxiv.org/abs/1711.08920" rel="external nofollow noopener" target="_blank">Fey et al., CVPR,¬†2018</a>)</figcaption></figure> <p>When we train our neural networks or ConvNets on images, we implicitly define images on a graph‚Ää‚Äî‚Ääa <em>regular</em> two-dimensional grid as the one on the figure below. Since this grid is the same for all training and test images and is <em>regular</em>, i.e. all pixels of the grid are connected to each other in exactly the same way across all images (i.e. have the same number of neighbors, length of edges, etc.), this regular grid graph has no information that will help us to tell one image from another. Below I visualize some 2D and 3D regular grids, where the order of nodes is color-coded. By the way, I‚Äôm using <a href="https://networkx.github.io/" rel="external nofollow noopener" target="_blank">NetworkX</a> in Python to do that, e.g. G = networkx.grid_graph([4, 4]).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/880/0*XlNlsS2iG45j-adq"><figcaption>Examples of regular 2D and 3D grids. Images are defined on 2D grids and videos are on 3D¬†grids.</figcaption></figure> <p>Given this 4√ó4 regular grid, let‚Äôs briefly look at how 2D convolution works to understand why it‚Äôs difficult to transfer this operator to graphs. A filter on a regular grid has the same order of nodes, but modern convolutional nets typically have small filters, such as 3√ó3 in the example below. This filter has 9 values: <em>W</em>‚ÇÅ,<em>W</em>‚ÇÇ,‚Ä¶, <em>W</em>‚Çâ, which is what we are updating during training using backprop to minimize the loss and solve the downstream task. In our example below, we just heuristically initialize this filter to be an edge detector (see other possible filters¬†<a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)" rel="external nofollow noopener" target="_blank">here</a>):</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/576/1*Y2xcReChWy3JYGSw0_Rt0Q.png"><figcaption>Example of a 3√ó3 filter on a regular 2D grid with arbitrary weights w on the left and an edge detector on the¬†right.</figcaption></figure> <p>When we perform convolution, we slide this filter in both directions: to the right and to the bottom, but nothing prevents us from starting in the bottom corner‚Ää‚Äî‚Ääthe important thing is to slide over all possible locations. At each location, we compute the <a href="https://en.wikipedia.org/wiki/Dot_product" rel="external nofollow noopener" target="_blank"><em>dot product</em></a><em> </em>between the values on the grid (let‚Äôs denote them as <em>X</em>) and the values of filters, <em>W</em>: <em>X</em>‚ÇÅ<em>W</em>‚ÇÅ+<em>X</em>‚ÇÇ<em>W</em>‚ÇÇ+‚Ä¶+<em>X</em>‚Çâ<em>W</em>‚Çâ, and store the result in the output image. In our visualization, we change the color of nodes during sliding to match the colors of nodes in the grid. In a regular grid, we always can match a node of the filter with a node of the grid. Unfortunately, this is not true for graphs as I‚Äôll explain later¬†below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VxofpMT7GwLKZGVGWc2XHQ.png"><figcaption>2 steps of 2D convolution on a regular grid. If we don‚Äôt apply padding, there will be 4 steps in total, so the result will be a 2√ó2 image. To make the resulting image larger, we need to apply <a href="https://deeplizard.com/learn/video/qSTv_m-KFk0" rel="external nofollow noopener" target="_blank">padding</a>. See a comprehensive guide to convolution in deep learning¬†<a href="https://arxiv.org/abs/1603.07285" rel="external nofollow noopener" target="_blank">here</a>.</figcaption></figure> <p>The dot product used above is one of so called ‚Äúaggregator operators‚Äù. Broadly speaking, the goal of an aggregator operator is to summarize data to a reduced form. In our example above, the dot product summarizes a 3√ó3 matrix to a <em>single</em> value. Another example is pooling in ConvNets. Keep in mind, that such methods as max or sum pooling are <em>permutation-invariant</em>, i.e. they will pool the same value from a spatial region even if you randomly shuffle all pixels inside that region. To make it clear, the dot product is <em>not</em> permutation-invariant simply because in general: <em>X</em>‚ÇÅ<em>W</em>‚ÇÅ+<em>X</em>‚ÇÇ<em>W</em>‚ÇÇ ‚â†<em>X</em>‚ÇÇ<em>W</em>‚ÇÅ+<em>X</em>‚ÇÅ<em>W</em>‚ÇÇ.</p> <p>Now let‚Äôs use our MNIST image and illustrate the meaning of a regular grid, a filter and convolution. Keeping in mind our graph terminology, this regular 28√ó28 grid will be our graph <em>G</em>, so that every cell in this grid is a node, and node features are an actual image <em>X</em>, i.e. every node will have just a single feature‚Ää‚Äî‚Ääpixel intensity from 0 (black) to 1¬†(white).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*8nWUYV-nIuwL1bGCw3TnEw.png"><figcaption>Regular 28√ó28 grid (left) and an image on that grid¬†(right).</figcaption></figure> <p>Next, we define a filter and let it be a famous <a href="https://en.wikipedia.org/wiki/Gabor_filter" rel="external nofollow noopener" target="_blank">Gabor filter</a> with some (almost) arbitrary parameters. Once we have an image and a filter, we can perform <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html" rel="external nofollow noopener" target="_blank">convolution</a> by sliding the filter over that image (of digit 7 in our case) and putting the result of the dot product to the output matrix after each¬†step.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*9pzcalk9Lu4e-jqQHgcIUg.png"><figcaption>A 28√ó28 filter (left) and the result of 2D convolution of this filter with the image of digit 7¬†(right).</figcaption></figure> <p>This is all cool, but as I mentioned before, it becomes tricky when you try to generalize convolution to¬†graphs.</p> <blockquote>Nodes are a set, and any permutation of this set does not change it. Therefore, the <em>aggregator</em> operator that people apply should be <em>permutation-invariant</em>.</blockquote> <p>As I have already mentioned, the dot product used above to compute convolution at each step is <em>sensitive</em> to the order. This sensitivity permits us to learn edge detectors similar to Gabor filters important to capture image features. The problem is that in graphs <em>there is no well-defined order of nodes</em> unless you learn to order them, or come up with some heuristic that will result in a consistent (canonical) order from graph to graph. In short, nodes are a set, and any permutation of this set does not change it. Therefore, the <em>aggregator</em> operator that people apply should be <em>permutation-invariant</em>. The most popular choices are averaging (GCN, <a href="https://arxiv.org/abs/1609.02907" rel="external nofollow noopener" target="_blank">Kipf &amp; Welling, ICLR, 2017</a>) and summation (GIN, <a href="https://arxiv.org/abs/1810.00826" rel="external nofollow noopener" target="_blank">Xu et al., ICLR, 2019</a>) of <strong>all</strong> neighbors, i.e. sum or mean pooling, followed by projection by a trainable vector <em>W</em>. See <a href="https://arxiv.org/abs/1706.02216" rel="external nofollow noopener" target="_blank">Hamilton et al., NIPS, 2017</a> for some other aggregators.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/746/1*r91KCqXWXm3ltrixv_kUcA.png"><figcaption>Illustration of ‚Äúconvolution on graphs‚Äù of node features <em>X with </em>filter <em>W</em> centered at node 1 (dark¬†blue).</figcaption></figure> <p>For example, for the graph above on the left, the output of the summation aggregator for node 1 will be <em>X</em>‚ÇÅ=(<em>X</em>‚ÇÅ+<em>X</em>‚ÇÇ+<em>X</em>‚ÇÉ+<em>X</em>‚ÇÑ)<em>W</em>‚ÇÅ, for node 2: <em>X</em>‚ÇÇ=(<em>X</em>‚ÇÅ+<em>X</em>‚ÇÇ+<em>X</em>‚ÇÉ+<em>X</em>‚ÇÖ)<em>W</em>‚ÇÅ and so forth for nodes 3, 4 and 5, i.e. we need to apply this aggregator for all nodes. In result, we will have the graph with the same structure, but node features will now contain features of neighbors. We can process the graph on the right using the same¬†idea.</p> <p>Colloquially, people call this averaging or summation ‚Äúconvolution‚Äù, since we also ‚Äúslide‚Äù from one node to another and apply an aggregator operator in each step. However, it‚Äôs important to keep in mind that this is a very specific form of convolution, where filters don‚Äôt have a sense of orientation. Below I‚Äôll show how those filters look like and give an idea how to make them¬†better.</p> <h3>3. What makes a neural network a graph neural¬†network?</h3> <p>You know how a classical neural network works, right? We have some <em>C</em>-dimensional features <em>X</em> as the input to the net. Using our running MNIST example, <em>X</em><strong> </strong>will be our <em>C</em>=784 dimensional pixel features (i.e. a ‚Äúflattened‚Äù image). These features get multiplied by <em>C</em>√ó<em>F </em>dimensional weights <em>W</em> that we update during training to get the output closer to what we expect<em>. </em>The result can be directly used to solve the task (e.g. in case of regression) or can be further fed to some nonlinearity (activation), like ReLU, or other differentiable (or more precisely, sub-differentiable) functions to form a multi-layer network. In general, the output of some layer <em>l</em>¬†is:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/741/1*PK-poDpkwSxcifQcO7Lidw.png"><figcaption>Fully-connected layer with learnable weights W. ‚ÄúFully-connected‚Äù means that each output value in X<em>‚ÅΩÀ°‚Å∫¬π‚Åæ</em> depends on, or ‚Äúconnected to‚Äù, all inputs X<em>‚ÅΩÀ°‚Åæ</em>. Typically, although not always, we add a bias term to the¬†output.</figcaption></figure> <p>The signal in MNIST is so strong, that you can get an accuracy of 91% by just using the formula above and the Cross Entropy loss without any nonlinearities and other tricks (I used a slightly modified <a href="https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py" rel="external nofollow noopener" target="_blank">PyTorch example</a> to do that). Such model is called multinomial (or multiclass, since we have 10 classes of digits) logistic regression.</p> <p>Now, how do we transform our vanilla neural network to a graph neural network? As you already know, the core idea behind GNNs is aggregation over ‚Äúneighbors‚Äù. Here, it is important to understand that in many cases, it is actually <strong>you</strong> who specifies ‚Äúneighbors‚Äù.</p> <p>Let‚Äôs consider a simple case first, when you are given some graph. For example, this can be a fragment (subgraph) of a social network with 5 persons and an edge between a pair of nodes denotes if two people are friends (or at least one of them think so). An <a href="https://en.wikipedia.org/wiki/Adjacency_matrix" rel="external nofollow noopener" target="_blank">adjacency matrix</a> (usually denoted as <em>A</em>) in the figure below on the right is a way to represent these edges in a matrix form, convenient for our deep learning frameworks. Yellow cells in the matrix represent the edge and blue‚Ää‚Äî‚Ääthe absence of the¬†edge.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7cmkI4y_CYsF-sj0qIJrEA.png"><figcaption>Example of a graph and its adjacency matrix. The order of nodes we defined in both cases is random, while the graph is still the¬†same.</figcaption></figure> <p>Now, let‚Äôs create an adjacency matrix <em>A</em> for our MNIST example based on coordinates of pixels (complete code is provided in the end of the¬†post):</p> <pre><em>import numpy as np<br>from scipy.spatial.distance import cdist</em></pre> <pre>img_size = 28  <strong># MNIST image width and height</strong><br>col, row = np.meshgrid(np.arange(img_size), np.arange(img_size))<br>coord = np.stack((col, row), axis=2).reshape(-1, 2) / img_size<br>dist = cdist(coord, coord)  <strong># see figure below on the left</strong><br>sigma = 0.2 * np.pi  <strong># width of a Gaussian</strong><br>A = np.exp(- dist ** 2 / sigma ** 2)  <strong># see figure below in the middle</strong></pre> <p>This is a typical, but not the only, way to define an adjacency matrix for visual tasks (<a href="https://arxiv.org/abs/1606.09375" rel="external nofollow noopener" target="_blank">Defferrard et al., NIPS, 2016</a>, <a href="https://arxiv.org/abs/1611.08097" rel="external nofollow noopener" target="_blank">Bronstein et al., 2016</a>). This adjacency matrix is our prior, or our inductive bias, we impose on the model based on our intuition that nearby pixels should be connected and remote pixels shouldn‚Äôt or should have very thin edge (edge of a small value). This is motivated by observations that in natural images nearby pixels often correspond to the same object or objects that interact frequently (the locality principle we mentioned in Section 2.1.), so it makes a lot of sense to connect such¬†pixels.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FD55QIOgm7CRGMG565KucQ.png"><figcaption>Adjacency matrix (<em>N</em>x<em>N) in the form of distances (left) and closeness (middle) between all pairs of nodes. (right) A subgraph with 16 neighboring pixels corresponding to the adjacency matrix in the middle. Since it‚Äôs a complete subgraph, it‚Äôs also called a ‚Äúclique‚Äù.</em></figcaption></figure> <p>So, now instead of having just features <em>X</em> we have some fancy matrix <em>A</em> with values in the range [0,1]. It‚Äôs important to note that once we know that our input is a graph, we assume that there is no canonical order of nodes that will be consistent across all other graphs in the dataset. In terms of images, it means that <em>pixels are assumed to be randomly shuffled</em>. Finding the canonical order of nodes is combinatorially unsolvable in practice. Even though for MNIST we technically can cheat by knowing this order (because data are originally from a regular grid), it‚Äôs not going to work on actual graph datasets.</p> <p>Remember that our matrix of features <em>X</em> has ùëÅ rows and C columns. So, in terms of graphs, each row corresponds to one node and <em>C</em> is the dimensionality of node features. But now the problem is that we don‚Äôt know the order of nodes, so we don‚Äôt know in which row to put features of a particular node. If we just pretend to ignore this problem and feed <em>X</em> directly to an MLP as we did before, the effect will be the same as feeding images with randomly shuffled pixels with <em>independent</em> (yet the same for each epoch) shuffling for each image! Surprisingly, a neural network can in principle still fit such random data (<a href="https://arxiv.org/abs/1611.03530" rel="external nofollow noopener" target="_blank">Zhang et al., ICLR, 2017</a>), however test performance will be close to random prediction. One of the solutions is to simply use the adjacency matrix <em>A,</em> we created before, in the following way:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/741/1*Ht9tBXaTrV2gkbMIe7zxMg.png"><figcaption>Graph neural layer with adjacency matrix A, input/output features X and learnable weights¬†W.</figcaption></figure> <p>We just need to make sure that row <em>i</em> in <em>A</em> corresponds to features of node in row <em>i</em> of <em>X</em>. Here, I‚Äôm using ùìê instead of plain <em>A</em>, because often you want to normalize <em>A</em>. If ùìê=<em>A</em>, the matrix multiplication ùìê<em>X‚ÅΩÀ°‚Åæ </em>will be equivalent to summing features of neighbors, which turned out to be useful in many tasks (<a href="https://arxiv.org/abs/1810.00826" rel="external nofollow noopener" target="_blank">Xu et al., ICLR, 2019</a>). Most commonly, you normalize it so that ùìê<em>X‚ÅΩÀ°‚Åæ </em>averages features of neighbors, i.e. ùìê=<em>A</em><strong>/</strong>Œ£·µ¢<em>A</em>·µ¢. A better way to normalize matrix <em>A</em> can be found in (<a href="https://arxiv.org/abs/1609.02907" rel="external nofollow noopener" target="_blank">Kipf &amp; Welling, ICLR,¬†2017</a>).</p> <p>Below is the comparison of NN and GNN in terms of PyTorch¬†code:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/025a3cd1ab937d3f9fc09f062c0ed5a0/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/025a3cd1ab937d3f9fc09f062c0ed5a0/href</a></iframe> <p>And <a href="https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py" rel="external nofollow noopener" target="_blank">HERE</a> is the full PyTorch code to train two models above: python mnist_fc.py --model fc to train the NN case; python mnist_fc.py --model graph to train the GNN case. As an exercise, try to randomly shuffle pixels in code in the --model graph case (don‚Äôt forget to shuffle <em>A</em> in the same way) and make sure that it will not affect the result. Is it going to be true for the --model fc¬†case?</p> <blockquote> <a href="https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py" rel="external nofollow noopener" target="_blank">Here</a> is the full PyTorch code to train two¬†models.</blockquote> <p>After running the code, you may notice that the classification accuracy is actually about the same. What‚Äôs the problem? Aren‚Äôt graph networks supposed to work better? Well, they are, in many cases. But not in this one, because the ùìê<em>X‚ÅΩÀ°‚Åæ </em>operator we added is actually nothing else, but a Gaussian¬†filter:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*njbAaq3jLybNb7kV5x7big.png"><figcaption>2D visualization of a filter used in a graph neural network and it‚Äôs effect on the¬†image.</figcaption></figure> <p>So, our graph neural network turned out to be equivalent to a convolutional neural network with a single Gaussian filter, that we never update during training, followed by the fully-connected layer. This filter basically blurs/smooths the image, which is not a particularly useful thing to do (see the image above on the right). However, this is the simplest variant of a graph neural network, which nevertheless works great on graph-structured data. To make GNNs work better on regular graphs, like images, we need to apply a bunch of tricks. For example, instead of using a predefined Gaussian filter, we can learn to predict an edge between any pair of pixels by using a differentiable function like¬†this:</p> <pre>import torch.nn as nn  # using PyTorch</pre> <pre>nn.Sequential(nn.Linear(4, 64),  <strong># map coordinates to a hidden layer</strong><br>              nn.ReLU(),         <strong># nonlinearity</strong><br>              nn.Linear(64, 1),  <strong># map hidden representation to edge</strong><br>              nn.Tanh())         <strong># squash edge values to [-1, 1]</strong></pre> <blockquote>To make GNNs work better on regular graphs, like images, we need to apply a bunch of tricks. For example, instead of using a predefined Gaussian filter, we can learn to predict an edge between any pair of¬†pixels.</blockquote> <p>This idea is similar to Dynamic Filter Networks (<a href="https://arxiv.org/abs/1605.09673" rel="external nofollow noopener" target="_blank">Brabander et al., NIPS, 2016</a>), Edge-conditioned Graph Networks (ECC, <a href="https://arxiv.org/abs/1704.02901" rel="external nofollow noopener" target="_blank">Simonovsky &amp; Komodakis, CVPR, 2017</a>) and (<a href="https://arxiv.org/abs/1811.09595" rel="external nofollow noopener" target="_blank">Knyazev et al., NeurIPS-W, 2018</a>). To try it using <a href="https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py" rel="external nofollow noopener" target="_blank">my code</a>, you just need to add the --pred_edge flag, so the entire command is python mnist_fc.py --model graph --pred_edge. Below I show the animation of the predefined Gaussian and learned filters. You may notice that the filter we just learned (in the middle) looks weird. That‚Äôs because the task is quite complicated since we optimize two models at the same time: the model that predicts edges and the model that predicts a digit class. To learn better filters (like the one on the right), we need to apply some other tricks from our <a href="https://arxiv.org/abs/1907.09000" rel="external nofollow noopener" target="_blank">BMVC paper</a>, which is beyond the scope of this part of the tutorial.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/292/1*fmwFNf4MaDL0MrdQEk1xqQ.gif"><figcaption>2D filter of a graph neural network centered in the red point. Averaging (left, accuracy 92.24%), learned based on coordinates (middle, accuracy 91.05%), learned based on coordinates with some tricks (right, accuracy¬†92.39%).</figcaption></figure> <p>The code to generate these GIFs is quite¬†simple:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/35d81808306e1fff2dbd090b7cf08a5e/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/35d81808306e1fff2dbd090b7cf08a5e/href</a></iframe> <p>I‚Äôm also sharing an <a href="https://nbviewer.jupyter.org/github/bknyaz/examples/blob/master/2d_convolution.ipynb" rel="external nofollow noopener" target="_blank">IPython notebook</a> showing 2D convolution of an image with a Gabor filter in terms of graphs (using an adjacency matrix) compared to using <a href="https://en.wikipedia.org/wiki/Circulant_matrix" rel="external nofollow noopener" target="_blank">circulant matrices</a>, which is often used in signal processing.</p> <p>In <a href="https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49" rel="external nofollow noopener" target="_blank">the next part of the tutorial</a>, I‚Äôll tell you about more advanced graph layers that can lead to better filters on¬†graphs.</p> <p><strong>Update:</strong></p> <p>Throught this blog post and in the code the dist variable should have been squared to make it a Gaussian. Thanks <a href="https://medium.com/u/96dddd48cbe9" rel="external nofollow noopener" target="_blank">Alfredo Canziani</a> for spotting that. All figures and results were generated without squaring it. If you observe very different results after squaring it, I suggest to tune¬†sigma.</p> <h3>Conclusion</h3> <p>Graph Neural Networks are a very flexible and interesting family of neural networks that can be applied to really complex data. As always, such flexibility must come at a certain cost. In case of GNNs it is the difficulty of regularizing the model by defining such operators as convolution. Research in that direction is advancing quite fast, so that GNNs will see application in increasingly wider areas of machine learning and computer¬†vision.</p> <p>See another <a href="https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications" rel="external nofollow noopener" target="_blank">nice blog post about GNNs</a> from <a href="http://neptune.ai" rel="external nofollow noopener" target="_blank">Neptune.ai</a>.</p> <p><em>Acknowledgement: A large portion of this tutorial was prepared during my internship at SRI International under the supervision of </em><a href="https://medium.com/u/6cf41cb2c546" rel="external nofollow noopener" target="_blank"><em>Mohamed Amer</em></a><em> (</em><a href="https://mohamedramer.com/" rel="external nofollow noopener" target="_blank"><em>homepage</em></a><em>) and my PhD advisor Graham Taylor (</em><a href="https://www.gwtaylor.ca/" rel="external nofollow noopener" target="_blank"><em>homepage</em></a><em>).</em></p> <p>Find me on <a href="https://github.com/bknyaz/" rel="external nofollow noopener" target="_blank">Github</a>, <a href="https://www.linkedin.com/in/boris-knyazev-39690948/" rel="external nofollow noopener" target="_blank">LinkedIn</a> and <a href="https://twitter.com/BorisAKnyazev" rel="external nofollow noopener" target="_blank">Twitter</a>. <a href="https://bknyaz.github.io/">My homepage</a>.</p> <p>If you want to cite this tutorial in your paper, please use:<br><em>@misc{knyazev2019tutorial,<br> title={Tutorial on Graph Neural Networks for Computer Vision and Beyond},<br> author={Knyazev, Boris and Taylor, Graham W and Amer, Mohamed R},<br> year={2019}<br>}</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3d9fada3b80d" width="1" height="1" alt=""></p> </body></html>